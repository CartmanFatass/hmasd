#!/usr/bin/env python3
"""
ã€å¢å¼ºç‰ˆã€‘HMASDä¸¥æ ¼æŒ‰è®ºæ–‡Algorithm 1 + Appendix Eçš„å¤šçº¿ç¨‹Rollout-basedè®­ç»ƒè„šæœ¬
é›†æˆä¸‰ä¸ªæ ¸å¿ƒå¢å¼ºç»„ä»¶ï¼š
1. AtomicDataBuffer: åŸå­æ€§æ“ä½œä¿è¯ã€ä¼˜å…ˆçº§é˜Ÿåˆ—å¤„ç†ã€æ‹¥å¡æ£€æµ‹å’Œè‡ªé€‚åº”å¤„ç†
2. ThreadSafeAgentProxy: åˆ†ç¦»é”å‡å°‘ç«äº‰ã€åå°å­˜å‚¨é˜Ÿåˆ—ç¼“å†²ã€åŸå­æ€§å­˜å‚¨æ“ä½œã€å­˜å‚¨å¤±è´¥æ¢å¤æœºåˆ¶
3. EnhancedTrainingWorker: æœ¬åœ°ç¼“å­˜å‡å°‘é”ç«äº‰ã€è‡ªé€‚åº”é‡è¯•ç­–ç•¥ã€æ•°æ®å®Œæ•´æ€§éªŒè¯ã€å¤±è´¥æ•°æ®æŒä¹…åŒ–å’Œæ¢å¤

æ ¸å¿ƒæ”¹è¿›ï¼š
- è§£å†³æ•°æ®ç«äº‰å’Œé”ç«äº‰é—®é¢˜
- æä¾›æ•°æ®é›¶ä¸¢å¤±ä¿è¯
- å®ç°æ™ºèƒ½é‡è¯•å’Œæ•…éšœæ¢å¤
- æ·»åŠ å…¨é¢çš„æ€§èƒ½ç›‘æ§
- æ”¯æŒè¿è¡Œæ—¶é…ç½®å’Œè°ƒä¼˜
"""

import os
import sys
import time
import numpy as np
import torch
import argparse
import threading
import queue
import multiprocessing as mp
from datetime import datetime
from collections import defaultdict, deque
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock, Event, Barrier
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# è®¾ç½®matplotlibåç«¯ï¼ˆé¿å…å¤šè¿›ç¨‹é—®é¢˜ï¼‰
import matplotlib
matplotlib.use('Agg')

from logger import get_logger, init_multiproc_logging, LOG_LEVELS, shutdown_logging
from config import Config
from hmasd.thread_safe_hmasd_agent import ThreadSafeHMASDAgent
from envs.pettingzoo.scenario1 import UAVBaseStationEnv
from envs.pettingzoo.scenario2 import UAVCooperativeNetworkEnv
from envs.pettingzoo.env_adapter import ParallelToArrayAdapter

# å¯¼å…¥å¢å¼ºç»„ä»¶
from atomic_data_buffer import AtomicDataBuffer
from thread_safe_agent_proxy import ThreadSafeAgentProxy
from enhanced_training_worker import EnhancedTrainingWorker

# å¯¼å…¥ Stable Baselines3 çš„å‘é‡åŒ–ç¯å¢ƒ
from stable_baselines3.common.vec_env import SubprocVecEnv

class ThreadSafeCounter:
    """çº¿ç¨‹å®‰å…¨çš„è®¡æ•°å™¨"""
    def __init__(self, initial_value=0):
        self._value = initial_value
        self._lock = Lock()
    
    def increment(self, amount=1):
        with self._lock:
            self._value += amount
            return self._value
    
    def get(self):
        with self._lock:
            return self._value
    
    def set(self, value):
        with self._lock:
            self._value = value

class EnhancedRolloutWorker:
    """ã€å¢å¼ºç‰ˆã€‘å•ä¸ªrollout workerï¼Œé›†æˆåŸå­æ€§æ•°æ®å¤„ç†"""
    def __init__(self, worker_id, env_factory, config, data_buffer, control_events, logger):
        self.worker_id = worker_id
        self.env_factory = env_factory
        self.config = config
        self.data_buffer = data_buffer
        self.control_events = control_events
        self.logger = logger
        
        # åˆ›å»ºç¯å¢ƒ
        self.env = env_factory()
        
        # çŠ¶æ€å˜é‡
        self.samples_collected = 0
        self.episodes_completed = 0
        self.total_reward = 0.0
        
        # rolloutå®Œæˆæ§åˆ¶
        self.rollout_completed = False
        self.target_rollout_steps = config.rollout_length
        
        # ç¯å¢ƒçŠ¶æ€
        self.env_state = None
        self.env_observations = None
        self.episode_step = 0
        
        # ä¸¥æ ¼çš„æ­¥æ•°è®¡æ•°æ–¹æ³•
        self.strict_step_counter = 0
        self.accumulated_reward = 0.0
        self.current_team_skill = None
        self.current_agent_skills = None
        self.skill_log_probs = None
        self.high_level_experiences_generated = 0
        
        # æ­¥æ•°éªŒè¯æ ‡å¿—
        self.step_validation_enabled = True
        self.last_reported_steps = 0
        
        # æŠ€èƒ½è®¡æ—¶å™¨åˆå§‹åŒ–
        self.skill_timer = 0
        
        # ã€å¢å¼ºåŠŸèƒ½ã€‘æ•°æ®å®Œæ•´æ€§éªŒè¯
        self.data_validation_enabled = True
        self.validation_failures = 0
        
        # ã€å¢å¼ºåŠŸèƒ½ã€‘æ€§èƒ½ç›‘æ§
        self.operation_times = deque(maxlen=100)
        self.last_performance_log = time.time()
        
    def reset_environment(self):
        """é‡ç½®ç¯å¢ƒ"""
        try:
            result = self.env.reset()
            if isinstance(result, tuple):
                self.env_observations, info = result
                self.env_state = info.get('state', np.zeros(self.config.state_dim))
            else:
                self.env_observations = result
                self.env_state = np.zeros(self.config.state_dim)
            self.episode_step = 0
            return True
        except Exception as e:
            self.logger.error(f"Worker {self.worker_id}: é‡ç½®ç¯å¢ƒå¤±è´¥: {e}")
            return False
    
    def step_environment(self, actions):
        """æ‰§è¡Œç¯å¢ƒæ­¥éª¤"""
        try:
            # ä¿®å¤ï¼šå¤„ç†Gymnasium APIçš„5ä¸ªè¿”å›å€¼
            result = self.env.step(actions)
            
            if len(result) == 5:
                # Gymnasiumæ ¼å¼: observations, reward, terminated, truncated, info
                next_observations, rewards, terminated, truncated, infos = result
                dones = terminated or truncated  # åˆå¹¶ç»ˆæ­¢æ¡ä»¶
            elif len(result) == 4:
                # ä¼ ç»Ÿæ ¼å¼: observations, rewards, dones, infos
                next_observations, rewards, dones, infos = result
            else:
                raise ValueError(f"Unexpected number of return values from env.step(): {len(result)}")
            
            # ä»infoä¸­æå–next_state
            if isinstance(infos, dict):
                next_state = infos.get('next_state', np.zeros(self.config.state_dim))
            elif isinstance(infos, list) and len(infos) > 0:
                next_state = infos[0].get('next_state', np.zeros(self.config.state_dim))
            else:
                next_state = np.zeros(self.config.state_dim)
            
            self.episode_step += 1
            return next_observations, rewards, dones, next_state
            
        except Exception as e:
            self.logger.error(f"Worker {self.worker_id}: ç¯å¢ƒæ­¥éª¤å¤±è´¥: {e}")
            # è¿”å›å®‰å…¨çš„é»˜è®¤å€¼è€Œä¸æ˜¯None
            n_agents = len(actions) if hasattr(actions, '__len__') else self.config.n_agents
            default_obs = np.zeros((n_agents, self.config.obs_dim))
            return default_obs, 0.0, True, np.zeros(self.config.state_dim)
    
    def run(self, agent_proxy):
        """ã€å¢å¼ºç‰ˆã€‘è¿è¡Œrollout workerä¸»å¾ªç¯"""
        self.logger.info(f"å¢å¼ºç‰ˆRollout worker {self.worker_id} å¼€å§‹è¿è¡Œï¼Œç›®æ ‡æ”¶é›†æ­¥æ•°: {self.target_rollout_steps}")
        
        # åˆå§‹åŒ–rolloutå¼€å§‹æ—¶é—´
        self.rollout_start_time = time.time()
        
        # é‡ç½®ç¯å¢ƒ
        if not self.reset_environment():
            self.logger.error(f"Worker {self.worker_id}: åˆå§‹åŒ–å¤±è´¥")
            return
        
        try:
            # æ— é™å¾ªç¯è®­ç»ƒæ¨¡å¼ï¼šæ¯æ¬¡å®Œæˆä¸€ä¸ªrolloutåç­‰å¾…æ–°çš„å‘¨æœŸå¼€å§‹
            while not self.control_events['stop'].is_set():
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æš‚åœ
                if self.control_events['pause'].is_set():
                    time.sleep(0.1)
                    continue
                
                # å¦‚æœå½“å‰rolloutå·²å®Œæˆï¼Œç­‰å¾…æ–°çš„rolloutå‘¨æœŸ
                if self.rollout_completed:
                    time.sleep(0.1)  # ç­‰å¾…è®­ç»ƒå®Œæˆå’ŒçŠ¶æ€é‡ç½®
                    continue
                
                # æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°rolloutæ­¥æ•°é™åˆ¶
                if self.samples_collected >= self.target_rollout_steps:
                    self.rollout_completed = True
                    self.complete_rollout()
                    continue
                
                # æ‰§è¡Œä¸€ä¸ªrolloutæ­¥éª¤
                success = self.run_step_enhanced(agent_proxy)
                if not success:
                    self.logger.warning(f"Worker {self.worker_id}: æ­¥éª¤æ‰§è¡Œå¤±è´¥ï¼Œé‡ç½®ç¯å¢ƒ")
                    if not self.reset_environment():
                        break
                
                # çŸ­æš‚ç¡çœ é¿å…è¿‡åº¦å ç”¨CPU
                time.sleep(0.001)
        
        except Exception as e:
            self.logger.error(f"Worker {self.worker_id}: è¿è¡Œå¼‚å¸¸: {e}")
        finally:
            try:
                self.env.close()
            except:
                pass
            self.logger.info(f"å¢å¼ºç‰ˆRollout worker {self.worker_id} ç»“æŸè¿è¡Œ")
    
    def run_step_enhanced(self, agent_proxy):
        """ã€å¢å¼ºç‰ˆã€‘æ‰§è¡Œå•ä¸ªrolloutæ­¥éª¤ - é›†æˆæ•°æ®éªŒè¯å’ŒåŸå­æ€§æ“ä½œ"""
        step_start_time = time.time()
        
        try:
            # ç¡®ä¿ç¯å¢ƒçŠ¶æ€æœ‰æ•ˆ
            if self.env_state is None:
                self.logger.warning(f"Worker {self.worker_id}: env_stateä¸ºNoneï¼Œé‡ç½®ç¯å¢ƒ")
                if not self.reset_environment():
                    return False
            
            if self.env_observations is None:
                self.logger.warning(f"Worker {self.worker_id}: env_observationsä¸ºNoneï¼Œé‡ç½®ç¯å¢ƒ")
                if not self.reset_environment():
                    return False
            
            # æ¯æ¬¡éƒ½é‡æ–°åˆ†é…æŠ€èƒ½ï¼Œä¿æŒç®€å•é€»è¾‘
            team_skill, agent_skills, log_probs = agent_proxy.assign_skills_for_worker(
                self.env_state, self.env_observations, self.worker_id
            )
            
            # æ›´æ–°å½“å‰æŠ€èƒ½çŠ¶æ€
            self.current_team_skill = team_skill
            self.current_agent_skills = agent_skills
            self.skill_log_probs = log_probs
            
            # ä»ä»£ç†è·å–åŠ¨ä½œ
            actions, action_logprobs = agent_proxy.get_actions_for_worker(
                self.env_state, self.env_observations, agent_skills, self.worker_id
            )
            
            # æ‰§è¡Œç¯å¢ƒæ­¥éª¤
            next_observations, rewards, dones, next_state = self.step_environment(actions)
            
            # åŸå­æ€§æ­¥æ•°è®¡æ•° - ç¡®ä¿ä¸€è‡´æ€§
            step_before_increment = self.samples_collected
            self.samples_collected += 1
            step_after_increment = self.samples_collected
            
            # ç¡®ä¿rewardsæ˜¯æœ‰æ•ˆçš„æ•°å€¼
            if rewards is None:
                current_reward = 0.0
                self.logger.warning(f"Worker {self.worker_id}: ç¯å¢ƒæ­¥éª¤è¿”å›Noneå¥–åŠ±ï¼Œä½¿ç”¨0.0")
            else:
                current_reward = rewards if isinstance(rewards, (int, float)) else np.sum(rewards)
            
            # ç´¯ç§¯å¥–åŠ±ï¼ˆç”¨äºé«˜å±‚ç»éªŒï¼‰
            self.accumulated_reward += current_reward
            self.total_reward += current_reward
            
            # ã€å¢å¼ºåŠŸèƒ½ã€‘å®‰å…¨å¤åˆ¶æ•°æ®ï¼Œé¿å…None.copy()é”™è¯¯
            def safe_copy(data, default_shape=None):
                if data is None:
                    if default_shape is not None:
                        return np.zeros(default_shape)
                    return None
                if hasattr(data, 'copy'):
                    return data.copy()
                return np.array(data)
            
            # ã€å¢å¼ºåŠŸèƒ½ã€‘æ•°æ®å®Œæ•´æ€§éªŒè¯
            if self.data_validation_enabled:
                if not self._validate_experience_data(current_reward, actions, next_state):
                    self.validation_failures += 1
                    self.logger.warning(f"Worker {self.worker_id}: ç»éªŒæ•°æ®éªŒè¯å¤±è´¥")
                    return False
            
            # å­˜å‚¨ä½å±‚ç»éªŒæ•°æ®
            low_level_experience = {
                'experience_type': 'low_level',
                'worker_id': self.worker_id,
                'state': safe_copy(self.env_state, (self.config.state_dim,)),
                'observations': safe_copy(self.env_observations, (self.config.n_agents, self.config.obs_dim)),
                'actions': safe_copy(actions, (self.config.n_agents, self.config.action_dim)),
                'rewards': current_reward,
                'next_state': safe_copy(next_state, (self.config.state_dim,)),
                'next_observations': safe_copy(next_observations, (self.config.n_agents, self.config.obs_dim)),
                'dones': dones,
                'episode_step': self.episode_step,
                'team_skill': team_skill,
                'agent_skills': safe_copy(agent_skills, (self.config.n_agents,)) if agent_skills is not None else [0] * self.config.n_agents,
                'action_logprobs': safe_copy(action_logprobs, (self.config.n_agents,)),
                'skill_log_probs': log_probs,
                'step_number': step_after_increment,
                'timestamp': time.time()  # ã€å¢å¼ºåŠŸèƒ½ã€‘æ·»åŠ æ—¶é—´æˆ³
            }
            
            # ã€å¢å¼ºåŠŸèƒ½ã€‘ä½¿ç”¨åŸå­æ€§æ•°æ®ç¼“å†²åŒº
            success = self.data_buffer.put(low_level_experience, block=True, timeout=None)
            if success:
                self.logger.debug(f"Worker {self.worker_id}: ä½å±‚ç»éªŒå·²æ”¾å…¥ç¼“å†²åŒº - æ­¥éª¤={step_after_increment}")
            else:
                self.logger.error(f"Worker {self.worker_id}: ä½å±‚ç»éªŒæ”¾å…¥ç¼“å†²åŒºå¤±è´¥ï¼")
                return False
            
            # æ„é€ StateSkillDatasetæ•°æ®
            state_skill_experience = {
                'experience_type': 'state_skill',
                'worker_id': self.worker_id,
                'state': safe_copy(next_state, (self.config.state_dim,)),
                'team_skill': team_skill,
                'observations': safe_copy(next_observations, (self.config.n_agents, self.config.obs_dim)),
                'agent_skills': safe_copy(agent_skills, (self.config.n_agents,)) if agent_skills is not None else [0] * self.config.n_agents,
                'step_number': step_after_increment,
                'timestamp': time.time()
            }
            
            # å°†StateSkillDatasetæ•°æ®æ”¾å…¥ç¼“å†²åŒº
            success = self.data_buffer.put(state_skill_experience, block=True, timeout=None)
            if success:
                self.logger.debug(f"Worker {self.worker_id}: StateSkillæ•°æ®å·²æ”¾å…¥ç¼“å†²åŒº")
            else:
                self.logger.error(f"Worker {self.worker_id}: StateSkillæ•°æ®æ”¾å…¥ç¼“å†²åŒºå¤±è´¥ï¼")
                return False
            
            # ç¡®å®šæ€§é«˜å±‚ç»éªŒæ”¶é›† - ä¸¥æ ¼æŒ‰kæ­¥æ”¶é›†
            if step_after_increment % self.config.k == 0:
                self.logger.debug(f"Worker {self.worker_id}: ç¡®å®šæ€§kæ­¥æ”¶é›†é«˜å±‚ç»éªŒ - "
                               f"æ­¥æ•°={step_after_increment}, k={self.config.k}, "
                               f"ç´¯ç§¯å¥–åŠ±={self.accumulated_reward:.4f}")
                
                success = self.store_high_level_experience_enhanced(f"ç¡®å®šæ€§kæ­¥æ”¶é›†(æ­¥æ•°={step_after_increment})")
                if success:
                    self.logger.debug(f"Worker {self.worker_id}: é«˜å±‚ç»éªŒæ”¶é›†æˆåŠŸ - ç¬¬{self.high_level_experiences_generated}ä¸ª")
                    # åªåœ¨æˆåŠŸå­˜å‚¨åé‡ç½®ç´¯ç§¯å¥–åŠ±
                    self.accumulated_reward = 0.0
                else:
                    self.logger.error(f"Worker {self.worker_id}: é«˜å±‚ç»éªŒå­˜å‚¨å¤±è´¥ï¼")
            
            # å®‰å…¨æ›´æ–°ç¯å¢ƒçŠ¶æ€
            self.env_state = safe_copy(next_state, (self.config.state_dim,))
            self.env_observations = safe_copy(next_observations, (self.config.n_agents, self.config.obs_dim))
            
            # æ£€æŸ¥episodeæ˜¯å¦ç»“æŸ
            max_episode_length = getattr(self.config, 'rollout_max_episode_length', 5000)
            if dones or self.episode_step >= max_episode_length:
                self.episodes_completed += 1
                termination_reason = "ç¯å¢ƒè‡ªç„¶ç»ˆæ­¢" if dones else f"è¾¾åˆ°æœ€å¤§æ­¥æ•°é™åˆ¶({max_episode_length})"
                self.logger.debug(f"Worker {self.worker_id}: Episode {self.episodes_completed} å®Œæˆ, "
                                f"æ­¥æ•°: {self.episode_step}, å¥–åŠ±: {self.total_reward:.2f}, "
                                f"ç»ˆæ­¢åŸå› : {termination_reason}")
                
                # é‡ç½®ç¯å¢ƒå’ŒæŠ€èƒ½çŠ¶æ€
                if not self.reset_environment():
                    return False
                self.reset_skill_state()
                self.total_reward = 0.0
            
            # ã€å¢å¼ºåŠŸèƒ½ã€‘æ€§èƒ½ç›‘æ§
            step_time = time.time() - step_start_time
            self.operation_times.append(step_time)
            
            # å®šæœŸè®°å½•æ€§èƒ½ç»Ÿè®¡
            if time.time() - self.last_performance_log > 60:  # æ¯åˆ†é’Ÿè®°å½•ä¸€æ¬¡
                self._log_performance_stats()
                self.last_performance_log = time.time()
            
            return True
            
        except Exception as e:
            self.logger.error(f"Worker {self.worker_id}: æ­¥éª¤æ‰§è¡Œå¼‚å¸¸: {e}")
            # å³ä½¿å¼‚å¸¸ä¹Ÿè¦ç¡®ä¿æ­¥æ•°è®¡æ•°çš„ä¸€è‡´æ€§
            if hasattr(self, 'samples_collected'):
                self.samples_collected += 1
            return False
    
    def _validate_experience_data(self, reward, actions, next_state):
        """ã€å¢å¼ºåŠŸèƒ½ã€‘éªŒè¯ç»éªŒæ•°æ®çš„å®Œæ•´æ€§"""
        try:
            # æ£€æŸ¥å¥–åŠ±æœ‰æ•ˆæ€§
            if reward is not None:
                reward_val = reward if isinstance(reward, (int, float)) else np.sum(reward)
                if np.isnan(reward_val) or np.isinf(reward_val):
                    return False
            
            # æ£€æŸ¥åŠ¨ä½œæœ‰æ•ˆæ€§
            if actions is not None:
                if np.any(np.isnan(actions)) or np.any(np.isinf(actions)):
                    return False
            
            # æ£€æŸ¥çŠ¶æ€æœ‰æ•ˆæ€§
            if next_state is not None:
                if np.any(np.isnan(next_state)) or np.any(np.isinf(next_state)):
                    return False
            
            return True
            
        except Exception as e:
            self.logger.error(f"Worker {self.worker_id}: æ•°æ®éªŒè¯å¼‚å¸¸: {e}")
            return False
    
    def store_high_level_experience_enhanced(self, reason="æŠ€èƒ½å‘¨æœŸå®Œæˆ"):
        """ã€å¢å¼ºç‰ˆã€‘å­˜å‚¨é«˜å±‚ç»éªŒåˆ°æ•°æ®ç¼“å†²åŒº"""
        if self.current_team_skill is None or self.current_agent_skills is None:
            self.logger.warning(f"Worker {self.worker_id}: æŠ€èƒ½çŠ¶æ€æ— æ•ˆï¼Œè·³è¿‡é«˜å±‚ç»éªŒå­˜å‚¨")
            return False
        
        try:
            high_level_experience = {
                'experience_type': 'high_level',
                'worker_id': self.worker_id,
                'state': self.env_state.copy(),
                'team_skill': self.current_team_skill,
                'observations': self.env_observations.copy(),
                'agent_skills': self.current_agent_skills.copy(),
                'accumulated_reward': self.accumulated_reward,
                'skill_log_probs': self.skill_log_probs.copy() if self.skill_log_probs else None,
                'skill_timer': self.skill_timer,
                'episode_step': self.episode_step,
                'reason': reason,
                'timestamp': time.time()  # ã€å¢å¼ºåŠŸèƒ½ã€‘æ·»åŠ æ—¶é—´æˆ³
            }
            
            # ã€å¢å¼ºåŠŸèƒ½ã€‘ä½¿ç”¨åŸå­æ€§æ•°æ®ç¼“å†²åŒº
            success = self.data_buffer.put(high_level_experience, block=True, timeout=None)
            if success:
                self.high_level_experiences_generated += 1
                self.logger.debug(f"Worker {self.worker_id}: é«˜å±‚ç»éªŒå·²å­˜å‚¨ - "
                                f"ç´¯ç§¯å¥–åŠ±={self.accumulated_reward:.4f}, åŸå› ={reason}, "
                                f"æ€»ç”Ÿæˆæ•°={self.high_level_experiences_generated}")
                
                # è®°å½•accumulated_rewardçš„é‡ç½®
                old_accumulated_reward = self.accumulated_reward
                self.accumulated_reward = 0.0
                self.skill_timer = 0
                self.logger.debug(f"ğŸ’° [REWARD_RESET] W{self.worker_id} accumulated_reward reset: "
                                f"{old_accumulated_reward:.4f} -> 0.0 (reason: {reason})")
                return True
            else:
                self.logger.error(f"Worker {self.worker_id}: é«˜å±‚ç»éªŒæ”¾å…¥ç¼“å†²åŒºå¤±è´¥ï¼")
                return False
                
        except Exception as e:
            self.logger.error(f"Worker {self.worker_id}: å­˜å‚¨é«˜å±‚ç»éªŒå¤±è´¥: {e}")
            return False
    
    def complete_rollout(self):
        """ã€å¢å¼ºç‰ˆã€‘å®Œæˆå½“å‰rollout"""
        completion_time = time.time()
        
        # åŸºç¡€éªŒè¯ - ç¡®ä¿æ­¥æ•°å‡†ç¡®
        if self.samples_collected != self.target_rollout_steps:
            self.logger.warning(f"Worker {self.worker_id}: æ­¥æ•°ä¸åŒ¹é…! "
                              f"æ”¶é›†={self.samples_collected}, ç›®æ ‡={self.target_rollout_steps}")
            # å¼ºåˆ¶åŒæ­¥æ­¥æ•°
            self.samples_collected = self.target_rollout_steps
        
        # ç¡®å®šæ€§é«˜å±‚ç»éªŒè®¡ç®—
        expected_high_level = self.target_rollout_steps // self.config.k
        current_high_level = self.high_level_experiences_generated
        missing = expected_high_level - current_high_level
        
        self.logger.info(f"Worker {self.worker_id}: ã€å¢å¼ºç‰ˆã€‘Rolloutå®ŒæˆéªŒè¯ - "
                       f"ç›®æ ‡æ­¥æ•°={self.target_rollout_steps}, å®é™…æ­¥æ•°={self.samples_collected}, "
                       f"æœŸæœ›é«˜å±‚={expected_high_level}, å½“å‰é«˜å±‚={current_high_level}, ç¼ºå¤±={missing}")
        
        # ç¡®å®šæ€§æ•°æ®è¡¥é½
        if missing > 0:
            self.logger.info(f"Worker {self.worker_id}: ã€å¢å¼ºç‰ˆã€‘å¼€å§‹ç¡®å®šæ€§è¡¥é½ {missing} ä¸ªé«˜å±‚ç»éªŒ")
            
            è¡¥é½æˆåŠŸè®¡æ•° = 0
            for i in range(missing):
                success = self.store_high_level_experience_enhanced(f"ã€å¢å¼ºç‰ˆã€‘ç¡®å®šæ€§è¡¥é½#{i+1}")
                if success:
                    è¡¥é½æˆåŠŸè®¡æ•° += 1
                    self.logger.debug(f"Worker {self.worker_id}: ç¡®å®šæ€§è¡¥é½#{i+1}æˆåŠŸ")
                else:
                    self.logger.error(f"Worker {self.worker_id}: ç¡®å®šæ€§è¡¥é½#{i+1}å¤±è´¥ï¼")
                    break
            
            # ç¡®ä¿è¡¥é½æˆåŠŸ
            if è¡¥é½æˆåŠŸè®¡æ•° == missing:
                self.logger.info(f"Worker {self.worker_id}: âœ… ç¡®å®šæ€§è¡¥é½å®Œæˆ {è¡¥é½æˆåŠŸè®¡æ•°}/{missing}")
            else:
                self.logger.error(f"Worker {self.worker_id}: âŒ ç¡®å®šæ€§è¡¥é½å¤±è´¥ {è¡¥é½æˆåŠŸè®¡æ•°}/{missing}")
        
        # æœ€ç»ˆéªŒè¯
        final_high_level = self.high_level_experiences_generated
        if final_high_level != expected_high_level:
            self.logger.error(f"Worker {self.worker_id}: âŒ ã€å¢å¼ºç‰ˆã€‘æœ€ç»ˆéªŒè¯å¤±è´¥! "
                            f"é«˜å±‚ç»éªŒ={final_high_level}, æœŸæœ›={expected_high_level}")
        else:
            self.logger.info(f"Worker {self.worker_id}: âœ… ã€å¢å¼ºç‰ˆã€‘æœ€ç»ˆéªŒè¯é€šè¿‡! "
                           f"é«˜å±‚ç»éªŒ={final_high_level}, æœŸæœ›={expected_high_level}")
        
        # ç­‰å¾…æ•°æ®ä¼ è¾“å®Œæˆ
        self.wait_for_data_transmission_complete_enhanced()
        
        # è®¡ç®—rolloutç»Ÿè®¡
        if not hasattr(self, 'rollout_start_time'):
            self.rollout_start_time = completion_time - 1.0
        
        rollout_duration = completion_time - self.rollout_start_time
        speed = self.samples_collected / rollout_duration if rollout_duration > 0 else 0
        
        self.logger.info(f"Worker {self.worker_id}: ã€å¢å¼ºç‰ˆã€‘Rolloutå®Œæˆç»Ÿè®¡ - "
                       f"æ­¥æ•°={self.samples_collected}, é«˜å±‚ç»éªŒ={self.high_level_experiences_generated}, "
                       f"è€—æ—¶={rollout_duration:.1f}s, é€Ÿåº¦={speed:.1f}æ­¥/s, "
                       f"éªŒè¯å¤±è´¥={self.validation_failures}")
        
        # é‡ç½®å¼€å§‹æ—¶é—´
        self.rollout_start_time = completion_time
    
    def wait_for_data_transmission_complete_enhanced(self):
        """ã€å¢å¼ºç‰ˆã€‘ç­‰å¾…æ•°æ®ä¼ è¾“100%å®Œæˆ"""
        max_wait_time = 15.0  # å¢å¼ºç‰ˆï¼šæ›´é•¿ç­‰å¾…æ—¶é—´
        wait_start = time.time()
        initial_queue_size = self.data_buffer.qsize()
        
        self.logger.debug(f"Worker {self.worker_id}: ã€å¢å¼ºç‰ˆã€‘å¼€å§‹ç­‰å¾…æ•°æ®ä¼ è¾“100%å®Œæˆ - "
                        f"åˆå§‹é˜Ÿåˆ—å¤§å°={initial_queue_size}")
        
        consecutive_empty_checks = 0
        required_empty_checks = 10  # éœ€è¦è¿ç»­10æ¬¡æ£€æŸ¥é˜Ÿåˆ—ä¸ºç©º
        
        while time.time() - wait_start < max_wait_time:
            current_queue_size = self.data_buffer.qsize()
            
            if current_queue_size == 0:
                consecutive_empty_checks += 1
                if consecutive_empty_checks >= required_empty_checks:
                    # è¿ç»­å¤šæ¬¡ç¡®è®¤é˜Ÿåˆ—ä¸ºç©ºï¼Œæ•°æ®ä¼ è¾“å®Œæˆ
                    break
            else:
                consecutive_empty_checks = 0  # é‡ç½®è®¡æ•°å™¨
            
            time.sleep(0.05)  # æ›´é¢‘ç¹çš„æ£€æŸ¥
        
        final_wait_time = time.time() - wait_start
        final_queue_size = self.data_buffer.qsize()
        
        if final_queue_size == 0 and consecutive_empty_checks >= required_empty_checks:
            self.logger.debug(f"Worker {self.worker_id}: âœ… ã€å¢å¼ºç‰ˆã€‘æ•°æ®ä¼ è¾“100%å®Œæˆ - "
                            f"ç­‰å¾…æ—¶é—´={final_wait_time:.2f}s, è¿ç»­ç©ºæ£€æŸ¥={consecutive_empty_checks}")
        else:
            self.logger.warning(f"Worker {self.worker_id}: âš ï¸ ã€å¢å¼ºç‰ˆã€‘æ•°æ®ä¼ è¾“æœªå®Œå…¨å®Œæˆ - "
                              f"ç­‰å¾…æ—¶é—´={final_wait_time:.2f}s, å‰©ä½™é˜Ÿåˆ—={final_queue_size}, "
                              f"è¿ç»­ç©ºæ£€æŸ¥={consecutive_empty_checks}/{required_empty_checks}")
    
    def _log_performance_stats(self):
        """ã€å¢å¼ºåŠŸèƒ½ã€‘è®°å½•æ€§èƒ½ç»Ÿè®¡"""
        if not self.operation_times:
            return
        
        avg_time = sum(self.operation_times) / len(self.operation_times)
        max_time = max(self.operation_times)
        min_time = min(self.operation_times)
        
        self.logger.debug(f"Worker {self.worker_id}: æ€§èƒ½ç»Ÿè®¡ - "
                        f"å¹³å‡æ­¥éª¤æ—¶é—´={avg_time*1000:.2f}ms, "
                        f"æœ€å¤§={max_time*1000:.2f}ms, æœ€å°={min_time*1000:.2f}ms, "
                        f"éªŒè¯å¤±è´¥={self.validation_failures}")
    
    def reset_skill_state(self):
        """é‡ç½®æŠ€èƒ½çŠ¶æ€"""
        self.accumulated_reward = 0.0
        self.current_team_skill = None
        self.current_agent_skills = None
        self.skill_log_probs = None
    
    def get_worker_stats(self):
        """è·å–workerç»Ÿè®¡ä¿¡æ¯"""
        return {
            'worker_id': self.worker_id,
            'samples_collected': self.samples_collected,
            'episodes_completed': self.episodes_completed,
            'high_level_experiences_generated': self.high_level_experiences_generated,
            'current_skill_timer': self.skill_timer,
            'current_accumulated_reward': self.accumulated_reward,
            'current_team_skill': self.current_team_skill,
            'current_episode_step': self.episode_step,
            'validation_failures': self.validation_failures,
            'avg_step_time_ms': sum(self.operation_times) / len(self.operation_times) * 1000 if self.operation_times else 0
        }

class EnhancedThreadedRolloutTrainer:
    """ã€å¢å¼ºç‰ˆã€‘å¤šçº¿ç¨‹HMASD Rollout-basedè®­ç»ƒå™¨"""
    
    def __init__(self, config, args=None):
        """
        åˆå§‹åŒ–å¢å¼ºç‰ˆå¤šçº¿ç¨‹è®­ç»ƒå™¨
        
        å‚æ•°:
            config: é…ç½®å¯¹è±¡
            args: å‘½ä»¤è¡Œå‚æ•°ï¼ˆå¯é€‰ï¼‰
        """
        self.config = config
        self.args = args or argparse.Namespace()
        
        # éªŒè¯å¹¶è®¾ç½®è®­ç»ƒæ¨¡å¼
        config.rollout_based_training = True
        config.episode_based_training = False
        config.sync_training_mode = False
        
        # éªŒè¯é…ç½®
        if not config.validate_rollout_config():
            raise ValueError("Rollouté…ç½®éªŒè¯å¤±è´¥")
        
        # çº¿ç¨‹é…ç½®ï¼ˆæŒ‰ç…§è®ºæ–‡ Appendix Eï¼‰
        self.num_training_threads = getattr(args, 'training_threads', 16)
        self.num_rollout_threads = getattr(args, 'rollout_threads', 32)
        
        # è®¾ç½®è®¾å¤‡
        self.device = self._get_device()
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.log_dir = f"logs/enhanced_threaded_rollout_training_{timestamp}"
        os.makedirs(self.log_dir, exist_ok=True)
        
        # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
        self._init_logging()
        
        # çº¿ç¨‹æ§åˆ¶
        self.control_events = {
            'stop': Event(),
            'pause': Event()
        }
        
        # ã€å¢å¼ºåŠŸèƒ½ã€‘ä½¿ç”¨åŸå­æ€§æ•°æ®ç¼“å†²åŒº
        buffer_size = getattr(args, 'buffer_size', 10000)
        persistence_dir = os.path.join(self.log_dir, 'buffer_persistence')
        enable_recovery = getattr(args, 'enable_recovery', True)
        
        self.data_buffer = AtomicDataBuffer(
            maxsize=buffer_size,
            persistence_dir=persistence_dir,
            enable_recovery=enable_recovery
        )
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.start_time = None
        self.total_updates = 0
        self.total_samples = ThreadSafeCounter()
        self.total_steps = ThreadSafeCounter()
        
        # ã€å¢å¼ºåŠŸèƒ½ã€‘æ€§èƒ½ç›‘æ§
        self.performance_monitor = {
            'rollout_speeds': deque(maxlen=100),
            'training_speeds': deque(maxlen=100),
            'buffer_utilizations': deque(maxlen=100),
            'last_monitor_time': time.time()
        }
        
        self.logger.info("ã€å¢å¼ºç‰ˆã€‘ThreadedRolloutTraineråˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"æ—¥å¿—ç›®å½•: {self.log_dir}")
        self.logger.info(f"è®­ç»ƒçº¿ç¨‹æ•°: {self.num_training_threads}")
        self.logger.info(f"Rolloutçº¿ç¨‹æ•°: {self.num_rollout_threads}")
        self.logger.info(f"æ•°æ®ç¼“å†²åŒºå¤§å°: {buffer_size}")
        self.logger.info(f"æ•°æ®æŒä¹…åŒ–: {persistence_dir}")
        self.logger.info(f"æ•…éšœæ¢å¤: {enable_recovery}")
        self.logger.info(config.get_rollout_summary())
    
    def _get_device(self):
        """è·å–è®¡ç®—è®¾å¤‡"""
        device_pref = getattr(self.args, 'device', 'auto')
        if device_pref == 'auto':
            return torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        elif device_pref == 'cuda':
            if torch.cuda.is_available():
                return torch.device('cuda')
            else:
                self.logger.warning("è¯·æ±‚CUDAä½†æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPU")
                return torch.device('cpu')
        else:
            return torch.device('cpu')
    
    def _init_logging(self):
        """åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ"""
        log_level = getattr(self.args, 'log_level', 'INFO')
        console_level = getattr(self.args, 'console_log_level', 'INFO')
        
        init_multiproc_logging(
            log_dir=self.log_dir,
            log_file='enhanced_threaded_rollout_training.log',
            file_level=LOG_LEVELS.get(log_level.lower(), 20),
            console_level=LOG_LEVELS.get(console_level.lower(), 20)
        )
        
        self.logger = get_logger("EnhancedThreadedRolloutTrainer")
    
    def create_env_factory(self):
        """åˆ›å»ºç¯å¢ƒå·¥å‚å‡½æ•°"""
        scenario = getattr(self.args, 'scenario', 2)
        n_uavs = getattr(self.args, 'n_uavs', 5)
        n_users = getattr(self.args, 'n_users', 50)
        user_distribution = getattr(self.args, 'user_distribution', 'uniform')
        channel_model = getattr(self.args, 'channel_model', '3gpp-36777')
        max_hops = getattr(self.args, 'max_hops', 3)
        
        def make_env():
            def _init():
                env_seed = int(time.time() * 1000) % 10000  # åŸºäºæ—¶é—´çš„éšæœºç§å­
                if scenario == 1:
                    raw_env = UAVBaseStationEnv(
                        n_uavs=n_uavs,
                        n_users=n_users,
                        user_distribution=user_distribution,
                        channel_model=channel_model,
                        seed=env_seed
                    )
                elif scenario == 2:
                    raw_env = UAVCooperativeNetworkEnv(
                        n_uavs=n_uavs,
                        n_users=n_users,
                        max_hops=max_hops,
                        user_distribution=user_distribution,
                        channel_model=channel_model,
                        seed=env_seed
                    )
                else:
                    raise ValueError(f"æœªçŸ¥åœºæ™¯: {scenario}")
                
                env = ParallelToArrayAdapter(raw_env, seed=env_seed)
                return env
            return _init()
        
        return make_env
    
    def validate_environment(self):
        """éªŒè¯ç¯å¢ƒæ¥å£å…¼å®¹æ€§"""
        self.logger.info("å¼€å§‹éªŒè¯ç¯å¢ƒæ¥å£å…¼å®¹æ€§...")
        
        temp_env = self.create_env_factory()()
        try:
            # æµ‹è¯•reset
            reset_result = temp_env.reset()
            self.logger.info(f"ç¯å¢ƒreset()è¿”å›: {len(reset_result)}ä¸ªå€¼")
            
            if len(reset_result) == 2:
                observations, info = reset_result
                self.logger.info(f"ResetæˆåŠŸ: observations.shape={observations.shape}, info keys={list(info.keys()) if isinstance(info, dict) else 'not dict'}")
            else:
                self.logger.warning(f"Resetè¿”å›å€¼æ•°é‡å¼‚å¸¸: {len(reset_result)}")
            
            # æµ‹è¯•step
            dummy_actions = np.random.randn(temp_env.n_uavs, temp_env.action_dim)
            step_result = temp_env.step(dummy_actions)
            self.logger.info(f"ç¯å¢ƒstep()è¿”å›: {len(step_result)}ä¸ªå€¼")
            
            if len(step_result) == 5:
                next_obs, reward, terminated, truncated, info = step_result
                self.logger.info(f"StepæˆåŠŸ (Gymnasiumæ ¼å¼): next_obs.shape={next_obs.shape}, "
                               f"reward={reward}, terminated={terminated}, truncated={truncated}")
            elif len(step_result) == 4:
                next_obs, reward, done, info = step_result
                self.logger.info(f"StepæˆåŠŸ (ä¼ ç»Ÿæ ¼å¼): next_obs.shape={next_obs.shape}, "
                               f"reward={reward}, done={done}")
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„stepè¿”å›å€¼æ•°é‡: {len(step_result)}")
            
            self.logger.info("âœ… ç¯å¢ƒæ¥å£éªŒè¯é€šè¿‡")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ç¯å¢ƒæ¥å£éªŒè¯å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
        finally:
            temp_env.close()
    
    def initialize_agent(self):
        """åˆå§‹åŒ–HMASDä»£ç†"""
        # éªŒè¯ç¯å¢ƒå…¼å®¹æ€§
        if not self.validate_environment():
            raise RuntimeError("ç¯å¢ƒæ¥å£éªŒè¯å¤±è´¥ï¼Œæ— æ³•ç»§ç»­è®­ç»ƒ")
        
        # åˆ›å»ºä¸´æ—¶ç¯å¢ƒè·å–ç»´åº¦ä¿¡æ¯
        temp_env = self.create_env_factory()()
        state_dim = temp_env.state_dim
        obs_dim = temp_env.obs_dim
        n_agents = temp_env.n_uavs
        temp_env.close()
        
        # æ›´æ–°é…ç½®
        self.config.update_env_dims(state_dim, obs_dim)
        self.config.n_agents = n_agents
        
        self.logger.info(f"ç¯å¢ƒç»´åº¦: state_dim={state_dim}, obs_dim={obs_dim}, n_agents={n_agents}")
        
        # åˆ›å»ºçº¿ç¨‹å®‰å…¨ä»£ç†
        self.agent = ThreadSafeHMASDAgent(
            config=self.config,
            log_dir=self.log_dir,
            device=self.device,
            debug=getattr(self.args, 'debug', False)
        )
        
        # ã€ä¿®å¤ã€‘ä½¿ç”¨ThreadSafeAgentProxyæ›¿ä»£åŸæ¥çš„AgentProxy
        self.agent_proxy = ThreadSafeAgentProxy(self.agent, self.config, self.logger, self.data_buffer)
        
        self.logger.info("ã€å¢å¼ºç‰ˆã€‘HMASDä»£ç†åˆå§‹åŒ–å®Œæˆ")
    
    def start_rollout_threads(self):
        """å¯åŠ¨rolloutçº¿ç¨‹"""
        self.logger.info(f"å¯åŠ¨ {self.num_rollout_threads} ä¸ªå¢å¼ºç‰ˆrolloutçº¿ç¨‹")
        
        self.rollout_workers = []
        self.rollout_threads = []
        
        env_factory = self.create_env_factory()
        
        for i in range(self.num_rollout_threads):
            # ã€å¢å¼ºåŠŸèƒ½ã€‘ä½¿ç”¨å¢å¼ºç‰ˆRolloutWorker
            worker = EnhancedRolloutWorker(
                worker_id=i,
                env_factory=env_factory,
                config=self.config,
                data_buffer=self.data_buffer,
                control_events=self.control_events,
                logger=self.logger
            )
            
            thread = threading.Thread(
                target=worker.run,
                args=(self.agent_proxy,),
                name=f"EnhancedRolloutWorker-{i}"
            )
            thread.daemon = True
            
            self.rollout_workers.append(worker)
            self.rollout_threads.append(thread)
            thread.start()
        
        self.logger.info("æ‰€æœ‰å¢å¼ºç‰ˆrolloutçº¿ç¨‹å·²å¯åŠ¨")
        
        # è®¾ç½®AgentProxyå¯¹rollout workersçš„å¼•ç”¨
        self.agent_proxy.rollout_workers = self.rollout_workers
    
    def start_training_threads(self):
        """å¯åŠ¨è®­ç»ƒçº¿ç¨‹"""
        self.logger.info(f"å¯åŠ¨ {self.num_training_threads} ä¸ªå¢å¼ºç‰ˆtrainingçº¿ç¨‹")
        
        self.training_workers = []
        self.training_threads = []
        
        for i in range(self.num_training_threads):
            # ã€å¢å¼ºåŠŸèƒ½ã€‘ä½¿ç”¨å¢å¼ºç‰ˆTrainingWorker
            worker = EnhancedTrainingWorker(
                worker_id=i,
                agent_proxy=self.agent_proxy,
                data_buffer=self.data_buffer,
                control_events=self.control_events,
                logger=self.logger,
                config=self.config,
                trainer=self
            )
            
            thread = threading.Thread(
                target=worker.run,
                name=f"EnhancedTrainingWorker-{i}"
            )
            thread.daemon = True
            
            self.training_workers.append(worker)
            self.training_threads.append(thread)
            thread.start()
        
        self.logger.info("æ‰€æœ‰å¢å¼ºç‰ˆtrainingçº¿ç¨‹å·²å¯åŠ¨")
    
    def monitor_training_enhanced(self, total_steps=100000):
        """ã€å¢å¼ºç‰ˆã€‘ç›‘æ§è®­ç»ƒè¿›åº¦"""
        self.logger.info(f"å¼€å§‹ã€å¢å¼ºç‰ˆã€‘è®­ç»ƒç›‘æ§ï¼Œç›®æ ‡æ­¥æ•°: {total_steps:,}")
        
        self.start_time = time.time()
        last_log_time = self.start_time
        last_stats_log_time = self.start_time
        last_performance_log_time = self.start_time
        last_step_count = 0
        
        try:
            while True:
                current_time = time.time()
                
                # ä½¿ç”¨æ­£ç¡®çš„ç´¯è®¡æ€»æ­¥æ•°
                cumulative_trainer_steps = self.total_steps.get()
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ­¥æ•°é™åˆ¶
                if cumulative_trainer_steps >= total_steps:
                    self.logger.info(f"è¾¾åˆ°è®­ç»ƒæ­¥æ•°é™åˆ¶ {total_steps:,} (å®é™…ç´¯è®¡: {cumulative_trainer_steps:,})ï¼Œåœæ­¢è®­ç»ƒ")
                    break
                
                # æ¯åˆ†é’Ÿè®°å½•ä¸€æ¬¡ç®€è¦è¿›åº¦
                if current_time - last_log_time >= 60:
                    self.log_progress_enhanced(cumulative_trainer_steps, total_steps)
                    last_log_time = current_time
                
                # æ¯10åˆ†é’Ÿè®°å½•ä¸€æ¬¡è¯¦ç»†ç»Ÿè®¡
                if current_time - last_stats_log_time >= 600:
                    self.log_detailed_stats_enhanced()
                    last_stats_log_time = current_time
                
                # ã€å¢å¼ºåŠŸèƒ½ã€‘æ¯5åˆ†é’Ÿè®°å½•ä¸€æ¬¡æ€§èƒ½ç›‘æ§
                if current_time - last_performance_log_time >= 300:
                    self.log_performance_monitoring()
                    last_performance_log_time = current_time
                
                # æ£€æŸ¥çº¿ç¨‹å¥åº·çŠ¶æ€
                self.check_thread_health_enhanced()
                
                time.sleep(30)  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
        
        except KeyboardInterrupt:
            self.logger.info("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        
        finally:
            self.stop_training_enhanced()
    
    def log_progress_enhanced(self, current_steps, total_steps):
        """ã€å¢å¼ºç‰ˆã€‘è®°å½•è®­ç»ƒè¿›åº¦"""
        progress_percent = (current_steps / total_steps) * 100
        remaining_steps = total_steps - current_steps
        
        # è®¡ç®—æ—¶é—´ç»Ÿè®¡
        elapsed_time = time.time() - self.start_time
        if current_steps > 0:
            estimated_total_time = elapsed_time * total_steps / current_steps
            remaining_time = estimated_total_time - elapsed_time
        else:
            remaining_time = 0
        
        # ã€å¢å¼ºåŠŸèƒ½ã€‘è·å–åŸå­æ€§ç¼“å†²åŒºç»Ÿè®¡
        buffer_stats = self.data_buffer.get_stats()
        
        # è®¡ç®—rollout workersç»Ÿè®¡
        total_samples = sum(worker.samples_collected for worker in self.rollout_workers)
        total_episodes = sum(worker.episodes_completed for worker in self.rollout_workers)
        total_high_level_exp = sum(worker.high_level_experiences_generated for worker in self.rollout_workers)
        total_validation_failures = sum(worker.validation_failures for worker in self.rollout_workers)
        
        # è®¡ç®—training workersç»Ÿè®¡
        total_updates = sum(worker.updates_performed for worker in self.training_workers)
        total_processed = sum(worker.samples_processed for worker in self.training_workers)
        
        # ã€å¢å¼ºåŠŸèƒ½ã€‘è·å–ä»£ç†ç»Ÿè®¡ - ä¿®å¤æ–¹æ³•è°ƒç”¨
        if hasattr(self.agent_proxy, 'get_storage_stats'):
            agent_stats = self.agent_proxy.get_storage_stats()
        else:
            # å›é€€åˆ°åŸºæœ¬ç»Ÿè®¡
            agent_stats = {
                'high_level_stored': getattr(self.agent_proxy, 'high_level_experiences_stored', 0),
                'low_level_stored': getattr(self.agent_proxy, 'low_level_experiences_stored', 0),
                'state_skill_stored': 0
            }
        
        # è®¡ç®—æ­¥æ•°é€Ÿåº¦
        steps_per_second = current_steps / elapsed_time if elapsed_time > 0 else 0
        
        self.logger.info(f"ã€å¢å¼ºç‰ˆã€‘è®­ç»ƒè¿›åº¦: {progress_percent:.1f}% "
                        f"({current_steps:,} / {total_steps:,} æ­¥), "
                        f"å‰©ä½™: {remaining_steps:,} æ­¥")
        self.logger.info(f"æ—¶é—´: å·²ç”¨={elapsed_time/3600:.1f}h, é¢„è®¡å‰©ä½™={remaining_time/3600:.1f}h, "
                        f"é€Ÿåº¦={steps_per_second:.1f} æ­¥/ç§’")
        self.logger.info(f"Rollout: æ ·æœ¬={total_samples:,}, Episodes={total_episodes:,}, "
                        f"é«˜å±‚ç»éªŒ={total_high_level_exp:,}, éªŒè¯å¤±è´¥={total_validation_failures}")
        self.logger.info(f"Training: æ›´æ–°={total_updates}, å¤„ç†æ ·æœ¬={total_processed:,}")
        self.logger.info(f"ä»£ç†å­˜å‚¨: é«˜å±‚={agent_stats['high_level_stored']}, "
                        f"ä½å±‚={agent_stats['low_level_stored']}, "
                        f"çŠ¶æ€æŠ€èƒ½={agent_stats['state_skill_stored']}")
        self.logger.info(f"åŸå­ç¼“å†²åŒº: é˜Ÿåˆ—={buffer_stats['queue_size']}, "
                        f"åˆ©ç”¨ç‡={buffer_stats['utilization']:.1%}, "
                        f"æ‹¥å¡={buffer_stats['congestion_detected']}")
    
    def log_detailed_stats_enhanced(self):
        """ã€å¢å¼ºç‰ˆã€‘è®°å½•è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
        self.logger.info("=== ã€å¢å¼ºç‰ˆã€‘è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ ===")
        
        # Rollout workersç»Ÿè®¡
        self.logger.info("å¢å¼ºç‰ˆRollout Workers:")
        for i, worker in enumerate(self.rollout_workers[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
            stats = worker.get_worker_stats()
            self.logger.info(f"  Worker {i}: æ ·æœ¬={stats['samples_collected']}, "
                           f"Episodes={stats['episodes_completed']}, "
                           f"é«˜å±‚ç»éªŒ={stats['high_level_experiences_generated']}, "
                           f"å½“å‰æŠ€èƒ½={stats['current_team_skill']}, "
                           f"ç´¯ç§¯å¥–åŠ±={stats['current_accumulated_reward']:.3f}, "
                           f"éªŒè¯å¤±è´¥={stats['validation_failures']}, "
                           f"å¹³å‡æ­¥éª¤æ—¶é—´={stats['avg_step_time_ms']:.2f}ms")
        if len(self.rollout_workers) > 5:
            self.logger.info(f"  ... è¿˜æœ‰ {len(self.rollout_workers) - 5} ä¸ªworkers")
        
        # Training workersç»Ÿè®¡
        self.logger.info("å¢å¼ºç‰ˆTraining Workers:")
        for i, worker in enumerate(self.training_workers[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
            stats = worker.get_performance_stats()
            self.logger.info(f"  Worker {i}: æ›´æ–°={stats['updates_performed']}, "
                           f"å¤„ç†æ ·æœ¬={stats['samples_processed']}, "
                           f"æˆåŠŸç‡={stats['success_rate']:.2%}, "
                           f"ç¼“å­˜å‘½ä¸­={stats['cache_hits']}, "
                           f"ç¼“å­˜æœªå‘½ä¸­={stats['cache_misses']}")
        if len(self.training_workers) > 5:
            self.logger.info(f"  ... è¿˜æœ‰ {len(self.training_workers) - 5} ä¸ªworkers")
        
        # ã€å¢å¼ºåŠŸèƒ½ã€‘åŸå­æ€§ç¼“å†²åŒºè¯¦ç»†ç»Ÿè®¡
        buffer_stats = self.data_buffer.get_stats()
        self.logger.info(f"åŸå­æ€§æ•°æ®ç¼“å†²åŒºè¯¦ç»†ç»Ÿè®¡:")
        self.logger.info(f"  é˜Ÿåˆ—å¤§å°: {buffer_stats['queue_size']}/{buffer_stats['max_size']}")
        self.logger.info(f"  åˆ©ç”¨ç‡: {buffer_stats['utilization']:.1%}")
        self.logger.info(f"  æ€»æ·»åŠ : {buffer_stats['total_added']}, æ€»æ¶ˆè´¹: {buffer_stats['total_consumed']}")
        self.logger.info(f"  é«˜ä¼˜å…ˆçº§: æ·»åŠ ={buffer_stats['high_priority_added']}, æ¶ˆè´¹={buffer_stats['high_priority_consumed']}")
        self.logger.info(f"  æ™®é€šä¼˜å…ˆçº§: æ·»åŠ ={buffer_stats['normal_priority_added']}, æ¶ˆè´¹={buffer_stats['normal_priority_consumed']}")
        self.logger.info(f"  å¤±è´¥é¡¹ç›®: {buffer_stats['failed_items']}")
        self.logger.info(f"  éªŒè¯å¤±è´¥: {buffer_stats['validation_failures']}")
        self.logger.info(f"  å¹³å‡æ“ä½œæ—¶é—´: {buffer_stats['avg_operation_time_ms']:.2f}ms")
        self.logger.info(f"  å¤„ç†é€Ÿåº¦: {buffer_stats['processing_speed']:.1f} é¡¹/ç§’")
        self.logger.info(f"  æ‹¥å¡æ£€æµ‹: {buffer_stats['congestion_detected']}")
        
        # ã€å¢å¼ºåŠŸèƒ½ã€‘çº¿ç¨‹å®‰å…¨ä»£ç†ç»Ÿè®¡
        agent_stats = self.agent_proxy.get_storage_stats()
        self.logger.info(f"çº¿ç¨‹å®‰å…¨ä»£ç†ç»Ÿè®¡:")
        self.logger.info(f"  å­˜å‚¨å°è¯•: {agent_stats['total_attempts']}")
        self.logger.info(f"  å­˜å‚¨æˆåŠŸ: {agent_stats['total_successes']}")
        self.logger.info(f"  å­˜å‚¨å¤±è´¥: {agent_stats['total_failures']}")
        self.logger.info(f"  é˜Ÿåˆ—æº¢å‡º: {agent_stats['queue_overflows']}")
        self.logger.info(f"  éªŒè¯å¤±è´¥: {agent_stats['validation_failures']}")
        
        # GPUå†…å­˜ä½¿ç”¨ï¼ˆå¦‚æœæœ‰GPUï¼‰
        if torch.cuda.is_available():
            memory_allocated = torch.cuda.memory_allocated(self.device) / 1024**3
            memory_reserved = torch.cuda.memory_reserved(self.device) / 1024**3
            self.logger.info(f"GPUå†…å­˜: å·²åˆ†é…={memory_allocated:.2f}GB, å·²ä¿ç•™={memory_reserved:.2f}GB")
    
    def log_performance_monitoring(self):
        """ã€å¢å¼ºåŠŸèƒ½ã€‘è®°å½•æ€§èƒ½ç›‘æ§ä¿¡æ¯"""
        current_time = time.time()
        
        # è®¡ç®—rollouté€Ÿåº¦
        total_samples = sum(worker.samples_collected for worker in self.rollout_workers)
        time_diff = current_time - self.performance_monitor['last_monitor_time']
        if time_diff > 0:
            rollout_speed = total_samples / time_diff
            self.performance_monitor['rollout_speeds'].append(rollout_speed)
        
        # è®¡ç®—ç¼“å†²åŒºåˆ©ç”¨ç‡
        buffer_stats = self.data_buffer.get_stats()
        self.performance_monitor['buffer_utilizations'].append(buffer_stats['utilization'])
        
        # è®¡ç®—å¹³å‡æ€§èƒ½æŒ‡æ ‡
        avg_rollout_speed = sum(self.performance_monitor['rollout_speeds']) / len(self.performance_monitor['rollout_speeds']) if self.performance_monitor['rollout_speeds'] else 0
        avg_buffer_util = sum(self.performance_monitor['buffer_utilizations']) / len(self.performance_monitor['buffer_utilizations']) if self.performance_monitor['buffer_utilizations'] else 0
        
        self.logger.info(f"ã€æ€§èƒ½ç›‘æ§ã€‘å¹³å‡rollouté€Ÿåº¦: {avg_rollout_speed:.1f} æ ·æœ¬/ç§’, "
                        f"å¹³å‡ç¼“å†²åŒºåˆ©ç”¨ç‡: {avg_buffer_util:.1%}")
        
        # æ£€æµ‹æ€§èƒ½å¼‚å¸¸
        if avg_buffer_util > 0.9:
            self.logger.warning("âš ï¸ ç¼“å†²åŒºåˆ©ç”¨ç‡è¿‡é«˜ï¼Œå¯èƒ½å­˜åœ¨æ€§èƒ½ç“¶é¢ˆ")
        if avg_rollout_speed < 10:
            self.logger.warning("âš ï¸ Rollouté€Ÿåº¦è¿‡ä½ï¼Œæ£€æŸ¥ç¯å¢ƒæˆ–ç½‘ç»œæ€§èƒ½")
        
        self.performance_monitor['last_monitor_time'] = current_time
    
    def check_thread_health_enhanced(self):
        """ã€å¢å¼ºç‰ˆã€‘æ£€æŸ¥çº¿ç¨‹å¥åº·çŠ¶æ€"""
        # æ£€æŸ¥rolloutçº¿ç¨‹
        dead_rollout_threads = [i for i, thread in enumerate(self.rollout_threads) if not thread.is_alive()]
        if dead_rollout_threads:
            self.logger.warning(f"å‘ç° {len(dead_rollout_threads)} ä¸ªæ­»äº¡çš„rolloutçº¿ç¨‹: {dead_rollout_threads}")
        
        # æ£€æŸ¥trainingçº¿ç¨‹
        dead_training_threads = [i for i, thread in enumerate(self.training_threads) if not thread.is_alive()]
        if dead_training_threads:
            self.logger.warning(f"å‘ç° {len(dead_training_threads)} ä¸ªæ­»äº¡çš„trainingçº¿ç¨‹: {dead_training_threads}")
        
        # ã€å¢å¼ºåŠŸèƒ½ã€‘æ£€æŸ¥æ•°æ®æµæ˜¯å¦æ­£å¸¸
        buffer_stats = self.data_buffer.get_stats()
        if buffer_stats['total_added'] == getattr(self, '_last_total_added', 0):
            self.logger.warning("æ•°æ®ç¼“å†²åŒºæ·»åŠ æ•°é‡æœªå¢åŠ ï¼Œå¯èƒ½rolloutçº¿ç¨‹æœ‰é—®é¢˜")
        if buffer_stats['total_consumed'] == getattr(self, '_last_total_consumed', 0):
            self.logger.warning("æ•°æ®ç¼“å†²åŒºæ¶ˆè´¹æ•°é‡æœªå¢åŠ ï¼Œå¯èƒ½trainingçº¿ç¨‹æœ‰é—®é¢˜")
        
        self._last_total_added = buffer_stats['total_added']
        self._last_total_consumed = buffer_stats['total_consumed']
        
        # ã€å¢å¼ºåŠŸèƒ½ã€‘æ£€æŸ¥ä»£ç†å­˜å‚¨çŠ¶æ€
        agent_stats = self.agent_proxy.get_storage_stats()
        if agent_stats['total_failures'] > agent_stats['total_successes'] * 0.1:  # å¤±è´¥ç‡è¶…è¿‡10%
            self.logger.warning(f"ä»£ç†å­˜å‚¨å¤±è´¥ç‡è¿‡é«˜: {agent_stats['total_failures']}/{agent_stats['total_attempts']}")
    
    def stop_training_enhanced(self):
        """ã€å¢å¼ºç‰ˆã€‘åœæ­¢è®­ç»ƒ"""
        self.logger.info("åœæ­¢ã€å¢å¼ºç‰ˆã€‘è®­ç»ƒ...")
        
        # è®¾ç½®åœæ­¢äº‹ä»¶
        self.control_events['stop'].set()
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹ç»“æŸ
        self.logger.info("ç­‰å¾…rolloutçº¿ç¨‹ç»“æŸ...")
        for i, thread in enumerate(self.rollout_threads):
            thread.join(timeout=15)  # å¢å¼ºç‰ˆï¼šæ›´é•¿ç­‰å¾…æ—¶é—´
            if thread.is_alive():
                self.logger.warning(f"Rolloutçº¿ç¨‹ {i} æœªèƒ½åœ¨15ç§’å†…ç»“æŸ")
        
        self.logger.info("ç­‰å¾…trainingçº¿ç¨‹ç»“æŸ...")
        for i, thread in enumerate(self.training_threads):
            thread.join(timeout=15)  # å¢å¼ºç‰ˆï¼šæ›´é•¿ç­‰å¾…æ—¶é—´
            if thread.is_alive():
                self.logger.warning(f"Trainingçº¿ç¨‹ {i} æœªèƒ½åœ¨15ç§’å†…ç»“æŸ")
        
        # ã€å¢å¼ºåŠŸèƒ½ã€‘å…³é—­çº¿ç¨‹å®‰å…¨ä»£ç†
        if hasattr(self, 'agent_proxy') and hasattr(self.agent_proxy, 'shutdown'):
            self.agent_proxy.shutdown()
        
        # ã€å¢å¼ºåŠŸèƒ½ã€‘å¼ºåˆ¶å¤‡ä»½ç¼“å†²åŒºæ•°æ®
        if hasattr(self, 'data_buffer'):
            self.data_buffer.force_backup()
        
        self.logger.info("æ‰€æœ‰ã€å¢å¼ºç‰ˆã€‘çº¿ç¨‹å·²åœæ­¢")
    
    def save_final_model(self):
        """ä¿å­˜æœ€ç»ˆæ¨¡å‹"""
        try:
            final_model_path = os.path.join(self.log_dir, 'enhanced_final_model.pt')
            self.agent.save_model(final_model_path)
            self.logger.info(f"ã€å¢å¼ºç‰ˆã€‘æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
        except Exception as e:
            self.logger.error(f"ä¿å­˜ã€å¢å¼ºç‰ˆã€‘æœ€ç»ˆæ¨¡å‹å¤±è´¥: {e}")
    
    def cleanup_enhanced(self):
        """ã€å¢å¼ºç‰ˆã€‘æ¸…ç†èµ„æº"""
        try:
            # 1. åœæ­¢è®­ç»ƒï¼ˆå¦‚æœè¿˜æ²¡åœæ­¢ï¼‰
            if not self.control_events['stop'].is_set():
                self.stop_training_enhanced()
            
            # 2. å…³é—­TensorBoard writer
            if hasattr(self.agent, 'writer') and self.agent.writer:
                try:
                    self.agent.writer.close()
                    self.logger.info("TensorBoard writerå·²å…³é—­")
                except Exception as e:
                    self.logger.warning(f"å…³é—­TensorBoard writeræ—¶å‡ºé”™: {e}")
            
            # 3. æ¸…ç†ä»£ç†ç¼“å†²åŒº
            if hasattr(self.agent, 'high_level_buffer'):
                self.agent.high_level_buffer.clear()
            if hasattr(self.agent, 'low_level_buffer'):
                self.agent.low_level_buffer.clear()
            if hasattr(self.agent, 'state_skill_dataset'):
                self.agent.state_skill_dataset.clear()
            
            # 4. ã€å¢å¼ºåŠŸèƒ½ã€‘æ¸…ç†åŸå­æ€§ç¼“å†²åŒº
            if hasattr(self, 'data_buffer'):
                cleared_count = self.data_buffer.clear()
                self.logger.info(f"åŸå­æ€§ç¼“å†²åŒºå·²æ¸…ç†: {cleared_count} é¡¹")
            
            # 5. ã€å¢å¼ºåŠŸèƒ½ã€‘è®°å½•æœ€ç»ˆæ€§èƒ½ç»Ÿè®¡
            self._log_final_performance_stats()
            
            self.logger.info("æ‰€æœ‰ã€å¢å¼ºç‰ˆã€‘èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            print(f"æ¸…ç†ã€å¢å¼ºç‰ˆã€‘èµ„æºæ—¶å‡ºé”™: {e}")
    
    def _log_final_performance_stats(self):
        """ã€å¢å¼ºåŠŸèƒ½ã€‘è®°å½•æœ€ç»ˆæ€§èƒ½ç»Ÿè®¡"""
        try:
            if hasattr(self, 'performance_monitor'):
                avg_rollout_speed = sum(self.performance_monitor['rollout_speeds']) / len(self.performance_monitor['rollout_speeds']) if self.performance_monitor['rollout_speeds'] else 0
                avg_buffer_util = sum(self.performance_monitor['buffer_utilizations']) / len(self.performance_monitor['buffer_utilizations']) if self.performance_monitor['buffer_utilizations'] else 0
                
                self.logger.info("=== ã€å¢å¼ºç‰ˆã€‘æœ€ç»ˆæ€§èƒ½ç»Ÿè®¡ ===")
                self.logger.info(f"å¹³å‡Rollouté€Ÿåº¦: {avg_rollout_speed:.1f} æ ·æœ¬/ç§’")
                self.logger.info(f"å¹³å‡ç¼“å†²åŒºåˆ©ç”¨ç‡: {avg_buffer_util:.1%}")
                
                # è·å–æœ€ç»ˆç¼“å†²åŒºç»Ÿè®¡
                buffer_stats = self.data_buffer.get_stats()
                self.logger.info(f"æœ€ç»ˆç¼“å†²åŒºç»Ÿè®¡: {buffer_stats}")
                
                # è·å–æœ€ç»ˆä»£ç†ç»Ÿè®¡
                agent_stats = self.agent_proxy.get_storage_stats()
                self.logger.info(f"æœ€ç»ˆä»£ç†ç»Ÿè®¡: {agent_stats}")
                
        except Exception as e:
            self.logger.error(f"è®°å½•æœ€ç»ˆæ€§èƒ½ç»Ÿè®¡å¤±è´¥: {e}")
    
    def train_enhanced(self, total_steps=100000):
        """
        ã€å¢å¼ºç‰ˆã€‘æ‰§è¡Œå®Œæ•´çš„å¤šçº¿ç¨‹rollout-basedè®­ç»ƒ
        
        å‚æ•°:
            total_steps: è®­ç»ƒæ€»æ­¥æ•°
        """
        self.logger.info(f"å¼€å§‹ã€å¢å¼ºç‰ˆã€‘HMASDå¤šçº¿ç¨‹Rollout-basedè®­ç»ƒ: {total_steps:,} æ­¥")
        self.logger.info(f"é…ç½®: {self.num_training_threads} è®­ç»ƒçº¿ç¨‹, {self.num_rollout_threads} rolloutçº¿ç¨‹")
        
        try:
            # åˆå§‹åŒ–ä»£ç†
            self.initialize_agent()
            
            # å¯åŠ¨rolloutçº¿ç¨‹
            self.start_rollout_threads()
            
            # ç­‰å¾…rolloutçº¿ç¨‹å¼€å§‹æ”¶é›†æ•°æ®
            time.sleep(5)
            
            # å¯åŠ¨trainingçº¿ç¨‹
            self.start_training_threads()
            
            # å¼€å§‹ç›‘æ§è®­ç»ƒ
            self.monitor_training_enhanced(total_steps)
            
        except KeyboardInterrupt:
            self.logger.info("ã€å¢å¼ºç‰ˆã€‘è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        except Exception as e:
            self.logger.error(f"ã€å¢å¼ºç‰ˆã€‘è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
        
        finally:
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            self.save_final_model()
            
            # æ¸…ç†èµ„æº
            self.cleanup_enhanced()
        
        # è®­ç»ƒå®Œæˆ
        if self.start_time:
            total_time = time.time() - self.start_time
            final_steps = sum(worker.samples_collected for worker in self.rollout_workers)
            self.logger.info(f"\nã€å¢å¼ºç‰ˆã€‘è®­ç»ƒå®Œæˆï¼")
            self.logger.info(f"æ€»æ—¶é—´: {total_time/3600:.2f}å°æ—¶")
            self.logger.info(f"æ€»æ­¥æ•°: {final_steps:,}")
            
            # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
            total_samples = sum(worker.samples_collected for worker in self.rollout_workers)
            total_episodes = sum(worker.episodes_completed for worker in self.rollout_workers)
            total_updates = sum(worker.updates_performed for worker in self.training_workers)
            total_validation_failures = sum(worker.validation_failures for worker in self.rollout_workers)
            
            self.logger.info(f"æ€»æ ·æœ¬æ•°: {total_samples:,}")
            self.logger.info(f"æ€»Episodes: {total_episodes:,}")
            self.logger.info(f"æ€»æ›´æ–°æ•°: {total_updates}")
            self.logger.info(f"æ€»éªŒè¯å¤±è´¥: {total_validation_failures}")
            
            if total_time > 0:
                self.logger.info(f"æ ·æœ¬æ”¶é›†é€Ÿåº¦: {total_samples/total_time:.1f} æ ·æœ¬/ç§’")
                self.logger.info(f"Episodeå®Œæˆé€Ÿåº¦: {total_episodes/total_time:.1f} episodes/ç§’")
                self.logger.info(f"æ­¥æ•°å®Œæˆé€Ÿåº¦: {final_steps/total_time:.1f} æ­¥/ç§’")

def parse_args_enhanced():
    """è§£æå‘½ä»¤è¡Œå‚æ•° - å¢å¼ºç‰ˆ"""
    parser = argparse.ArgumentParser(description='ã€å¢å¼ºç‰ˆã€‘HMASDå¤šçº¿ç¨‹Rollout-basedè®­ç»ƒï¼ˆé›†æˆä¸‰å¤§å¢å¼ºç»„ä»¶ï¼‰')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--steps', type=int, default=None, help='è®­ç»ƒæ€»æ­¥æ•°ï¼ˆå¦‚æœä¸æŒ‡å®šï¼Œå°†ä»config.pyä¸­è¯»å–ï¼‰')
    parser.add_argument('--device', type=str, default='auto', choices=['auto', 'cpu', 'cuda'])
    parser.add_argument('--debug', action='store_true', help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
    
    # çº¿ç¨‹é…ç½®ï¼ˆæŒ‰ç…§è®ºæ–‡ Appendix Eï¼‰
    parser.add_argument('--training_threads', type=int, default=16, help='è®­ç»ƒçº¿ç¨‹æ•°ï¼ˆè®ºæ–‡é»˜è®¤16ï¼‰')
    parser.add_argument('--rollout_threads', type=int, default=32, help='Rolloutçº¿ç¨‹æ•°ï¼ˆè®ºæ–‡é»˜è®¤32ï¼‰')
    parser.add_argument('--buffer_size', type=int, default=10000, help='æ•°æ®ç¼“å†²åŒºå¤§å°')
    
    # ã€å¢å¼ºåŠŸèƒ½ã€‘æ–°å¢å‚æ•°
    parser.add_argument('--enable_recovery', action='store_true', default=True, help='å¯ç”¨æ•…éšœæ¢å¤æœºåˆ¶')
    parser.add_argument('--enable_validation', action='store_true', default=True, help='å¯ç”¨æ•°æ®å®Œæ•´æ€§éªŒè¯')
    parser.add_argument('--enable_persistence', action='store_true', default=True, help='å¯ç”¨æ•°æ®æŒä¹…åŒ–')
    
    # ç¯å¢ƒå‚æ•°
    parser.add_argument('--scenario', type=int, default=2, choices=[1, 2], help='åœºæ™¯é€‰æ‹©')
    parser.add_argument('--n_uavs', type=int, default=5, help='æ— äººæœºæ•°é‡')
    parser.add_argument('--n_users', type=int, default=50, help='ç”¨æˆ·æ•°é‡')
    parser.add_argument('--user_distribution', type=str, default='uniform', 
                       choices=['uniform', 'cluster', 'hotspot'])
    parser.add_argument('--channel_model', type=str, default='3gpp-36777',
                       choices=['free_space', 'urban', 'suburban', '3gpp-36777'])
    parser.add_argument('--max_hops', type=int, default=3, help='æœ€å¤§è·³æ•°ï¼ˆä»…åœºæ™¯2ï¼‰')
    
    # æ—¥å¿—å‚æ•°
    parser.add_argument('--log_level', type=str, default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'])
    parser.add_argument('--console_log_level', type=str, default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'])
    
    return parser.parse_args()

def main():
    """ã€å¢å¼ºç‰ˆã€‘ä¸»å‡½æ•°"""
    args = parse_args_enhanced()
    
    # åˆ›å»ºé…ç½®
    config = Config()
    
    # ç¡®å®šè®­ç»ƒæ­¥æ•°ï¼šä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œå…¶æ¬¡ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼
    if args.steps is not None:
        total_steps = args.steps
        print(f"ğŸ“ˆ ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„è®­ç»ƒæ­¥æ•°: {total_steps:,}")
    else:
        total_steps = int(config.total_timesteps)
        print(f"ğŸ“ˆ ä»config.pyè¯»å–è®­ç»ƒæ­¥æ•°: {total_steps:,}")
    
    print("ğŸš€ ã€å¢å¼ºç‰ˆã€‘HMASDå¤šçº¿ç¨‹Rollout-basedè®­ç»ƒï¼ˆé›†æˆä¸‰å¤§å¢å¼ºç»„ä»¶ï¼‰")
    print("=" * 80)
    print(f"ğŸ“Š çº¿ç¨‹é…ç½®: {args.training_threads} è®­ç»ƒçº¿ç¨‹ + {args.rollout_threads} rolloutçº¿ç¨‹")
    print(f"ğŸ¯ è®­ç»ƒæ­¥æ•°: {total_steps:,}")
    print(f"ğŸ—‚ï¸ ç¼“å†²åŒºå¤§å°: {args.buffer_size}")
    print(f"ğŸ”§ æ•…éšœæ¢å¤: {args.enable_recovery}")
    print(f"âœ… æ•°æ®éªŒè¯: {args.enable_validation}")
    print(f"ğŸ’¾ æ•°æ®æŒä¹…åŒ–: {args.enable_persistence}")
    
    # éªŒè¯å¹¶æ‰“å°é…ç½®
    config.validate_training_mode()
    config.validate_rollout_config()
    print(config.get_rollout_summary())
    
    try:
        # åˆ›å»ºå¢å¼ºç‰ˆè®­ç»ƒå™¨
        trainer = EnhancedThreadedRolloutTrainer(config, args)
        
        # å¼€å§‹è®­ç»ƒ
        trainer.train_enhanced(total_steps=total_steps)
        
        print("ğŸ‰ ã€å¢å¼ºç‰ˆã€‘è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ã€å¢å¼ºç‰ˆã€‘è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ã€å¢å¼ºç‰ˆã€‘è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        try:
            shutdown_logging()
        except:
            pass

if __name__ == "__main__":
    main()

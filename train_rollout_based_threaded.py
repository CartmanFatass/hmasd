#!/usr/bin/env python3
"""
HMASDä¸¥æ ¼æŒ‰è®ºæ–‡Algorithm 1 + Appendix Eçš„å¤šçº¿ç¨‹Rollout-basedè®­ç»ƒè„šæœ¬
å®ç°è¦ç‚¹ï¼š
1. 32ä¸ªrollout threads: æŒç»­ç¯å¢ƒäº¤äº’å’Œæ•°æ®æ”¶é›†
2. 16ä¸ªtraining threads: æŒç»­ç¥ç»ç½‘ç»œè®­ç»ƒ
3. çº¿ç¨‹å®‰å…¨çš„æ•°æ®ä¼ è¾“: ä½¿ç”¨é˜Ÿåˆ—åœ¨rolloutå’Œtrainingçº¿ç¨‹é—´ä¼ è¾“æ•°æ®
4. å¼‚æ­¥è®­ç»ƒ: æ•°æ®æ”¶é›†å’Œæ¨¡å‹è®­ç»ƒå¹¶è¡Œæ‰§è¡Œ
5. ä¸¥æ ¼æŒ‰ç…§è®ºæ–‡: 15è½®PPO + åˆ¤åˆ«å™¨è®­ç»ƒ + ç¼“å†²åŒºç®¡ç†
"""

import os
import sys
import time
import numpy as np
import torch
import argparse
import threading
import queue
import multiprocessing as mp
from datetime import datetime
from collections import defaultdict, deque
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock, Event, Barrier
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# è®¾ç½®matplotlibåç«¯ï¼ˆé¿å…å¤šè¿›ç¨‹é—®é¢˜ï¼‰
import matplotlib
matplotlib.use('Agg')

from logger import get_logger, init_multiproc_logging, LOG_LEVELS, shutdown_logging
from config import Config
from hmasd.agent import HMASDAgent
from envs.pettingzoo.scenario1 import UAVBaseStationEnv
from envs.pettingzoo.scenario2 import UAVCooperativeNetworkEnv
from envs.pettingzoo.env_adapter import ParallelToArrayAdapter

# å¯¼å…¥ Stable Baselines3 çš„å‘é‡åŒ–ç¯å¢ƒ
from stable_baselines3.common.vec_env import SubprocVecEnv

class ThreadSafeCounter:
    """çº¿ç¨‹å®‰å…¨çš„è®¡æ•°å™¨"""
    def __init__(self, initial_value=0):
        self._value = initial_value
        self._lock = Lock()
    
    def increment(self, amount=1):
        with self._lock:
            self._value += amount
            return self._value
    
    def get(self):
        with self._lock:
            return self._value
    
    def set(self, value):
        with self._lock:
            self._value = value

class DataBuffer:
    """çº¿ç¨‹å®‰å…¨çš„æ•°æ®ç¼“å†²åŒº"""
    def __init__(self, maxsize=10000):
        self.queue = queue.Queue(maxsize=maxsize)
        self.total_added = ThreadSafeCounter()
        self.total_consumed = ThreadSafeCounter()
        
    def put(self, item, block=True, timeout=None):
        """æ·»åŠ æ•°æ®åˆ°ç¼“å†²åŒº"""
        try:
            self.queue.put(item, block=block, timeout=timeout)
            self.total_added.increment()
            return True
        except queue.Full:
            return False
    
    def get(self, block=True, timeout=None):
        """ä»ç¼“å†²åŒºè·å–æ•°æ®"""
        try:
            item = self.queue.get(block=block, timeout=timeout)
            self.total_consumed.increment()
            return item
        except queue.Empty:
            return None
    
    def qsize(self):
        """è·å–å½“å‰é˜Ÿåˆ—å¤§å°"""
        return self.queue.qsize()
    
    def empty(self):
        """æ£€æŸ¥é˜Ÿåˆ—æ˜¯å¦ä¸ºç©º"""
        return self.queue.empty()
    
    def get_stats(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'queue_size': self.qsize(),
            'total_added': self.total_added.get(),
            'total_consumed': self.total_consumed.get()
        }

class RolloutWorker:
    """å•ä¸ªrollout workerï¼Œåœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œ"""
    def __init__(self, worker_id, env_factory, config, data_buffer, control_events, logger):
        self.worker_id = worker_id
        self.env_factory = env_factory
        self.config = config
        self.data_buffer = data_buffer
        self.control_events = control_events
        self.logger = logger
        
        # åˆ›å»ºç¯å¢ƒ
        self.env = env_factory()
        
        # çŠ¶æ€å˜é‡
        self.samples_collected = 0
        self.episodes_completed = 0
        self.total_reward = 0.0
        
        # ã€å…³é”®ä¿®å¤ã€‘æ·»åŠ rolloutå®Œæˆæ§åˆ¶
        self.rollout_completed = False
        self.target_rollout_steps = config.rollout_length  # æ¯ä¸ªworkerçš„ç›®æ ‡æ­¥æ•°ï¼ˆ128ï¼‰
        
        # ç¯å¢ƒçŠ¶æ€
        self.env_state = None
        self.env_observations = None
        self.episode_step = 0
        
        # ã€æ–°å¢ã€‘æŠ€èƒ½å‘¨æœŸç®¡ç†
        self.skill_timer = 0
        self.accumulated_reward = 0.0
        self.current_team_skill = None
        self.current_agent_skills = None
        self.skill_log_probs = None
        self.high_level_experiences_generated = 0
        self.last_skill_assignment_step = 0
        
    def reset_environment(self):
        """é‡ç½®ç¯å¢ƒ"""
        try:
            result = self.env.reset()
            if isinstance(result, tuple):
                self.env_observations, info = result
                self.env_state = info.get('state', np.zeros(self.config.state_dim))
            else:
                self.env_observations = result
                self.env_state = np.zeros(self.config.state_dim)
            self.episode_step = 0
            return True
        except Exception as e:
            self.logger.error(f"Worker {self.worker_id}: é‡ç½®ç¯å¢ƒå¤±è´¥: {e}")
            return False
    
    def step_environment(self, actions):
        """æ‰§è¡Œç¯å¢ƒæ­¥éª¤"""
        try:
            # ä¿®å¤ï¼šå¤„ç†Gymnasium APIçš„5ä¸ªè¿”å›å€¼
            result = self.env.step(actions)
            
            if len(result) == 5:
                # Gymnasiumæ ¼å¼: observations, reward, terminated, truncated, info
                next_observations, rewards, terminated, truncated, infos = result
                dones = terminated or truncated  # åˆå¹¶ç»ˆæ­¢æ¡ä»¶
            elif len(result) == 4:
                # ä¼ ç»Ÿæ ¼å¼: observations, rewards, dones, infos
                next_observations, rewards, dones, infos = result
            else:
                raise ValueError(f"Unexpected number of return values from env.step(): {len(result)}")
            
            # ä»infoä¸­æå–next_state
            if isinstance(infos, dict):
                next_state = infos.get('next_state', np.zeros(self.config.state_dim))
            elif isinstance(infos, list) and len(infos) > 0:
                next_state = infos[0].get('next_state', np.zeros(self.config.state_dim))
            else:
                next_state = np.zeros(self.config.state_dim)
            
            self.episode_step += 1
            return next_observations, rewards, dones, next_state
            
        except Exception as e:
            self.logger.error(f"Worker {self.worker_id}: ç¯å¢ƒæ­¥éª¤å¤±è´¥: {e}")
            # è¿”å›å®‰å…¨çš„é»˜è®¤å€¼è€Œä¸æ˜¯None
            n_agents = len(actions) if hasattr(actions, '__len__') else self.config.n_agents
            default_obs = np.zeros((n_agents, self.config.obs_dim))
            return default_obs, 0.0, True, np.zeros(self.config.state_dim)
    
    def run(self, agent_proxy):
        """è¿è¡Œrollout workerä¸»å¾ªç¯ - ä¿®å¤ç‰ˆæœ¬ï¼šæ·»åŠ ä¸¥æ ¼çš„æ­¥æ•°æ§åˆ¶"""
        self.logger.info(f"Rollout worker {self.worker_id} å¼€å§‹è¿è¡Œï¼Œç›®æ ‡æ”¶é›†æ­¥æ•°: {self.target_rollout_steps}")
        
        # é‡ç½®ç¯å¢ƒ
        if not self.reset_environment():
            self.logger.error(f"Worker {self.worker_id}: åˆå§‹åŒ–å¤±è´¥")
            return
        
        try:
            # ã€å…³é”®ä¿®å¤ã€‘æ— é™å¾ªç¯è®­ç»ƒæ¨¡å¼ï¼šæ¯æ¬¡å®Œæˆä¸€ä¸ªrolloutåç­‰å¾…æ–°çš„å‘¨æœŸå¼€å§‹
            while not self.control_events['stop'].is_set():
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æš‚åœ
                if self.control_events['pause'].is_set():
                    time.sleep(0.1)
                    continue
                
                # ã€å…³é”®ä¿®å¤ã€‘å¦‚æœå½“å‰rolloutå·²å®Œæˆï¼Œç­‰å¾…æ–°çš„rolloutå‘¨æœŸ
                if self.rollout_completed:
                    time.sleep(0.1)  # ç­‰å¾…è®­ç»ƒå®Œæˆå’ŒçŠ¶æ€é‡ç½®
                    continue
                
                # ã€å…³é”®ä¿®å¤ã€‘æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°rolloutæ­¥æ•°é™åˆ¶
                if self.samples_collected >= self.target_rollout_steps:
                    self.rollout_completed = True
                    self.logger.info(f"ğŸ”„ Worker {self.worker_id} å®Œæˆrollout: "
                                   f"{self.samples_collected}/{self.target_rollout_steps} æ­¥")
                    continue
                
                # æ‰§è¡Œä¸€ä¸ªrolloutæ­¥éª¤
                success = self.run_step(agent_proxy)
                if not success:
                    self.logger.warning(f"Worker {self.worker_id}: æ­¥éª¤æ‰§è¡Œå¤±è´¥ï¼Œé‡ç½®ç¯å¢ƒ")
                    if not self.reset_environment():
                        break
                
                # çŸ­æš‚ç¡çœ é¿å…è¿‡åº¦å ç”¨CPU
                time.sleep(0.001)
        
        except Exception as e:
            self.logger.error(f"Worker {self.worker_id}: è¿è¡Œå¼‚å¸¸: {e}")
        finally:
            try:
                self.env.close()
            except:
                pass
            self.logger.info(f"Rollout worker {self.worker_id} ç»“æŸè¿è¡Œ")
    
    def run_step(self, agent_proxy):
        """æ‰§è¡Œå•ä¸ªrolloutæ­¥éª¤ - ä¿®å¤ç‰ˆæœ¬ï¼šç¡®ä¿æ¯æ¬¡éƒ½æ­£ç¡®è®¡æ•°"""
        
        try:
            # ã€ä¿®å¤ã€‘æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åˆ†é…æŠ€èƒ½
            if self.should_reassign_skills():
                # åœ¨é‡åˆ†é…å‰ï¼Œå…ˆå­˜å‚¨ä¸Šä¸€å‘¨æœŸçš„é«˜å±‚ç»éªŒï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                if (self.current_team_skill is not None and 
                    self.skill_timer >= self.config.k and 
                    self.accumulated_reward != 0):
                    self.store_high_level_experience("æŠ€èƒ½å‘¨æœŸå®Œæˆ")
                
                # é‡æ–°åˆ†é…æŠ€èƒ½
                self.assign_new_skills(agent_proxy)
            
            # ä»ä»£ç†è·å–åŠ¨ä½œï¼ˆä½¿ç”¨workerè‡ªå·±çš„æŠ€èƒ½çŠ¶æ€ï¼‰
            actions, action_logprobs = agent_proxy.get_actions_for_worker(
                self.env_state, self.env_observations, self.current_agent_skills, self.worker_id
            )
            
            # æ‰§è¡Œç¯å¢ƒæ­¥éª¤
            next_observations, rewards, dones, next_state = self.step_environment(actions)
            
            # ã€å…³é”®ä¿®å¤ã€‘æ— è®ºåç»­å¤„ç†å¦‚ä½•ï¼Œç¯å¢ƒæ­¥éª¤å·²æ‰§è¡Œï¼Œå¿…é¡»è®¡æ•°
            self.samples_collected += 1
            
            # ã€ä¿®å¤A2ã€‘ç¡®ä¿rewardsæ˜¯æœ‰æ•ˆçš„æ•°å€¼
            if rewards is None:
                current_reward = 0.0
                self.logger.warning(f"Worker {self.worker_id}: ç¯å¢ƒæ­¥éª¤è¿”å›Noneå¥–åŠ±ï¼Œä½¿ç”¨0.0")
            else:
                current_reward = rewards if isinstance(rewards, (int, float)) else np.sum(rewards)
            
            # ã€ä¿®å¤A3ã€‘ç´¯ç§¯å¥–åŠ±å’Œæ›´æ–°æŠ€èƒ½è®¡æ—¶å™¨
            self.accumulated_reward += current_reward
            self.skill_timer += 1
            self.total_reward += current_reward
            
            # æ„é€ ä½å±‚ç»éªŒæ•°æ®
            low_level_experience = {
                'experience_type': 'low_level',
                'worker_id': self.worker_id,
                'state': self.env_state.copy(),
                'observations': self.env_observations.copy(),
                'actions': actions.copy(),
                'rewards': current_reward,
                'next_state': next_state.copy(),
                'next_observations': next_observations.copy(),
                'dones': dones,
                'episode_step': self.episode_step,
                'team_skill': self.current_team_skill,
                'agent_skills': self.current_agent_skills.copy() if self.current_agent_skills is not None else None,
                'action_logprobs': action_logprobs,
                'skill_log_probs': self.skill_log_probs
            }
            
            # ã€ä¿®å¤A4ã€‘å°†ä½å±‚ç»éªŒæ”¾å…¥ç¼“å†²åŒºï¼Œè®°å½•æ˜¯å¦æˆåŠŸ
            buffer_success = self.data_buffer.put(low_level_experience, block=False)
            if not buffer_success:
                self.logger.warning(f"Worker {self.worker_id}: æ•°æ®ç¼“å†²åŒºæ»¡ï¼Œä¸¢å¼ƒä½å±‚æ ·æœ¬")
            
            # ã€ä¿®å¤ã€‘åªåœ¨ç¯å¢ƒç»ˆæ­¢æ—¶å­˜å‚¨é«˜å±‚ç»éªŒï¼ˆæŠ€èƒ½å‘¨æœŸå®Œæˆçš„æƒ…å†µå·²åœ¨é‡åˆ†é…æ—¶å¤„ç†ï¼‰
            if dones:
                self.store_high_level_experience("ç¯å¢ƒç»ˆæ­¢")
            
            # æ›´æ–°ç¯å¢ƒçŠ¶æ€
            self.env_state = next_state
            self.env_observations = next_observations
            
            # æ£€æŸ¥episodeæ˜¯å¦ç»“æŸ
            if dones or self.episode_step >= 1000:  # æœ€å¤§æ­¥æ•°é™åˆ¶
                self.episodes_completed += 1
                self.logger.debug(f"Worker {self.worker_id}: Episode {self.episodes_completed} å®Œæˆ, "
                                f"æ­¥æ•°: {self.episode_step}, å¥–åŠ±: {self.total_reward:.2f}")
                
                # å¼ºåˆ¶å­˜å‚¨ä»»ä½•pendingçš„é«˜å±‚ç»éªŒ
                if self.skill_timer > 0 and self.accumulated_reward != 0:
                    self.store_high_level_experience("Episodeç»“æŸ")
                
                # é‡ç½®ç¯å¢ƒå’ŒæŠ€èƒ½çŠ¶æ€
                if not self.reset_environment():
                    return False
                self.reset_skill_state()
                self.total_reward = 0.0
            
            return True
            
        except Exception as e:
            self.logger.error(f"Worker {self.worker_id}: æ­¥éª¤æ‰§è¡Œå¼‚å¸¸: {e}")
            # ã€å…³é”®ä¿®å¤ã€‘å³ä½¿å¼‚å¸¸ï¼Œç¯å¢ƒæ­¥éª¤ä¹Ÿå·²æ‰§è¡Œï¼Œå¿…é¡»è®¡æ•°
            self.samples_collected += 1
            return False
    
    def should_reassign_skills(self):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åˆ†é…æŠ€èƒ½"""
        return (self.skill_timer >= self.config.k or 
                self.current_team_skill is None)
    
    def should_store_high_level_experience(self, dones):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦å­˜å‚¨é«˜å±‚ç»éªŒ - ä¿®å¤ç‰ˆæœ¬ï¼šåªåœ¨æŠ€èƒ½å‘¨æœŸå®Œæˆæˆ–ç¯å¢ƒç»ˆæ­¢æ—¶å­˜å‚¨"""
        # åªåœ¨ä»¥ä¸‹æƒ…å†µå­˜å‚¨é«˜å±‚ç»éªŒï¼š
        # 1. æŠ€èƒ½å‘¨æœŸå®Œæˆï¼ˆè¾¾åˆ°kæ­¥ï¼‰
        # 2. ç¯å¢ƒç»ˆæ­¢ï¼ˆepisodeç»“æŸï¼‰
        return (self.skill_timer >= self.config.k or dones)
    
    def assign_new_skills(self, agent_proxy):
        """é‡æ–°åˆ†é…æŠ€èƒ½"""
        try:
            team_skill, agent_skills, log_probs = agent_proxy.assign_skills_for_worker(
                self.env_state, self.env_observations, self.worker_id
            )
            
            self.current_team_skill = team_skill
            self.current_agent_skills = agent_skills
            self.skill_log_probs = log_probs
            self.skill_timer = 0
            self.accumulated_reward = 0.0
            self.last_skill_assignment_step = self.episode_step
            
            self.logger.debug(f"Worker {self.worker_id}: æŠ€èƒ½é‡æ–°åˆ†é… - "
                            f"team_skill={team_skill}, agent_skills={agent_skills}")
            
        except Exception as e:
            self.logger.error(f"Worker {self.worker_id}: æŠ€èƒ½åˆ†é…å¤±è´¥: {e}")
            # ä½¿ç”¨é»˜è®¤æŠ€èƒ½ä½œä¸ºå›é€€
            self.current_team_skill = 0
            self.current_agent_skills = [0] * self.config.n_agents
            self.skill_log_probs = {'team_log_prob': 0.0, 'agent_log_probs': [0.0] * self.config.n_agents}
    
    def store_high_level_experience(self, reason="æŠ€èƒ½å‘¨æœŸå®Œæˆ"):
        """å­˜å‚¨é«˜å±‚ç»éªŒåˆ°æ•°æ®ç¼“å†²åŒº"""
        if self.current_team_skill is None or self.current_agent_skills is None:
            self.logger.warning(f"Worker {self.worker_id}: æŠ€èƒ½çŠ¶æ€æ— æ•ˆï¼Œè·³è¿‡é«˜å±‚ç»éªŒå­˜å‚¨")
            return False
        
        try:
            high_level_experience = {
                'experience_type': 'high_level',
                'worker_id': self.worker_id,
                'state': self.env_state.copy(),
                'team_skill': self.current_team_skill,
                'observations': self.env_observations.copy(),
                'agent_skills': self.current_agent_skills.copy(),
                'accumulated_reward': self.accumulated_reward,
                'skill_log_probs': self.skill_log_probs.copy() if self.skill_log_probs else None,
                'skill_timer': self.skill_timer,
                'episode_step': self.episode_step,
                'reason': reason
            }
            
            # å°†é«˜å±‚ç»éªŒæ”¾å…¥ç¼“å†²åŒº
            if self.data_buffer.put(high_level_experience, block=False):
                self.high_level_experiences_generated += 1
                self.logger.debug(f"Worker {self.worker_id}: é«˜å±‚ç»éªŒå·²å­˜å‚¨ - "
                                f"ç´¯ç§¯å¥–åŠ±={self.accumulated_reward:.4f}, åŸå› ={reason}, "
                                f"æ€»ç”Ÿæˆæ•°={self.high_level_experiences_generated}")
                
                # é‡ç½®ç´¯ç§¯çŠ¶æ€
                self.accumulated_reward = 0.0
                self.skill_timer = 0
                return True
            else:
                self.logger.warning(f"Worker {self.worker_id}: æ•°æ®ç¼“å†²åŒºæ»¡ï¼Œä¸¢å¼ƒé«˜å±‚ç»éªŒ")
                return False
                
        except Exception as e:
            self.logger.error(f"Worker {self.worker_id}: å­˜å‚¨é«˜å±‚ç»éªŒå¤±è´¥: {e}")
            return False
    
    def reset_skill_state(self):
        """é‡ç½®æŠ€èƒ½çŠ¶æ€"""
        self.skill_timer = 0
        self.accumulated_reward = 0.0
        self.current_team_skill = None
        self.current_agent_skills = None
        self.skill_log_probs = None
        self.last_skill_assignment_step = 0
    
    def get_worker_stats(self):
        """è·å–workerç»Ÿè®¡ä¿¡æ¯"""
        return {
            'worker_id': self.worker_id,
            'samples_collected': self.samples_collected,
            'episodes_completed': self.episodes_completed,
            'high_level_experiences_generated': self.high_level_experiences_generated,
            'current_skill_timer': self.skill_timer,
            'current_accumulated_reward': self.accumulated_reward,
            'current_team_skill': self.current_team_skill,
            'current_episode_step': self.episode_step
        }

class AgentProxy:
    """ä»£ç†ä»£ç†ï¼Œä¸ºrollout workersæä¾›çº¿ç¨‹å®‰å…¨çš„ä»£ç†æ¥å£"""
    def __init__(self, agent, config, logger):
        self.agent = agent
        self.config = config
        self.logger = logger
        self.lock = Lock()
        
        # å…¨å±€rolloutæ­¥æ•°è®¡æ•°å™¨ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦åº”è¯¥æ›´æ–°ï¼‰
        self.global_rollout_steps = 0
        self.high_level_experiences_stored = 0
        self.low_level_experiences_stored = 0
    
    def assign_skills_for_worker(self, state, observations, worker_id):
        """ä¸ºç‰¹å®šworkeråˆ†é…æŠ€èƒ½"""
        with self.lock:
            try:
                team_skill, agent_skills, log_probs = self.agent.assign_skills(
                    state, observations, deterministic=False
                )
                
                self.logger.debug(f"Worker {worker_id}: æŠ€èƒ½åˆ†é… - "
                                f"team_skill={team_skill}, agent_skills={agent_skills}")
                
                return team_skill, agent_skills, log_probs
                
            except Exception as e:
                self.logger.error(f"Worker {worker_id}: æŠ€èƒ½åˆ†é…å¤±è´¥: {e}")
                # è¿”å›é»˜è®¤æŠ€èƒ½
                return 0, [0] * self.config.n_agents, {
                    'team_log_prob': 0.0, 
                    'agent_log_probs': [0.0] * self.config.n_agents
                }
    
    def get_actions_for_worker(self, state, observations, agent_skills, worker_id):
        """ä¸ºç‰¹å®šworkerè·å–åŠ¨ä½œï¼ˆä½¿ç”¨ç»™å®šçš„æŠ€èƒ½ï¼‰"""
        with self.lock:
            try:
                actions, action_logprobs = self.agent.select_action(
                    observations, agent_skills, deterministic=False, env_id=worker_id
                )
                
                return actions, action_logprobs
                
            except Exception as e:
                self.logger.error(f"Worker {worker_id}: è·å–åŠ¨ä½œå¤±è´¥: {e}")
                # è¿”å›éšæœºåŠ¨ä½œä½œä¸ºå›é€€
                n_agents = len(observations)
                random_actions = np.random.randn(n_agents, self.config.action_dim)
                return random_actions, np.zeros(n_agents)
    
    def store_experience(self, experience_batch):
        """æ‰¹é‡å­˜å‚¨ç»éªŒåˆ°ä»£ç† - ä¿®å¤ç‰ˆæœ¬ï¼šç¡®ä¿æ­¥æ•°è®¡æ•°ç»Ÿä¸€"""
        with self.lock:
            stored_count = 0
            low_level_stored = 0
            high_level_stored = 0
            
            for experience in experience_batch:
                try:
                    worker_id = experience['worker_id']
                    experience_type = experience.get('experience_type', 'low_level')
                    
                    if experience_type == 'high_level':
                        # å­˜å‚¨é«˜å±‚ç»éªŒ
                        success = self.store_high_level_experience(experience)
                        if success:
                            self.high_level_experiences_stored += 1
                            high_level_stored += 1
                            stored_count += 1
                            self.logger.debug(f"Worker {worker_id}: é«˜å±‚ç»éªŒå·²å­˜å‚¨åˆ°ä»£ç† - "
                                            f"ç´¯ç§¯å¥–åŠ±={experience['accumulated_reward']:.4f}")
                    
                    elif experience_type == 'low_level':
                        # å­˜å‚¨ä½å±‚ç»éªŒ
                        success = self.store_low_level_experience(experience)
                        if success:
                            self.low_level_experiences_stored += 1
                            low_level_stored += 1
                            stored_count += 1
                            # ã€å…³é”®ä¿®å¤ã€‘æ¯ä¸ªæˆåŠŸçš„ä½å±‚ç»éªŒå¯¹åº”ä¸€ä¸ªç¯å¢ƒæ­¥éª¤
                            self.global_rollout_steps += 1
                    
                    else:
                        self.logger.warning(f"æœªçŸ¥ç»éªŒç±»å‹: {experience_type}")
                    
                except Exception as e:
                    self.logger.error(f"å­˜å‚¨ç»éªŒå¤±è´¥: {e}")
            
            # ã€å…³é”®ä¿®å¤ã€‘åŒæ­¥ä»£ç†çš„æ­¥æ•°è®¡æ•°å™¨
            old_steps = self.agent.steps_collected
            self.agent.steps_collected = self.global_rollout_steps
            
            # ã€è¯¦ç»†è°ƒè¯•ã€‘è®°å½•æ­¥æ•°åŒæ­¥æƒ…å†µ
            if self.global_rollout_steps % 100 == 0 and low_level_stored > 0:
                self.logger.info(f"[STEP_SYNC] æ­¥æ•°åŒæ­¥: agent.steps_collected {old_steps}â†’{self.agent.steps_collected}, "
                                f"global_rollout_steps={self.global_rollout_steps}, "
                                f"æœ¬æ‰¹æ¬¡ä½å±‚æ ·æœ¬={low_level_stored}")
            
            return stored_count
    
    def store_high_level_experience(self, experience):
        """å­˜å‚¨é«˜å±‚ç»éªŒåˆ°ä»£ç†"""
        try:
            # è°ƒç”¨ä»£ç†çš„é«˜å±‚ç»éªŒå­˜å‚¨æ–¹æ³•
            success = self.agent.store_high_level_transition(
                state=experience['state'],
                team_skill=experience['team_skill'],
                observations=experience['observations'],
                agent_skills=experience['agent_skills'],
                accumulated_reward=experience['accumulated_reward'],
                skill_log_probs=experience['skill_log_probs'],
                worker_id=experience['worker_id']
            )
            return success
        except Exception as e:
            self.logger.error(f"å­˜å‚¨é«˜å±‚ç»éªŒå¤±è´¥: {e}")
            return False
    
    def store_low_level_experience(self, experience):
        """å­˜å‚¨ä½å±‚ç»éªŒåˆ°ä»£ç†"""
        try:
            # è°ƒç”¨ä»£ç†çš„ä½å±‚ç»éªŒå­˜å‚¨æ–¹æ³•
            success = self.agent.store_low_level_transition(
                state=experience['state'],
                next_state=experience['next_state'],
                observations=experience['observations'],
                next_observations=experience['next_observations'],
                actions=experience['actions'],
                rewards=experience['rewards'],
                dones=experience['dones'],
                team_skill=experience['team_skill'],
                agent_skills=experience['agent_skills'],
                action_logprobs=experience['action_logprobs'],
                skill_log_probs=experience['skill_log_probs'],
                worker_id=experience['worker_id']
            )
            return success
        except Exception as e:
            self.logger.error(f"å­˜å‚¨ä½å±‚ç»éªŒå¤±è´¥: {e}")
            return False
    
    def should_update(self):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ›´æ–° - ä¿®å¤ç‰ˆæœ¬ï¼šåŸºäºæ‰€æœ‰workerså®ŒæˆçŠ¶æ€åˆ¤æ–­"""
        with self.lock:
            # ã€å…³é”®ä¿®å¤ã€‘æ£€æŸ¥æ‰€æœ‰rollout workersæ˜¯å¦éƒ½å®Œæˆäº†rollout
            if not hasattr(self, 'rollout_workers'):
                # å¦‚æœè¿˜æ²¡æœ‰rollout_workerså¼•ç”¨ï¼Œä½¿ç”¨æ—§é€»è¾‘ä½œä¸ºå›é€€
                self.agent.steps_collected = self.global_rollout_steps
                return self.agent.should_rollout_update()
            
            # ç»Ÿè®¡å®Œæˆçš„workersæ•°é‡
            completed_workers = sum(1 for worker in self.rollout_workers 
                                  if getattr(worker, 'rollout_completed', False))
            total_workers = len(self.rollout_workers)
            
            # æ‰€æœ‰workerséƒ½å®Œæˆrolloutæ—¶æ‰å¼€å§‹æ›´æ–°
            all_completed = completed_workers == total_workers
            
            if all_completed:
                # è®¡ç®—æ€»æ”¶é›†æ­¥æ•°
                total_collected = sum(worker.samples_collected for worker in self.rollout_workers)
                target_steps = self.rollout_workers[0].target_rollout_steps * total_workers
                
                self.logger.info(f"ğŸ¯ æ‰€æœ‰workerså®Œæˆrollout: {completed_workers}/{total_workers}, "
                               f"æ€»æ”¶é›†æ­¥æ•°: {total_collected}/{target_steps}")
                
                # åŒæ­¥ä»£ç†æ­¥æ•°è®¡æ•°å™¨
                self.agent.steps_collected = self.global_rollout_steps
                return True
            
            # è®°å½•è¿›åº¦ï¼ˆæ¯50æ¬¡æ£€æŸ¥è®°å½•ä¸€æ¬¡ï¼‰
            if not hasattr(self, '_update_check_count'):
                self._update_check_count = 0
            self._update_check_count += 1
            
            if self._update_check_count % 50 == 0:
                self.logger.debug(f"â³ ç­‰å¾…rolloutå®Œæˆ: {completed_workers}/{total_workers} workerså®Œæˆ")
            
            return False
    
    def update(self):
        """æ‰§è¡Œæ¨¡å‹æ›´æ–°"""
        with self.lock:
            try:
                update_info = self.agent.rollout_update()
                
                # ã€å…³é”®ä¿®å¤ã€‘è®­ç»ƒå®Œæˆåé‡ç½®æ‰€æœ‰workersçš„rolloutçŠ¶æ€
                if update_info:
                    self.reset_all_workers_rollout_state()
                
                return update_info
            except Exception as e:
                self.logger.error(f"æ¨¡å‹æ›´æ–°å¤±è´¥: {e}")
                return None
    
    def reset_all_workers_rollout_state(self):
        """é‡ç½®æ‰€æœ‰workersçš„rolloutçŠ¶æ€ï¼Œå‡†å¤‡ä¸‹ä¸€ä¸ªrolloutå‘¨æœŸ"""
        if not hasattr(self, 'rollout_workers'):
            return
        
        reset_count = 0
        for worker in self.rollout_workers:
            # é‡ç½®æ¯ä¸ªworkerçš„rolloutç›¸å…³çŠ¶æ€
            worker.samples_collected = 0
            worker.rollout_completed = False
            reset_count += 1
        
        self.logger.info(f"ğŸ”„ å·²é‡ç½® {reset_count} ä¸ªworkersçš„rolloutçŠ¶æ€ï¼Œå‡†å¤‡æ–°çš„rolloutå‘¨æœŸ")

class TrainingWorker:
    """è®­ç»ƒworkerï¼Œåœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œ"""
    def __init__(self, worker_id, agent_proxy, data_buffer, control_events, logger, config):
        self.worker_id = worker_id
        self.agent_proxy = agent_proxy
        self.data_buffer = data_buffer
        self.control_events = control_events
        self.logger = logger
        self.config = config
        
        # è®­ç»ƒç»Ÿè®¡
        self.updates_performed = 0
        self.samples_processed = 0
        self.last_update_time = time.time()
        
    def run(self):
        """è¿è¡Œè®­ç»ƒworkerä¸»å¾ªç¯"""
        self.logger.info(f"Training worker {self.worker_id} å¼€å§‹è¿è¡Œ")
        
        experience_batch = []
        batch_size = 32  # æ‰¹å¤„ç†å¤§å°
        
        try:
            while not self.control_events['stop'].is_set():
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æš‚åœ
                if self.control_events['pause'].is_set():
                    time.sleep(0.1)
                    continue
                
                # æ”¶é›†ç»éªŒæ•°æ®
                experience = self.data_buffer.get(block=True, timeout=1.0)
                if experience is None:
                    continue  # è¶…æ—¶ï¼Œç»§ç»­å¾ªç¯
                
                experience_batch.append(experience)
                
                # è¾¾åˆ°æ‰¹å¤§å°æˆ–æ•°æ®å……è¶³æ—¶å¤„ç†
                if (len(experience_batch) >= batch_size or 
                    (len(experience_batch) > 0 and self.data_buffer.qsize() < batch_size)):
                    
                    # å­˜å‚¨ç»éªŒåˆ°ä»£ç†
                    stored_count = self.agent_proxy.store_experience(experience_batch)
                    self.samples_processed += stored_count
                    
                    if stored_count > 0:
                        self.logger.debug(f"Training worker {self.worker_id}: å¤„ç†äº† {stored_count} ä¸ªæ ·æœ¬")
                    
                    experience_batch = []
                    
                    # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿æ¯ä¸ªworkeréƒ½æ£€æŸ¥æ›´æ–°æ¡ä»¶
                    if self.agent_proxy.should_update():
                        self.perform_update()
                
                # çŸ­æš‚ç¡çœ é¿å…è¿‡åº¦å ç”¨CPU
                time.sleep(0.001)
        
        except Exception as e:
            self.logger.error(f"Training worker {self.worker_id}: è¿è¡Œå¼‚å¸¸: {e}")
        finally:
            # å¤„ç†å‰©ä½™çš„ç»éªŒ
            if experience_batch:
                stored_count = self.agent_proxy.store_experience(experience_batch)
                self.samples_processed += stored_count
            
            self.logger.info(f"Training worker {self.worker_id} ç»“æŸè¿è¡Œ")
    
    def perform_update(self):
        """æ‰§è¡Œæ¨¡å‹æ›´æ–°"""
        try:
            update_start = time.time()
            
            # æ‰§è¡Œæ›´æ–°ï¼ˆè¿™é‡Œåªæœ‰ä¸€ä¸ªworkeræ‰§è¡Œæ›´æ–°ï¼Œå…¶ä»–workerç»§ç»­å¤„ç†æ•°æ®ï¼‰
            if self.worker_id == 0:  # åªæœ‰ç¬¬ä¸€ä¸ªtraining workeræ‰§è¡Œæ›´æ–°
                self.logger.info(f"Training worker {self.worker_id}: å¼€å§‹æ¨¡å‹æ›´æ–°")
                
                update_info = self.agent_proxy.update()
                
                if update_info:
                    self.updates_performed += 1
                    update_time = time.time() - update_start
                    self.last_update_time = time.time()
                    
                    self.logger.info(f"Training worker {self.worker_id}: æ¨¡å‹æ›´æ–°å®Œæˆ "
                                   f"#{self.updates_performed}, è€—æ—¶: {update_time:.3f}s")
                    
                    # è®°å½•æ›´æ–°ä¿¡æ¯
                    if 'coordinator' in update_info:
                        coord_loss = update_info['coordinator'].get('coordinator_loss', 0)
                        self.logger.debug(f"Coordinator loss: {coord_loss:.6f}")
                    
                    if 'discoverer' in update_info:
                        disc_loss = update_info['discoverer'].get('discoverer_loss', 0)
                        self.logger.debug(f"Discoverer loss: {disc_loss:.6f}")
                else:
                    self.logger.warning(f"Training worker {self.worker_id}: æ¨¡å‹æ›´æ–°è¿”å›None")
        
        except Exception as e:
            self.logger.error(f"Training worker {self.worker_id}: æ¨¡å‹æ›´æ–°å¼‚å¸¸: {e}")

class ThreadedRolloutTrainer:
    """å¤šçº¿ç¨‹HMASD Rollout-basedè®­ç»ƒå™¨"""
    
    def __init__(self, config, args=None):
        """
        åˆå§‹åŒ–å¤šçº¿ç¨‹è®­ç»ƒå™¨
        
        å‚æ•°:
            config: é…ç½®å¯¹è±¡
            args: å‘½ä»¤è¡Œå‚æ•°ï¼ˆå¯é€‰ï¼‰
        """
        self.config = config
        self.args = args or argparse.Namespace()
        
        # éªŒè¯å¹¶è®¾ç½®è®­ç»ƒæ¨¡å¼
        config.rollout_based_training = True
        config.episode_based_training = False
        config.sync_training_mode = False
        
        # éªŒè¯é…ç½®
        if not config.validate_rollout_config():
            raise ValueError("Rollouté…ç½®éªŒè¯å¤±è´¥")
        
        # çº¿ç¨‹é…ç½®ï¼ˆæŒ‰ç…§è®ºæ–‡ Appendix Eï¼‰
        self.num_training_threads = getattr(args, 'training_threads', 16)
        self.num_rollout_threads = getattr(args, 'rollout_threads', 32)
        
        # è®¾ç½®è®¾å¤‡
        self.device = self._get_device()
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.log_dir = f"logs/threaded_rollout_training_{timestamp}"
        os.makedirs(self.log_dir, exist_ok=True)
        
        # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
        self._init_logging()
        
        # çº¿ç¨‹æ§åˆ¶
        self.control_events = {
            'stop': Event(),
            'pause': Event()
        }
        
        # æ•°æ®ç¼“å†²åŒº
        buffer_size = getattr(args, 'buffer_size', 10000)
        self.data_buffer = DataBuffer(maxsize=buffer_size)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.start_time = None
        self.total_updates = 0
        self.total_samples = ThreadSafeCounter()
        self.total_steps = ThreadSafeCounter()  # æ·»åŠ æ€»æ­¥æ•°è®¡æ•°å™¨
        
        self.logger.info("ThreadedRolloutTraineråˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"æ—¥å¿—ç›®å½•: {self.log_dir}")
        self.logger.info(f"è®­ç»ƒçº¿ç¨‹æ•°: {self.num_training_threads}")
        self.logger.info(f"Rolloutçº¿ç¨‹æ•°: {self.num_rollout_threads}")
        self.logger.info(f"æ•°æ®ç¼“å†²åŒºå¤§å°: {buffer_size}")
        self.logger.info(config.get_rollout_summary())
    
    def _get_device(self):
        """è·å–è®¡ç®—è®¾å¤‡"""
        device_pref = getattr(self.args, 'device', 'auto')
        if device_pref == 'auto':
            return torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        elif device_pref == 'cuda':
            if torch.cuda.is_available():
                return torch.device('cuda')
            else:
                self.logger.warning("è¯·æ±‚CUDAä½†æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPU")
                return torch.device('cpu')
        else:
            return torch.device('cpu')
    
    def _init_logging(self):
        """åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ"""
        log_level = getattr(self.args, 'log_level', 'INFO')
        console_level = getattr(self.args, 'console_log_level', 'INFO')
        
        init_multiproc_logging(
            log_dir=self.log_dir,
            log_file='threaded_rollout_training.log',
            file_level=LOG_LEVELS.get(log_level.lower(), 20),
            console_level=LOG_LEVELS.get(console_level.lower(), 20)
        )
        
        self.logger = get_logger("ThreadedRolloutTrainer")
    
    def create_env_factory(self):
        """åˆ›å»ºç¯å¢ƒå·¥å‚å‡½æ•°"""
        scenario = getattr(self.args, 'scenario', 2)
        n_uavs = getattr(self.args, 'n_uavs', 5)
        n_users = getattr(self.args, 'n_users', 50)
        user_distribution = getattr(self.args, 'user_distribution', 'uniform')
        channel_model = getattr(self.args, 'channel_model', '3gpp-36777')
        max_hops = getattr(self.args, 'max_hops', 3)
        
        def make_env():
            def _init():
                env_seed = int(time.time() * 1000) % 10000  # åŸºäºæ—¶é—´çš„éšæœºç§å­
                if scenario == 1:
                    raw_env = UAVBaseStationEnv(
                        n_uavs=n_uavs,
                        n_users=n_users,
                        user_distribution=user_distribution,
                        channel_model=channel_model,
                        seed=env_seed
                    )
                elif scenario == 2:
                    raw_env = UAVCooperativeNetworkEnv(
                        n_uavs=n_uavs,
                        n_users=n_users,
                        max_hops=max_hops,
                        user_distribution=user_distribution,
                        channel_model=channel_model,
                        seed=env_seed
                    )
                else:
                    raise ValueError(f"æœªçŸ¥åœºæ™¯: {scenario}")
                
                env = ParallelToArrayAdapter(raw_env, seed=env_seed)
                return env
            return _init()
        
        return make_env
    
    def validate_environment(self):
        """éªŒè¯ç¯å¢ƒæ¥å£å…¼å®¹æ€§"""
        self.logger.info("å¼€å§‹éªŒè¯ç¯å¢ƒæ¥å£å…¼å®¹æ€§...")
        
        temp_env = self.create_env_factory()()
        try:
            # æµ‹è¯•reset
            reset_result = temp_env.reset()
            self.logger.info(f"ç¯å¢ƒreset()è¿”å›: {len(reset_result)}ä¸ªå€¼")
            
            if len(reset_result) == 2:
                observations, info = reset_result
                self.logger.info(f"ResetæˆåŠŸ: observations.shape={observations.shape}, info keys={list(info.keys()) if isinstance(info, dict) else 'not dict'}")
            else:
                self.logger.warning(f"Resetè¿”å›å€¼æ•°é‡å¼‚å¸¸: {len(reset_result)}")
            
            # æµ‹è¯•step
            dummy_actions = np.random.randn(temp_env.n_uavs, temp_env.action_dim)
            step_result = temp_env.step(dummy_actions)
            self.logger.info(f"ç¯å¢ƒstep()è¿”å›: {len(step_result)}ä¸ªå€¼")
            
            if len(step_result) == 5:
                next_obs, reward, terminated, truncated, info = step_result
                self.logger.info(f"StepæˆåŠŸ (Gymnasiumæ ¼å¼): next_obs.shape={next_obs.shape}, "
                               f"reward={reward}, terminated={terminated}, truncated={truncated}")
            elif len(step_result) == 4:
                next_obs, reward, done, info = step_result
                self.logger.info(f"StepæˆåŠŸ (ä¼ ç»Ÿæ ¼å¼): next_obs.shape={next_obs.shape}, "
                               f"reward={reward}, done={done}")
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„stepè¿”å›å€¼æ•°é‡: {len(step_result)}")
            
            self.logger.info("âœ… ç¯å¢ƒæ¥å£éªŒè¯é€šè¿‡")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ç¯å¢ƒæ¥å£éªŒè¯å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
        finally:
            temp_env.close()
    
    def initialize_agent(self):
        """åˆå§‹åŒ–HMASDä»£ç†"""
        # éªŒè¯ç¯å¢ƒå…¼å®¹æ€§
        if not self.validate_environment():
            raise RuntimeError("ç¯å¢ƒæ¥å£éªŒè¯å¤±è´¥ï¼Œæ— æ³•ç»§ç»­è®­ç»ƒ")
        
        # åˆ›å»ºä¸´æ—¶ç¯å¢ƒè·å–ç»´åº¦ä¿¡æ¯
        temp_env = self.create_env_factory()()
        state_dim = temp_env.state_dim
        obs_dim = temp_env.obs_dim
        n_agents = temp_env.n_uavs
        temp_env.close()
        
        # æ›´æ–°é…ç½®
        self.config.update_env_dims(state_dim, obs_dim)
        self.config.n_agents = n_agents
        
        self.logger.info(f"ç¯å¢ƒç»´åº¦: state_dim={state_dim}, obs_dim={obs_dim}, n_agents={n_agents}")
        
        # åˆ›å»ºä»£ç†
        self.agent = HMASDAgent(
            config=self.config,
            log_dir=self.log_dir,
            device=self.device,
            debug=getattr(self.args, 'debug', False)
        )
        
        # åˆ›å»ºä»£ç†ä»£ç†
        self.agent_proxy = AgentProxy(self.agent, self.config, self.logger)
        
        self.logger.info("HMASDä»£ç†åˆå§‹åŒ–å®Œæˆ")
    
    def start_rollout_threads(self):
        """å¯åŠ¨rolloutçº¿ç¨‹"""
        self.logger.info(f"å¯åŠ¨ {self.num_rollout_threads} ä¸ªrolloutçº¿ç¨‹")
        
        self.rollout_workers = []
        self.rollout_threads = []
        
        env_factory = self.create_env_factory()
        
        for i in range(self.num_rollout_threads):
            worker = RolloutWorker(
                worker_id=i,
                env_factory=env_factory,
                config=self.config,
                data_buffer=self.data_buffer,
                control_events=self.control_events,
                logger=self.logger
            )
            
            thread = threading.Thread(
                target=worker.run,
                args=(self.agent_proxy,),
                name=f"RolloutWorker-{i}"
            )
            thread.daemon = True
            
            self.rollout_workers.append(worker)
            self.rollout_threads.append(thread)
            thread.start()
        
        self.logger.info("æ‰€æœ‰rolloutçº¿ç¨‹å·²å¯åŠ¨")
        
        # ã€å…³é”®ä¿®å¤ã€‘è®¾ç½®AgentProxyå¯¹rollout workersçš„å¼•ç”¨
        self.agent_proxy.rollout_workers = self.rollout_workers
        
        # ã€å…³é”®ä¿®å¤ã€‘è®¾ç½®AgentProxyå¯¹rollout workersçš„å¼•ç”¨
        self.agent_proxy.rollout_workers = self.rollout_workers
    
    def start_training_threads(self):
        """å¯åŠ¨è®­ç»ƒçº¿ç¨‹"""
        self.logger.info(f"å¯åŠ¨ {self.num_training_threads} ä¸ªtrainingçº¿ç¨‹")
        
        self.training_workers = []
        self.training_threads = []
        
        for i in range(self.num_training_threads):
            worker = TrainingWorker(
                worker_id=i,
                agent_proxy=self.agent_proxy,
                data_buffer=self.data_buffer,
                control_events=self.control_events,
                logger=self.logger,
                config=self.config
            )
            
            thread = threading.Thread(
                target=worker.run,
                name=f"TrainingWorker-{i}"
            )
            thread.daemon = True
            
            self.training_workers.append(worker)
            self.training_threads.append(thread)
            thread.start()
        
        self.logger.info("æ‰€æœ‰trainingçº¿ç¨‹å·²å¯åŠ¨")
    
    def monitor_training(self, total_steps=100000):
        """ç›‘æ§è®­ç»ƒè¿›åº¦"""
        self.logger.info(f"å¼€å§‹è®­ç»ƒç›‘æ§ï¼Œç›®æ ‡æ­¥æ•°: {total_steps:,}")
        
        self.start_time = time.time()
        last_log_time = self.start_time
        last_stats_log_time = self.start_time
        last_step_count = 0
        
        try:
            while True:
                current_time = time.time()
                
                # è®¡ç®—å½“å‰æ€»æ­¥æ•°
                current_steps = sum(worker.samples_collected for worker in self.rollout_workers)
                self.total_steps.set(current_steps)
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ­¥æ•°é™åˆ¶
                if current_steps >= total_steps:
                    self.logger.info(f"è¾¾åˆ°è®­ç»ƒæ­¥æ•°é™åˆ¶ {total_steps:,}ï¼Œåœæ­¢è®­ç»ƒ")
                    break
                
                # æ¯åˆ†é’Ÿè®°å½•ä¸€æ¬¡ç®€è¦è¿›åº¦
                if current_time - last_log_time >= 60:
                    self.log_progress(current_steps, total_steps)
                    last_log_time = current_time
                
                # æ¯10åˆ†é’Ÿè®°å½•ä¸€æ¬¡è¯¦ç»†ç»Ÿè®¡
                if current_time - last_stats_log_time >= 600:
                    self.log_detailed_stats()
                    last_stats_log_time = current_time
                
                # æ£€æŸ¥çº¿ç¨‹å¥åº·çŠ¶æ€
                self.check_thread_health()
                
                time.sleep(30)  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
        
        except KeyboardInterrupt:
            self.logger.info("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        
        finally:
            self.stop_training()
    
    def log_progress(self, current_steps, total_steps):
        """è®°å½•è®­ç»ƒè¿›åº¦"""
        progress_percent = (current_steps / total_steps) * 100
        remaining_steps = total_steps - current_steps
        
        # è®¡ç®—æ—¶é—´ç»Ÿè®¡
        elapsed_time = time.time() - self.start_time
        if current_steps > 0:
            estimated_total_time = elapsed_time * total_steps / current_steps
            remaining_time = estimated_total_time - elapsed_time
        else:
            remaining_time = 0
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        buffer_stats = self.data_buffer.get_stats()
        
        # è®¡ç®—rollout workersç»Ÿè®¡
        total_samples = sum(worker.samples_collected for worker in self.rollout_workers)
        total_episodes = sum(worker.episodes_completed for worker in self.rollout_workers)
        total_high_level_exp = sum(worker.high_level_experiences_generated for worker in self.rollout_workers)
        
        # è®¡ç®—training workersç»Ÿè®¡
        total_updates = sum(worker.updates_performed for worker in self.training_workers)
        total_processed = sum(worker.samples_processed for worker in self.training_workers)
        
        # è·å–ä»£ç†ç»Ÿè®¡
        high_level_stored = self.agent_proxy.high_level_experiences_stored
        low_level_stored = self.agent_proxy.low_level_experiences_stored
        
        # è®¡ç®—æ­¥æ•°é€Ÿåº¦
        steps_per_second = current_steps / elapsed_time if elapsed_time > 0 else 0
        
        self.logger.info(f"è®­ç»ƒè¿›åº¦: {progress_percent:.1f}% "
                        f"({current_steps:,} / {total_steps:,} æ­¥), "
                        f"å‰©ä½™: {remaining_steps:,} æ­¥")
        self.logger.info(f"æ—¶é—´: å·²ç”¨={elapsed_time/3600:.1f}h, é¢„è®¡å‰©ä½™={remaining_time/3600:.1f}h, "
                        f"é€Ÿåº¦={steps_per_second:.1f} æ­¥/ç§’")
        self.logger.info(f"Rollout: æ ·æœ¬={total_samples:,}, Episodes={total_episodes:,}, "
                        f"é«˜å±‚ç»éªŒ={total_high_level_exp:,}")
        self.logger.info(f"Training: æ›´æ–°={total_updates}, å¤„ç†æ ·æœ¬={total_processed:,}")
        self.logger.info(f"ç»éªŒå­˜å‚¨: é«˜å±‚={high_level_stored:,}, ä½å±‚={low_level_stored:,}")
        self.logger.info(f"Buffer: é˜Ÿåˆ—={buffer_stats['queue_size']}, "
                        f"æ·»åŠ ={buffer_stats['total_added']:,}, "
                        f"æ¶ˆè´¹={buffer_stats['total_consumed']:,}")
    
    def log_detailed_stats(self):
        """è®°å½•è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
        self.logger.info("=== è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ ===")
        
        # Rollout workersç»Ÿè®¡
        self.logger.info("Rollout Workers:")
        for i, worker in enumerate(self.rollout_workers[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
            stats = worker.get_worker_stats()
            self.logger.info(f"  Worker {i}: æ ·æœ¬={stats['samples_collected']}, "
                           f"Episodes={stats['episodes_completed']}, "
                           f"é«˜å±‚ç»éªŒ={stats['high_level_experiences_generated']}, "
                           f"å½“å‰æŠ€èƒ½={stats['current_team_skill']}, "
                           f"æŠ€èƒ½è®¡æ—¶å™¨={stats['current_skill_timer']}, "
                           f"ç´¯ç§¯å¥–åŠ±={stats['current_accumulated_reward']:.3f}")
        if len(self.rollout_workers) > 5:
            self.logger.info(f"  ... è¿˜æœ‰ {len(self.rollout_workers) - 5} ä¸ªworkers")
        
        # Training workersç»Ÿè®¡
        self.logger.info("Training Workers:")
        for i, worker in enumerate(self.training_workers[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
            self.logger.info(f"  Worker {i}: æ›´æ–°={worker.updates_performed}, "
                           f"å¤„ç†æ ·æœ¬={worker.samples_processed}")
        if len(self.training_workers) > 5:
            self.logger.info(f"  ... è¿˜æœ‰ {len(self.training_workers) - 5} ä¸ªworkers")
        
        # æŠ€èƒ½å‘¨æœŸç»Ÿè®¡
        total_high_level_exp = sum(worker.high_level_experiences_generated for worker in self.rollout_workers)
        active_skills = [worker.current_team_skill for worker in self.rollout_workers if worker.current_team_skill is not None]
        skill_distribution = {}
        for skill in active_skills:
            skill_distribution[skill] = skill_distribution.get(skill, 0) + 1
        
        self.logger.info(f"æŠ€èƒ½å‘¨æœŸç»Ÿè®¡: æ€»é«˜å±‚ç»éªŒ={total_high_level_exp}, "
                        f"æ´»è·ƒæŠ€èƒ½åˆ†å¸ƒ={skill_distribution}")
        
        # ä»£ç†ç»éªŒå­˜å‚¨ç»Ÿè®¡
        self.logger.info(f"ä»£ç†ç»éªŒå­˜å‚¨: é«˜å±‚={self.agent_proxy.high_level_experiences_stored}, "
                        f"ä½å±‚={self.agent_proxy.low_level_experiences_stored}")
        
        # ç¼“å†²åŒºè¯¦ç»†ç»Ÿè®¡
        buffer_stats = self.data_buffer.get_stats()
        self.logger.info(f"Data Buffer: {buffer_stats}")
        
        # GPUå†…å­˜ä½¿ç”¨ï¼ˆå¦‚æœæœ‰GPUï¼‰
        if torch.cuda.is_available():
            memory_allocated = torch.cuda.memory_allocated(self.device) / 1024**3
            memory_reserved = torch.cuda.memory_reserved(self.device) / 1024**3
            self.logger.info(f"GPUå†…å­˜: å·²åˆ†é…={memory_allocated:.2f}GB, å·²ä¿ç•™={memory_reserved:.2f}GB")
    
    def check_thread_health(self):
        """æ£€æŸ¥çº¿ç¨‹å¥åº·çŠ¶æ€"""
        # æ£€æŸ¥rolloutçº¿ç¨‹
        dead_rollout_threads = [i for i, thread in enumerate(self.rollout_threads) if not thread.is_alive()]
        if dead_rollout_threads:
            self.logger.warning(f"å‘ç° {len(dead_rollout_threads)} ä¸ªæ­»äº¡çš„rolloutçº¿ç¨‹: {dead_rollout_threads}")
        
        # æ£€æŸ¥trainingçº¿ç¨‹
        dead_training_threads = [i for i, thread in enumerate(self.training_threads) if not thread.is_alive()]
        if dead_training_threads:
            self.logger.warning(f"å‘ç° {len(dead_training_threads)} ä¸ªæ­»äº¡çš„trainingçº¿ç¨‹: {dead_training_threads}")
        
        # æ£€æŸ¥æ•°æ®æµæ˜¯å¦æ­£å¸¸
        buffer_stats = self.data_buffer.get_stats()
        if buffer_stats['total_added'] == getattr(self, '_last_total_added', 0):
            self.logger.warning("æ•°æ®ç¼“å†²åŒºæ·»åŠ æ•°é‡æœªå¢åŠ ï¼Œå¯èƒ½rolloutçº¿ç¨‹æœ‰é—®é¢˜")
        if buffer_stats['total_consumed'] == getattr(self, '_last_total_consumed', 0):
            self.logger.warning("æ•°æ®ç¼“å†²åŒºæ¶ˆè´¹æ•°é‡æœªå¢åŠ ï¼Œå¯èƒ½trainingçº¿ç¨‹æœ‰é—®é¢˜")
        
        self._last_total_added = buffer_stats['total_added']
        self._last_total_consumed = buffer_stats['total_consumed']
    
    def stop_training(self):
        """åœæ­¢è®­ç»ƒ"""
        self.logger.info("åœæ­¢è®­ç»ƒ...")
        
        # è®¾ç½®åœæ­¢äº‹ä»¶
        self.control_events['stop'].set()
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹ç»“æŸ
        self.logger.info("ç­‰å¾…rolloutçº¿ç¨‹ç»“æŸ...")
        for i, thread in enumerate(self.rollout_threads):
            thread.join(timeout=10)
            if thread.is_alive():
                self.logger.warning(f"Rolloutçº¿ç¨‹ {i} æœªèƒ½åœ¨10ç§’å†…ç»“æŸ")
        
        self.logger.info("ç­‰å¾…trainingçº¿ç¨‹ç»“æŸ...")
        for i, thread in enumerate(self.training_threads):
            thread.join(timeout=10)
            if thread.is_alive():
                self.logger.warning(f"Trainingçº¿ç¨‹ {i} æœªèƒ½åœ¨10ç§’å†…ç»“æŸ")
        
        self.logger.info("æ‰€æœ‰çº¿ç¨‹å·²åœæ­¢")
    
    def save_final_model(self):
        """ä¿å­˜æœ€ç»ˆæ¨¡å‹"""
        try:
            final_model_path = os.path.join(self.log_dir, 'final_model.pt')
            self.agent.save_model(final_model_path)
            self.logger.info(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
        except Exception as e:
            self.logger.error(f"ä¿å­˜æœ€ç»ˆæ¨¡å‹å¤±è´¥: {e}")
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            # 1. åœæ­¢è®­ç»ƒï¼ˆå¦‚æœè¿˜æ²¡åœæ­¢ï¼‰
            if not self.control_events['stop'].is_set():
                self.stop_training()
            
            # 2. å…³é—­TensorBoard writer
            if hasattr(self.agent, 'writer') and self.agent.writer:
                try:
                    self.agent.writer.close()
                    self.logger.info("TensorBoard writerå·²å…³é—­")
                except Exception as e:
                    self.logger.warning(f"å…³é—­TensorBoard writeræ—¶å‡ºé”™: {e}")
            
            # 3. æ¸…ç†ä»£ç†ç¼“å†²åŒº
            if hasattr(self.agent, 'high_level_buffer'):
                self.agent.high_level_buffer.clear()
            if hasattr(self.agent, 'low_level_buffer'):
                self.agent.low_level_buffer.clear()
            if hasattr(self.agent, 'state_skill_dataset'):
                self.agent.state_skill_dataset.clear()
            
            self.logger.info("æ‰€æœ‰èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            print(f"æ¸…ç†èµ„æºæ—¶å‡ºé”™: {e}")
    
    def train(self, total_steps=100000):
        """
        æ‰§è¡Œå®Œæ•´çš„å¤šçº¿ç¨‹rollout-basedè®­ç»ƒ
        
        å‚æ•°:
            total_steps: è®­ç»ƒæ€»æ­¥æ•°
        """
        self.logger.info(f"å¼€å§‹HMASDå¤šçº¿ç¨‹Rollout-basedè®­ç»ƒ: {total_steps:,} æ­¥")
        self.logger.info(f"é…ç½®: {self.num_training_threads} è®­ç»ƒçº¿ç¨‹, {self.num_rollout_threads} rolloutçº¿ç¨‹")
        
        try:
            # åˆå§‹åŒ–ä»£ç†
            self.initialize_agent()
            
            # å¯åŠ¨rolloutçº¿ç¨‹
            self.start_rollout_threads()
            
            # ç­‰å¾…rolloutçº¿ç¨‹å¼€å§‹æ”¶é›†æ•°æ®
            time.sleep(5)
            
            # å¯åŠ¨trainingçº¿ç¨‹
            self.start_training_threads()
            
            # å¼€å§‹ç›‘æ§è®­ç»ƒ
            self.monitor_training(total_steps)
            
        except KeyboardInterrupt:
            self.logger.info("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        except Exception as e:
            self.logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
        
        finally:
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            self.save_final_model()
            
            # æ¸…ç†èµ„æº
            self.cleanup()
        
        # è®­ç»ƒå®Œæˆ
        if self.start_time:
            total_time = time.time() - self.start_time
            final_steps = sum(worker.samples_collected for worker in self.rollout_workers)
            self.logger.info(f"\nè®­ç»ƒå®Œæˆï¼")
            self.logger.info(f"æ€»æ—¶é—´: {total_time/3600:.2f}å°æ—¶")
            self.logger.info(f"æ€»æ­¥æ•°: {final_steps:,}")
            
            # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
            total_samples = sum(worker.samples_collected for worker in self.rollout_workers)
            total_episodes = sum(worker.episodes_completed for worker in self.rollout_workers)
            total_updates = sum(worker.updates_performed for worker in self.training_workers)
            
            self.logger.info(f"æ€»æ ·æœ¬æ•°: {total_samples:,}")
            self.logger.info(f"æ€»Episodes: {total_episodes:,}")
            self.logger.info(f"æ€»æ›´æ–°æ•°: {total_updates}")
            
            if total_time > 0:
                self.logger.info(f"æ ·æœ¬æ”¶é›†é€Ÿåº¦: {total_samples/total_time:.1f} æ ·æœ¬/ç§’")
                self.logger.info(f"Episodeå®Œæˆé€Ÿåº¦: {total_episodes/total_time:.1f} episodes/ç§’")
                self.logger.info(f"æ­¥æ•°å®Œæˆé€Ÿåº¦: {final_steps/total_time:.1f} æ­¥/ç§’")

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='HMASDå¤šçº¿ç¨‹Rollout-basedè®­ç»ƒï¼ˆæŒ‰è®ºæ–‡Appendix Eï¼‰')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--steps', type=int, default=None, help='è®­ç»ƒæ€»æ­¥æ•°ï¼ˆå¦‚æœä¸æŒ‡å®šï¼Œå°†ä»config.pyä¸­è¯»å–ï¼‰')
    parser.add_argument('--device', type=str, default='auto', choices=['auto', 'cpu', 'cuda'])
    parser.add_argument('--debug', action='store_true', help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
    
    # çº¿ç¨‹é…ç½®ï¼ˆæŒ‰ç…§è®ºæ–‡ Appendix Eï¼‰
    parser.add_argument('--training_threads', type=int, default=16, help='è®­ç»ƒçº¿ç¨‹æ•°ï¼ˆè®ºæ–‡é»˜è®¤16ï¼‰')
    parser.add_argument('--rollout_threads', type=int, default=32, help='Rolloutçº¿ç¨‹æ•°ï¼ˆè®ºæ–‡é»˜è®¤32ï¼‰')
    parser.add_argument('--buffer_size', type=int, default=10000, help='æ•°æ®ç¼“å†²åŒºå¤§å°')
    
    # ç¯å¢ƒå‚æ•°
    parser.add_argument('--scenario', type=int, default=2, choices=[1, 2], help='åœºæ™¯é€‰æ‹©')
    parser.add_argument('--n_uavs', type=int, default=5, help='æ— äººæœºæ•°é‡')
    parser.add_argument('--n_users', type=int, default=50, help='ç”¨æˆ·æ•°é‡')
    parser.add_argument('--user_distribution', type=str, default='uniform', 
                       choices=['uniform', 'cluster', 'hotspot'])
    parser.add_argument('--channel_model', type=str, default='3gpp-36777',
                       choices=['free_space', 'urban', 'suburban', '3gpp-36777'])
    parser.add_argument('--max_hops', type=int, default=3, help='æœ€å¤§è·³æ•°ï¼ˆä»…åœºæ™¯2ï¼‰')
    
    # æ—¥å¿—å‚æ•°
    parser.add_argument('--log_level', type=str, default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'])
    parser.add_argument('--console_log_level', type=str, default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'])
    
    return parser.parse_args()

def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # åˆ›å»ºé…ç½®
    config = Config()
    
    # ç¡®å®šè®­ç»ƒæ­¥æ•°ï¼šä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œå…¶æ¬¡ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼
    if args.steps is not None:
        total_steps = args.steps
        print(f"ğŸ“ˆ ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„è®­ç»ƒæ­¥æ•°: {total_steps:,}")
    else:
        total_steps = int(config.total_timesteps)
        print(f"ğŸ“ˆ ä»config.pyè¯»å–è®­ç»ƒæ­¥æ•°: {total_steps:,}")
    
    print("ğŸš€ HMASDå¤šçº¿ç¨‹Rollout-basedè®­ç»ƒï¼ˆä¸¥æ ¼æŒ‰è®ºæ–‡å®ç°ï¼‰")
    print("=" * 60)
    print(f"ğŸ“Š çº¿ç¨‹é…ç½®: {args.training_threads} è®­ç»ƒçº¿ç¨‹ + {args.rollout_threads} rolloutçº¿ç¨‹")
    print(f"ğŸ¯ è®­ç»ƒæ­¥æ•°: {total_steps:,}")
    print(f"ğŸ—‚ï¸ ç¼“å†²åŒºå¤§å°: {args.buffer_size}")
    
    # éªŒè¯å¹¶æ‰“å°é…ç½®
    config.validate_training_mode()
    config.validate_rollout_config()
    print(config.get_rollout_summary())
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = ThreadedRolloutTrainer(config, args)
        
        # å¼€å§‹è®­ç»ƒ
        trainer.train(total_steps=total_steps)
        
        print("ğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        try:
            shutdown_logging()
        except:
            pass

if __name__ == "__main__":
    main()

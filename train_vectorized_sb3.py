#!/usr/bin/env python3
"""
åŸºäºStable Baselines3å‘é‡åŒ–ç¯å¢ƒçš„HMASDè®­ç»ƒè„šæœ¬
ä½¿ç”¨SubprocVecEnvå®ç°çœŸæ­£çš„å¹¶è¡Œç¯å¢ƒæ‰§è¡Œï¼Œå¤§å¹…æå‡è®­ç»ƒæ•ˆç‡

æ ¸å¿ƒä¼˜åŠ¿ï¼š
1. 32ä¸ªå¹¶è¡Œè¿›ç¨‹åŒæ—¶æ‰§è¡Œç¯å¢ƒ
2. æ‰¹é‡æ•°æ®æ”¶é›†å’Œå¤„ç†
3. æ¶ˆé™¤é”ç«äº‰å’Œçº¿ç¨‹åŒæ­¥é—®é¢˜
4. GPUå‹å¥½çš„æ‰¹é‡è®¡ç®—
5. ç®€åŒ–çš„æ•°æ®æµæ¶æ„
"""

import os
import sys
import time
import numpy as np
import torch
import argparse
import multiprocessing as mp
from datetime import datetime
from collections import deque
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# è®¾ç½®matplotlibåç«¯ï¼ˆé¿å…å¤šè¿›ç¨‹é—®é¢˜ï¼‰
import matplotlib
matplotlib.use('Agg')

from logger import get_logger, init_multiproc_logging, LOG_LEVELS, shutdown_logging
from config import Config
from hmasd.thread_safe_hmasd_agent import ThreadSafeHMASDAgent
from envs.pettingzoo.scenario1 import UAVBaseStationEnv
from envs.pettingzoo.scenario2 import UAVCooperativeNetworkEnv
from envs.pettingzoo.env_adapter import ParallelToArrayAdapter

# å¯¼å…¥ Stable Baselines3 çš„å‘é‡åŒ–ç¯å¢ƒ
from stable_baselines3.common.vec_env import SubprocVecEnv, VecEnv
from stable_baselines3.common.vec_env.base_vec_env import VecEnvWrapper

class UAVVecEnvWrapper(VecEnvWrapper):
    """UAVç¯å¢ƒçš„å‘é‡åŒ–åŒ…è£…å™¨"""
    
    def __init__(self, venv):
        super().__init__(venv)
        self.n_uavs = None
        self.state_dim = None
        self.obs_dim = None
        self.action_dim = None
        
        # ä»ç¬¬ä¸€ä¸ªç¯å¢ƒè·å–ç»´åº¦ä¿¡æ¯
        self._get_env_dims()
    
    def _get_env_dims(self):
        """è·å–ç¯å¢ƒç»´åº¦ä¿¡æ¯"""
        # ä¸´æ—¶åˆ›å»ºä¸€ä¸ªç¯å¢ƒæ¥è·å–ç»´åº¦
        temp_env = self.venv.envs[0].env
        if hasattr(temp_env, 'unwrapped'):
            temp_env = temp_env.unwrapped
        
        self.n_uavs = temp_env.n_uavs
        self.state_dim = temp_env.state_dim
        self.obs_dim = temp_env.obs_dim
        self.action_dim = temp_env.action_dim
    
    def reset(self):
        """é‡ç½®æ‰€æœ‰ç¯å¢ƒ"""
        observations = self.venv.reset()
        return observations
    
    def step_async(self, actions):
        """å¼‚æ­¥æ‰§è¡Œæ­¥éª¤"""
        self.venv.step_async(actions)
    
    def step_wait(self):
        """ç­‰å¾…æ­¥éª¤å®Œæˆ"""
        return self.venv.step_wait()
    
    def get_global_states(self):
        """è·å–æ‰€æœ‰ç¯å¢ƒçš„å…¨å±€çŠ¶æ€"""
        states = []
        for env in self.venv.envs:
            if hasattr(env.env, 'get_state'):
                state = env.env.get_state()
            elif hasattr(env.env, 'unwrapped') and hasattr(env.env.unwrapped, 'get_state'):
                state = env.env.unwrapped.get_state()
            else:
                # å›é€€ï¼šä½¿ç”¨é›¶çŠ¶æ€
                state = np.zeros(self.state_dim)
            states.append(state)
        return np.array(states)

class VectorizedHMASDTrainer:
    """åŸºäºSB3å‘é‡åŒ–ç¯å¢ƒçš„HMASDè®­ç»ƒå™¨"""
    
    def __init__(self, config, args=None):
        self.config = config
        self.args = args or argparse.Namespace()
        
        # éªŒè¯å¹¶è®¾ç½®è®­ç»ƒæ¨¡å¼
        config.rollout_based_training = True
        config.episode_based_training = False
        config.sync_training_mode = False
        
        # å‘é‡åŒ–ç¯å¢ƒé…ç½®
        self.n_envs = getattr(args, 'n_envs', 32)
        
        # è®¾ç½®è®¾å¤‡
        self.device = self._get_device()
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.log_dir = f"logs/vectorized_sb3_training_{timestamp}"
        os.makedirs(self.log_dir, exist_ok=True)
        
        # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
        self._init_logging()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.start_time = None
        self.total_updates = 0
        self.total_samples = 0
        self.total_episodes = 0
        
        # æ€§èƒ½ç›‘æ§
        self.performance_stats = {
            'rollout_times': deque(maxlen=100),
            'update_times': deque(maxlen=100),
            'samples_per_second': deque(maxlen=100),
            'last_log_time': time.time()
        }
        
        self.logger.info("åŸºäºSB3å‘é‡åŒ–ç¯å¢ƒçš„HMASDè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"æ—¥å¿—ç›®å½•: {self.log_dir}")
        self.logger.info(f"å‘é‡åŒ–ç¯å¢ƒæ•°é‡: {self.n_envs}")
        self.logger.info(config.get_rollout_summary())
    
    def _get_device(self):
        """è·å–è®¡ç®—è®¾å¤‡"""
        device_pref = getattr(self.args, 'device', 'auto')
        if device_pref == 'auto':
            return torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        elif device_pref == 'cuda':
            if torch.cuda.is_available():
                return torch.device('cuda')
            else:
                self.logger.warning("è¯·æ±‚CUDAä½†æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPU")
                return torch.device('cpu')
        else:
            return torch.device('cpu')
    
    def _init_logging(self):
        """åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ"""
        log_level = getattr(self.args, 'log_level', 'INFO')
        console_level = getattr(self.args, 'console_log_level', 'INFO')
        
        init_multiproc_logging(
            log_dir=self.log_dir,
            log_file='vectorized_sb3_training.log',
            file_level=LOG_LEVELS.get(log_level.lower(), 20),
            console_level=LOG_LEVELS.get(console_level.lower(), 20)
        )
        
        self.logger = get_logger("VectorizedHMASDTrainer")
    
    def create_env_factory(self):
        """åˆ›å»ºç¯å¢ƒå·¥å‚å‡½æ•°"""
        scenario = getattr(self.args, 'scenario', 2)
        n_uavs = getattr(self.args, 'n_uavs', 5)
        n_users = getattr(self.args, 'n_users', 50)
        user_distribution = getattr(self.args, 'user_distribution', 'uniform')
        channel_model = getattr(self.args, 'channel_model', '3gpp-36777')
        max_hops = getattr(self.args, 'max_hops', 3)
        
        def make_env():
            def _init():
                env_seed = np.random.randint(0, 10000)  # éšæœºç§å­
                if scenario == 1:
                    raw_env = UAVBaseStationEnv(
                        n_uavs=n_uavs,
                        n_users=n_users,
                        user_distribution=user_distribution,
                        channel_model=channel_model,
                        seed=env_seed
                    )
                elif scenario == 2:
                    raw_env = UAVCooperativeNetworkEnv(
                        n_uavs=n_uavs,
                        n_users=n_users,
                        max_hops=max_hops,
                        user_distribution=user_distribution,
                        channel_model=channel_model,
                        seed=env_seed
                    )
                else:
                    raise ValueError(f"æœªçŸ¥åœºæ™¯: {scenario}")
                
                env = ParallelToArrayAdapter(raw_env, seed=env_seed)
                return env
            return _init
        
        return make_env
    
    def create_vectorized_env(self):
        """åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒ"""
        self.logger.info(f"åˆ›å»º {self.n_envs} ä¸ªå‘é‡åŒ–ç¯å¢ƒ...")
        
        env_factory = self.create_env_factory()
        env_fns = [env_factory() for _ in range(self.n_envs)]
        
        # åˆ›å»ºSubprocVecEnv
        vec_env = SubprocVecEnv(env_fns, start_method='spawn')
        
        # åŒ…è£…ä¸ºUAVVecEnvWrapper
        self.vec_env = UAVVecEnvWrapper(vec_env)
        
        self.logger.info(f"å‘é‡åŒ–ç¯å¢ƒåˆ›å»ºå®Œæˆ")
        self.logger.info(f"ç¯å¢ƒç»´åº¦: n_uavs={self.vec_env.n_uavs}, "
                        f"state_dim={self.vec_env.state_dim}, "
                        f"obs_dim={self.vec_env.obs_dim}, "
                        f"action_dim={self.vec_env.action_dim}")
        
        return self.vec_env
    
    def initialize_agent(self):
        """åˆå§‹åŒ–HMASDä»£ç†"""
        # åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒ
        vec_env = self.create_vectorized_env()
        
        # æ›´æ–°é…ç½®
        self.config.update_env_dims(vec_env.state_dim, vec_env.obs_dim)
        self.config.n_agents = vec_env.n_uavs
        
        # åˆ›å»ºä»£ç†
        self.agent = ThreadSafeHMASDAgent(
            config=self.config,
            log_dir=self.log_dir,
            device=self.device,
            debug=getattr(self.args, 'debug', False)
        )
        
        self.logger.info("HMASDä»£ç†åˆå§‹åŒ–å®Œæˆ")
    
    def collect_vectorized_rollout(self):
        """å‘é‡åŒ–rolloutæ•°æ®æ”¶é›†"""
        rollout_start_time = time.time()
        
        # é‡ç½®æ‰€æœ‰ç¯å¢ƒ
        observations = self.vec_env.reset()  # (n_envs, n_agents, obs_dim)
        states = self.vec_env.get_global_states()  # (n_envs, state_dim)
        
        # å­˜å‚¨æ”¶é›†çš„ç»éªŒ
        collected_experiences = {
            'high_level': [],
            'low_level': [],
            'state_skill': []
        }
        
        # æŠ€èƒ½çŠ¶æ€è·Ÿè¸ª
        accumulated_rewards = np.zeros(self.n_envs)
        skill_timers = np.zeros(self.n_envs)
        current_team_skills = None
        current_agent_skills = None
        
        self.logger.info(f"å¼€å§‹å‘é‡åŒ–rolloutæ”¶é›†ï¼Œç›®æ ‡æ­¥æ•°: {self.config.rollout_length}")
        
        for step in range(self.config.rollout_length):
            step_start_time = time.time()
            
            # æ‰¹é‡æŠ€èƒ½åˆ†é…
            if step % self.config.k == 0 or current_team_skills is None:
                team_skills, agent_skills, skill_log_probs = self._assign_skills_batch(states, observations)
                current_team_skills = team_skills
                current_agent_skills = agent_skills
                
                # å­˜å‚¨é«˜å±‚ç»éªŒï¼ˆå¦‚æœä¸æ˜¯ç¬¬ä¸€æ­¥ï¼‰
                if step > 0:
                    self._store_high_level_experiences_batch(
                        states, current_team_skills, observations, current_agent_skills,
                        accumulated_rewards, skill_log_probs, collected_experiences
                    )
                    accumulated_rewards.fill(0.0)  # é‡ç½®ç´¯ç§¯å¥–åŠ±
                    skill_timers.fill(0)
            
            # æ‰¹é‡åŠ¨ä½œé€‰æ‹©
            actions, action_logprobs = self._select_actions_batch(observations, current_agent_skills)
            
            # å‘é‡åŒ–ç¯å¢ƒæ­¥éª¤
            next_observations, rewards, dones, infos = self.vec_env.step(actions)
            next_states = self.vec_env.get_global_states()
            
            # ç´¯ç§¯å¥–åŠ±
            accumulated_rewards += rewards
            skill_timers += 1
            
            # å­˜å‚¨ä½å±‚ç»éªŒ
            self._store_low_level_experiences_batch(
                states, observations, actions, rewards, next_states, next_observations,
                dones, current_team_skills, current_agent_skills, action_logprobs,
                skill_log_probs, collected_experiences
            )
            
            # å­˜å‚¨çŠ¶æ€æŠ€èƒ½æ•°æ®
            self._store_state_skill_experiences_batch(
                next_states, current_team_skills, next_observations, current_agent_skills,
                collected_experiences
            )
            
            # æ›´æ–°çŠ¶æ€
            states = next_states
            observations = next_observations
            
            # å¤„ç†ç¯å¢ƒé‡ç½®
            if np.any(dones):
                self._handle_environment_resets(dones)
            
            # æ€§èƒ½ç›‘æ§
            step_time = time.time() - step_start_time
            if step % 100 == 0:
                self.logger.debug(f"æ­¥éª¤ {step}/{self.config.rollout_length}, "
                                f"æ­¥éª¤è€—æ—¶: {step_time*1000:.2f}ms")
        
        # å­˜å‚¨æœ€åçš„é«˜å±‚ç»éªŒ
        self._store_high_level_experiences_batch(
            states, current_team_skills, observations, current_agent_skills,
            accumulated_rewards, skill_log_probs, collected_experiences
        )
        
        rollout_time = time.time() - rollout_start_time
        self.performance_stats['rollout_times'].append(rollout_time)
        
        # ç»Ÿè®¡æ”¶é›†çš„ç»éªŒæ•°é‡
        high_level_count = len(collected_experiences['high_level'])
        low_level_count = len(collected_experiences['low_level'])
        state_skill_count = len(collected_experiences['state_skill'])
        
        self.logger.info(f"å‘é‡åŒ–rolloutæ”¶é›†å®Œæˆï¼Œè€—æ—¶: {rollout_time:.2f}s")
        self.logger.info(f"æ”¶é›†ç»éªŒ: é«˜å±‚={high_level_count}, ä½å±‚={low_level_count}, çŠ¶æ€æŠ€èƒ½={state_skill_count}")
        
        return collected_experiences
    
    def _assign_skills_batch(self, states, observations):
        """æ‰¹é‡æŠ€èƒ½åˆ†é…"""
        try:
            # è½¬æ¢ä¸ºtensor
            states_tensor = torch.FloatTensor(states).to(self.device)
            observations_tensor = torch.FloatTensor(observations).to(self.device)
            
            # æ‰¹é‡åˆ†é…æŠ€èƒ½
            team_skills_list = []
            agent_skills_list = []
            skill_log_probs_list = []
            
            for i in range(self.n_envs):
                team_skill, agent_skills, log_probs = self.agent.assign_skills(
                    states_tensor[i], observations_tensor[i], deterministic=False
                )
                
                # è½¬æ¢ä¸ºnumpy
                if isinstance(team_skill, torch.Tensor):
                    team_skill = team_skill.cpu().item()
                if isinstance(agent_skills, torch.Tensor):
                    agent_skills = agent_skills.cpu().numpy()
                
                team_skills_list.append(team_skill)
                agent_skills_list.append(agent_skills)
                skill_log_probs_list.append(log_probs)
            
            return np.array(team_skills_list), np.array(agent_skills_list), skill_log_probs_list
            
        except Exception as e:
            self.logger.error(f"æ‰¹é‡æŠ€èƒ½åˆ†é…å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤æŠ€èƒ½
            return (np.zeros(self.n_envs, dtype=int), 
                   np.zeros((self.n_envs, self.config.n_agents), dtype=int),
                   [{'team_log_prob': 0.0, 'agent_log_probs': [0.0] * self.config.n_agents} for _ in range(self.n_envs)])
    
    def _select_actions_batch(self, observations, agent_skills):
        """æ‰¹é‡åŠ¨ä½œé€‰æ‹©"""
        try:
            observations_tensor = torch.FloatTensor(observations).to(self.device)
            agent_skills_tensor = torch.LongTensor(agent_skills).to(self.device)
            
            actions_list = []
            action_logprobs_list = []
            
            for i in range(self.n_envs):
                actions, action_logprobs = self.agent.select_action(
                    observations_tensor[i], agent_skills_tensor[i], 
                    deterministic=False, env_id=i
                )
                
                # è½¬æ¢ä¸ºnumpy
                if isinstance(actions, torch.Tensor):
                    actions = actions.cpu().detach().numpy()
                if isinstance(action_logprobs, torch.Tensor):
                    action_logprobs = action_logprobs.cpu().detach().numpy()
                
                actions_list.append(actions)
                action_logprobs_list.append(action_logprobs)
            
            return np.array(actions_list), np.array(action_logprobs_list)
            
        except Exception as e:
            self.logger.error(f"æ‰¹é‡åŠ¨ä½œé€‰æ‹©å¤±è´¥: {e}")
            # è¿”å›éšæœºåŠ¨ä½œ
            return (np.random.randn(self.n_envs, self.config.n_agents, self.config.action_dim),
                   np.zeros((self.n_envs, self.config.n_agents)))
    
    def _store_high_level_experiences_batch(self, states, team_skills, observations, agent_skills,
                                          accumulated_rewards, skill_log_probs, collected_experiences):
        """æ‰¹é‡å­˜å‚¨é«˜å±‚ç»éªŒ"""
        for i in range(self.n_envs):
            experience = {
                'experience_type': 'high_level',
                'env_id': i,
                'state': states[i].copy(),
                'team_skill': team_skills[i],
                'observations': observations[i].copy(),
                'agent_skills': agent_skills[i].copy(),
                'accumulated_reward': accumulated_rewards[i],
                'skill_log_probs': skill_log_probs[i] if skill_log_probs else None,
                'timestamp': time.time()
            }
            collected_experiences['high_level'].append(experience)
    
    def _store_low_level_experiences_batch(self, states, observations, actions, rewards, next_states,
                                         next_observations, dones, team_skills, agent_skills,
                                         action_logprobs, skill_log_probs, collected_experiences):
        """æ‰¹é‡å­˜å‚¨ä½å±‚ç»éªŒ"""
        for i in range(self.n_envs):
            experience = {
                'experience_type': 'low_level',
                'env_id': i,
                'state': states[i].copy(),
                'observations': observations[i].copy(),
                'actions': actions[i].copy(),
                'rewards': rewards[i],
                'next_state': next_states[i].copy(),
                'next_observations': next_observations[i].copy(),
                'dones': dones[i],
                'team_skill': team_skills[i],
                'agent_skills': agent_skills[i].copy(),
                'action_logprobs': action_logprobs[i].copy(),
                'skill_log_probs': skill_log_probs[i] if skill_log_probs else None,
                'timestamp': time.time()
            }
            collected_experiences['low_level'].append(experience)
    
    def _store_state_skill_experiences_batch(self, states, team_skills, observations, agent_skills,
                                           collected_experiences):
        """æ‰¹é‡å­˜å‚¨çŠ¶æ€æŠ€èƒ½ç»éªŒ"""
        for i in range(self.n_envs):
            experience = {
                'experience_type': 'state_skill',
                'env_id': i,
                'state': states[i].copy(),
                'team_skill': team_skills[i],
                'observations': observations[i].copy(),
                'agent_skills': agent_skills[i].copy(),
                'timestamp': time.time()
            }
            collected_experiences['state_skill'].append(experience)
    
    def _handle_environment_resets(self, dones):
        """å¤„ç†ç¯å¢ƒé‡ç½®"""
        reset_count = np.sum(dones)
        if reset_count > 0:
            self.total_episodes += reset_count
            self.logger.debug(f"é‡ç½®äº† {reset_count} ä¸ªç¯å¢ƒ")
    
    def store_experiences_to_agent(self, collected_experiences):
        """å°†æ”¶é›†çš„ç»éªŒå­˜å‚¨åˆ°ä»£ç†"""
        store_start_time = time.time()
        
        stored_counts = {'high_level': 0, 'low_level': 0, 'state_skill': 0}
        
        # å­˜å‚¨é«˜å±‚ç»éªŒ
        for exp in collected_experiences['high_level']:
            success = self.agent.store_high_level_transition(
                state=exp['state'],
                team_skill=exp['team_skill'],
                observations=exp['observations'],
                agent_skills=exp['agent_skills'],
                accumulated_reward=exp['accumulated_reward'],
                skill_log_probs=exp.get('skill_log_probs'),
                worker_id=exp['env_id']
            )
            if success:
                stored_counts['high_level'] += 1
        
        # å­˜å‚¨ä½å±‚ç»éªŒ
        for exp in collected_experiences['low_level']:
            success = self.agent.store_low_level_transition(
                state=exp['state'],
                next_state=exp['next_state'],
                observations=exp['observations'],
                next_observations=exp['next_observations'],
                actions=exp['actions'],
                rewards=exp['rewards'],
                dones=exp['dones'],
                team_skill=exp['team_skill'],
                agent_skills=exp['agent_skills'],
                action_logprobs=exp['action_logprobs'],
                skill_log_probs=exp.get('skill_log_probs'),
                worker_id=exp['env_id']
            )
            if success:
                stored_counts['low_level'] += 1
        
        # å­˜å‚¨çŠ¶æ€æŠ€èƒ½æ•°æ®
        for exp in collected_experiences['state_skill']:
            try:
                state_tensor = torch.FloatTensor(exp['state']).to(self.device)
                team_skill_tensor = torch.tensor(exp['team_skill'], device=self.device)
                observations_tensor = torch.FloatTensor(exp['observations']).to(self.device)
                agent_skills_tensor = torch.tensor(exp['agent_skills'], device=self.device)
                
                self.agent.state_skill_dataset.push(
                    state_tensor, team_skill_tensor, observations_tensor, agent_skills_tensor
                )
                stored_counts['state_skill'] += 1
            except Exception as e:
                self.logger.error(f"çŠ¶æ€æŠ€èƒ½æ•°æ®å­˜å‚¨å¤±è´¥: {e}")
        
        store_time = time.time() - store_start_time
        
        self.logger.info(f"ç»éªŒå­˜å‚¨å®Œæˆï¼Œè€—æ—¶: {store_time:.3f}s")
        self.logger.info(f"å­˜å‚¨ç»Ÿè®¡: é«˜å±‚={stored_counts['high_level']}, "
                        f"ä½å±‚={stored_counts['low_level']}, "
                        f"çŠ¶æ€æŠ€èƒ½={stored_counts['state_skill']}")
        
        return stored_counts
    
    def perform_update(self):
        """æ‰§è¡Œæ¨¡å‹æ›´æ–°"""
        update_start_time = time.time()
        
        try:
            self.logger.info("å¼€å§‹æ¨¡å‹æ›´æ–°...")
            
            update_info = self.agent.rollout_update()
            
            if update_info:
                self.total_updates += 1
                update_time = time.time() - update_start_time
                self.performance_stats['update_times'].append(update_time)
                
                self.logger.info(f"æ¨¡å‹æ›´æ–°å®Œæˆ #{self.total_updates}ï¼Œè€—æ—¶: {update_time:.3f}s")
                
                # è®°å½•æ›´æ–°ä¿¡æ¯
                if isinstance(update_info, dict):
                    for key, value in update_info.items():
                        if isinstance(value, (int, float)):
                            self.logger.debug(f"æ›´æ–°æŒ‡æ ‡ {key}: {value:.6f}")
                
                return True
            else:
                self.logger.warning("æ¨¡å‹æ›´æ–°è¿”å›None")
                return False
                
        except Exception as e:
            self.logger.error(f"æ¨¡å‹æ›´æ–°å¤±è´¥: {e}")
            return False
    
    def log_training_progress(self, current_samples, total_samples):
        """è®°å½•è®­ç»ƒè¿›åº¦"""
        current_time = time.time()
        
        if current_time - self.performance_stats['last_log_time'] < 60:
            return  # æ¯åˆ†é’Ÿè®°å½•ä¸€æ¬¡
        
        progress_percent = (current_samples / total_samples) * 100
        elapsed_time = current_time - self.start_time
        
        # è®¡ç®—é€Ÿåº¦
        samples_per_second = current_samples / elapsed_time if elapsed_time > 0 else 0
        self.performance_stats['samples_per_second'].append(samples_per_second)
        
        # è®¡ç®—å¹³å‡æ€§èƒ½
        avg_rollout_time = np.mean(self.performance_stats['rollout_times']) if self.performance_stats['rollout_times'] else 0
        avg_update_time = np.mean(self.performance_stats['update_times']) if self.performance_stats['update_times'] else 0
        avg_samples_per_sec = np.mean(self.performance_stats['samples_per_second']) if self.performance_stats['samples_per_second'] else 0
        
        # ä¼°è®¡å‰©ä½™æ—¶é—´
        if samples_per_second > 0:
            remaining_samples = total_samples - current_samples
            remaining_time = remaining_samples / samples_per_second
        else:
            remaining_time = 0
        
        self.logger.info(f"è®­ç»ƒè¿›åº¦: {progress_percent:.1f}% ({current_samples:,} / {total_samples:,} æ ·æœ¬)")
        self.logger.info(f"æ—¶é—´: å·²ç”¨={elapsed_time/3600:.1f}h, é¢„è®¡å‰©ä½™={remaining_time/3600:.1f}h")
        self.logger.info(f"æ€§èƒ½: æ ·æœ¬é€Ÿåº¦={avg_samples_per_sec:.1f}/s, "
                        f"å¹³å‡rollout={avg_rollout_time:.2f}s, å¹³å‡æ›´æ–°={avg_update_time:.2f}s")
        self.logger.info(f"ç»Ÿè®¡: æ›´æ–°æ¬¡æ•°={self.total_updates}, Episodes={self.total_episodes}")
        
        # GPUå†…å­˜ä½¿ç”¨ï¼ˆå¦‚æœæœ‰GPUï¼‰
        if torch.cuda.is_available():
            memory_allocated = torch.cuda.memory_allocated(self.device) / 1024**3
            memory_reserved = torch.cuda.memory_reserved(self.device) / 1024**3
            self.logger.info(f"GPUå†…å­˜: å·²åˆ†é…={memory_allocated:.2f}GB, å·²ä¿ç•™={memory_reserved:.2f}GB")
        
        self.performance_stats['last_log_time'] = current_time
    
    def save_model(self):
        """ä¿å­˜æ¨¡å‹"""
        try:
            model_path = os.path.join(self.log_dir, 'vectorized_sb3_model.pt')
            self.agent.save_model(model_path)
            self.logger.info(f"æ¨¡å‹å·²ä¿å­˜: {model_path}")
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            # å…³é—­å‘é‡åŒ–ç¯å¢ƒ
            if hasattr(self, 'vec_env'):
                self.vec_env.close()
                self.logger.info("å‘é‡åŒ–ç¯å¢ƒå·²å…³é—­")
            
            # å…³é—­TensorBoard writer
            if hasattr(self.agent, 'writer') and self.agent.writer:
                self.agent.writer.close()
                self.logger.info("TensorBoard writerå·²å…³é—­")
            
            # æ¸…ç†ä»£ç†ç¼“å†²åŒº
            if hasattr(self.agent, 'high_level_buffer'):
                self.agent.high_level_buffer.clear()
            if hasattr(self.agent, 'low_level_buffer'):
                self.agent.low_level_buffer.clear()
            if hasattr(self.agent, 'state_skill_dataset'):
                self.agent.state_skill_dataset.clear()
            
            self.logger.info("æ‰€æœ‰èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            print(f"æ¸…ç†èµ„æºæ—¶å‡ºé”™: {e}")
    
    def train(self, total_samples=100000):
        """
        æ‰§è¡Œå®Œæ•´çš„å‘é‡åŒ–è®­ç»ƒ
        
        å‚æ•°:
            total_samples: è®­ç»ƒæ€»æ ·æœ¬æ•°
        """
        self.logger.info(f"å¼€å§‹åŸºäºSB3å‘é‡åŒ–ç¯å¢ƒçš„HMASDè®­ç»ƒ: {total_samples:,} æ ·æœ¬")
        self.logger.info(f"é…ç½®: {self.n_envs} ä¸ªå‘é‡åŒ–ç¯å¢ƒ")
        
        try:
            # åˆå§‹åŒ–ä»£ç†
            self.initialize_agent()
            
            # å¼€å§‹è®­ç»ƒ
            self.start_time = time.time()
            current_samples = 0
            
            while current_samples < total_samples:
                # å‘é‡åŒ–rolloutæ”¶é›†
                collected_experiences = self.collect_vectorized_rollout()
                
                # å­˜å‚¨ç»éªŒåˆ°ä»£ç†
                stored_counts = self.store_experiences_to_agent(collected_experiences)
                
                # æ›´æ–°æ ·æœ¬è®¡æ•°
                rollout_samples = self.n_envs * self.config.rollout_length
                current_samples += rollout_samples
                self.total_samples = current_samples
                
                # æ‰§è¡Œæ¨¡å‹æ›´æ–°
                if self.agent.should_rollout_update():
                    self.perform_update()
                
                # è®°å½•è®­ç»ƒè¿›åº¦
                self.log_training_progress(current_samples, total_samples)
            
            # è®­ç»ƒå®Œæˆ
            self.logger.info(f"å‘é‡åŒ–è®­ç»ƒå®Œæˆï¼æ€»æ ·æœ¬: {current_samples:,}")
            
        except KeyboardInterrupt:
            self.logger.info("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        except Exception as e:
            self.logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
        finally:
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            self.save_model()
            
            # æ¸…ç†èµ„æº
            self.cleanup()
        
        # è®­ç»ƒå®Œæˆç»Ÿè®¡
        if self.start_time:
            total_time = time.time() - self.start_time
            self.logger.info(f"\nå‘é‡åŒ–è®­ç»ƒå®Œæˆï¼")
            self.logger.info(f"æ€»æ—¶é—´: {total_time/3600:.2f}å°æ—¶")
            self.logger.info(f"æ€»æ ·æœ¬æ•°: {self.total_samples:,}")
            self.logger.info(f"æ€»æ›´æ–°æ•°: {self.total_updates}")
            self.logger.info(f"æ€»Episodes: {self.total_episodes}")
            
            if total_time > 0:
                self.logger.info(f"æ ·æœ¬æ”¶é›†é€Ÿåº¦: {self.total_samples/total_time:.1f} æ ·æœ¬/ç§’")
                self.logger.info(f"Episodeå®Œæˆé€Ÿåº¦: {self.total_episodes/total_time:.1f} episodes/ç§’")

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='åŸºäºSB3å‘é‡åŒ–ç¯å¢ƒçš„HMASDè®­ç»ƒ')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--samples', type=int, default=None, help='è®­ç»ƒæ€»æ ·æœ¬æ•°ï¼ˆå¦‚æœä¸æŒ‡å®šï¼Œå°†ä»config.pyä¸­è¯»å–ï¼‰')
    parser.add_argument('--device', type=str, default='auto', choices=['auto', 'cpu', 'cuda'])
    parser.add_argument('--debug', action='store_true', help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
    
    # å‘é‡åŒ–ç¯å¢ƒé…ç½®
    parser.add_argument('--n_envs', type=int, default=32, help='å‘é‡åŒ–ç¯å¢ƒæ•°é‡')
    
    # ç¯å¢ƒå‚æ•°
    parser.add_argument('--scenario', type=int, default=2, choices=[1, 2], help='åœºæ™¯é€‰æ‹©')
    parser.add_argument('--n_uavs', type=int, default=5, help='æ— äººæœºæ•°é‡')
    parser.add_argument('--n_users', type=int, default=50, help='ç”¨æˆ·æ•°é‡')
    parser.add_argument('--user_distribution', type=str, default='uniform', 
                       choices=['uniform', 'cluster', 'hotspot'])
    parser.add_argument('--channel_model', type=str, default='3gpp-36777',
                       choices=['free_space', 'urban', 'suburban', '3gpp-36777'])
    parser.add_argument('--max_hops', type=int, default=3, help='æœ€å¤§è·³æ•°ï¼ˆä»…åœºæ™¯2ï¼‰')
    
    # æ—¥å¿—å‚æ•°
    parser.add_argument('--log_level', type=str, default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'])
    parser.add_argument('--console_log_level', type=str, default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'])
    
    return parser.parse_args()

def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # åˆ›å»ºé…ç½®
    config = Config()
    
    # ç¡®å®šè®­ç»ƒæ ·æœ¬æ•°ï¼šä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œå…¶æ¬¡ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼
    if args.samples is not None:
        total_samples = args.samples
        print(f"ğŸ“ˆ ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„è®­ç»ƒæ ·æœ¬æ•°: {total_samples:,}")
    else:
        total_samples = int(config.total_timesteps)
        print(f"ğŸ“ˆ ä»config.pyè¯»å–è®­ç»ƒæ ·æœ¬æ•°: {total_samples:,}")
    
    print("ğŸš€ åŸºäºSB3å‘é‡åŒ–ç¯å¢ƒçš„HMASDè®­ç»ƒ")
    print("=" * 80)
    print(f"ğŸ“Š å‘é‡åŒ–ç¯å¢ƒæ•°é‡: {args.n_envs}")
    print(f"ğŸ¯ è®­ç»ƒæ ·æœ¬æ•°: {total_samples:,}")
    print(f"ğŸ”§ è®¾å¤‡: {args.device}")
    print(f"ğŸŒ åœºæ™¯: {args.scenario}")
    print(f"ğŸš æ— äººæœºæ•°é‡: {args.n_uavs}")
    print(f"ğŸ‘¥ ç”¨æˆ·æ•°é‡: {args.n_users}")
    
    # éªŒè¯å¹¶æ‰“å°é…ç½®
    config.validate_training_mode()
    config.validate_rollout_config()
    print(config.get_rollout_summary())
    
    try:
        # åˆ›å»ºå‘é‡åŒ–è®­ç»ƒå™¨
        trainer = VectorizedHMASDTrainer(config, args)
        
        # å¼€å§‹è®­ç»ƒ
        trainer.train(total_samples=total_samples)
        
        print("ğŸ‰ å‘é‡åŒ–è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        try:
            shutdown_logging()
        except:
            pass

if __name__ == "__main__":
    main()

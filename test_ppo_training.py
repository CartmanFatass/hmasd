import os
import sys
import argparse
import logging
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from logger import init_multiproc_logging, get_logger, shutdown_logging, LOG_LEVELS

def test_ppo_imports():
    """æµ‹è¯•PPOè®­ç»ƒè„šæœ¬çš„å¯¼å…¥"""
    print("æµ‹è¯•å¯¼å…¥...")
    
    try:
        # æµ‹è¯•åŸºç¡€å¯¼å…¥
        import torch
        import numpy as np
        from stable_baselines3 import PPO
        from stable_baselines3.common.vec_env import SubprocVecEnv
        print("âœ“ åŸºç¡€åº“å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•é¡¹ç›®å¯¼å…¥
        from config_1 import Config
        from envs.pettingzoo.scenario1 import UAVBaseStationEnv
        from envs.pettingzoo.scenario2 import UAVCooperativeNetworkEnv
        from envs.pettingzoo.env_adapter import ParallelToArrayAdapter
        print("âœ“ é¡¹ç›®æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•PPOè®­ç»ƒè„šæœ¬å¯¼å…¥
        from train_ppo_enhanced_tracking import (
            EnhancedRewardTracker, 
            CustomActorCriticPolicy,
            TrainingCallback,
            make_env,
            get_device
        )
        print("âœ“ PPOè®­ç»ƒè„šæœ¬å¯¼å…¥æˆåŠŸ")
        
        return True
        
    except ImportError as e:
        print(f"âœ— å¯¼å…¥å¤±è´¥: {e}")
        return False

def test_environment_creation():
    """æµ‹è¯•ç¯å¢ƒåˆ›å»º"""
    print("\næµ‹è¯•ç¯å¢ƒåˆ›å»º...")
    
    try:
        from config_1 import Config
        from train_ppo_enhanced_tracking import make_env
        
        config = Config()
        
        # æµ‹è¯•ç¯å¢ƒåˆ›å»ºå‡½æ•°
        env_fn = make_env(
            scenario=2,
            n_uavs=3,
            n_users=10,
            user_distribution='uniform',
            channel_model='3gpp-36777',
            max_hops=3,
            render_mode=None,
            rank=0,
            seed=42
        )
        
        # åˆ›å»ºç¯å¢ƒå®ä¾‹
        env = env_fn()
        print(f"âœ“ ç¯å¢ƒåˆ›å»ºæˆåŠŸ: ç±»å‹ {type(env)}")
        
        # æµ‹è¯•ç¯å¢ƒé‡ç½®
        obs, info = env.reset()
        print(f"âœ“ ç¯å¢ƒé‡ç½®æˆåŠŸ: è§‚æµ‹å½¢çŠ¶ {obs.shape}")
        
        # æµ‹è¯•éšæœºåŠ¨ä½œ
        action_space = env.action_space
        random_action = action_space.sample()
        next_obs, reward, terminated, truncated, info = env.step(random_action)
        print(f"âœ“ ç¯å¢ƒæ­¥éª¤æˆåŠŸ: å¥–åŠ± {reward:.3f}")
        
        env.close()
        print("âœ“ ç¯å¢ƒå…³é—­æˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âœ— ç¯å¢ƒæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_ppo_model_creation():
    """æµ‹è¯•PPOæ¨¡å‹åˆ›å»º"""
    print("\næµ‹è¯•PPOæ¨¡å‹åˆ›å»º...")
    
    try:
        import torch
        from stable_baselines3 import PPO
        from stable_baselines3.common.vec_env import DummyVecEnv
        from train_ppo_enhanced_tracking import CustomActorCriticPolicy, make_env
        
        # åˆ›å»ºå•ä¸ªç¯å¢ƒç”¨äºæµ‹è¯•
        env_fn = make_env(
            scenario=1,
            n_uavs=3,
            n_users=10,
            user_distribution='uniform',
            channel_model='3gpp-36777',
            max_hops=None,
            render_mode=None,
            rank=0,
            seed=42
        )
        
        # åŒ…è£…ä¸ºå‘é‡åŒ–ç¯å¢ƒ
        vec_env = DummyVecEnv([env_fn])
        print(f"âœ“ å‘é‡åŒ–ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºPPOæ¨¡å‹
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        model = PPO(
            CustomActorCriticPolicy,
            vec_env,
            learning_rate=3e-4,
            n_steps=64,  # å°å€¼ç”¨äºæµ‹è¯•
            batch_size=32,
            n_epochs=2,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            ent_coef=0.01,
            vf_coef=0.5,
            max_grad_norm=0.5,
            verbose=0,
            device=device
        )
        
        print(f"âœ“ PPOæ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œè®¾å¤‡: {device}")
        print(f"âœ“ ç­–ç•¥ç½‘ç»œ: {type(model.policy)}")
        
        # æµ‹è¯•æ¨¡å‹é¢„æµ‹
        obs = vec_env.reset()
        actions, _ = model.predict(obs, deterministic=True)
        print(f"âœ“ æ¨¡å‹é¢„æµ‹æˆåŠŸ: åŠ¨ä½œå½¢çŠ¶ {actions.shape}")
        
        vec_env.close()
        return True
        
    except Exception as e:
        print(f"âœ— PPOæ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_reward_tracker():
    """æµ‹è¯•å¥–åŠ±è¿½è¸ªå™¨"""
    print("\næµ‹è¯•å¥–åŠ±è¿½è¸ªå™¨...")
    
    try:
        import tempfile
        from train_ppo_enhanced_tracking import EnhancedRewardTracker
        from config_1 import Config
        
        config = Config()
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        with tempfile.TemporaryDirectory() as temp_dir:
            tracker = EnhancedRewardTracker(temp_dir, config)
            print("âœ“ å¥–åŠ±è¿½è¸ªå™¨åˆ›å»ºæˆåŠŸ")
            
            # æµ‹è¯•è®°å½•è®­ç»ƒæ­¥éª¤
            tracker.log_training_step(
                step=1,
                env_id=0,
                reward=10.5,
                info={'served_users': 8, 'total_users': 10}
            )
            print("âœ“ è®­ç»ƒæ­¥éª¤è®°å½•æˆåŠŸ")
            
            # æµ‹è¯•è®°å½•episodeå®Œæˆ
            tracker.log_episode_completion(
                episode_num=1,
                env_id=0,
                total_reward=100.0,
                episode_length=50,
                info={'coverage_ratio': 0.8}
            )
            print("âœ“ Episodeå®Œæˆè®°å½•æˆåŠŸ")
            
            # æµ‹è¯•è·å–æ‘˜è¦ç»Ÿè®¡
            summary = tracker.get_summary_statistics()
            print(f"âœ“ æ‘˜è¦ç»Ÿè®¡è·å–æˆåŠŸ: {len(summary)} é¡¹æŒ‡æ ‡")
            
        return True
        
    except Exception as e:
        print(f"âœ— å¥–åŠ±è¿½è¸ªå™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_training_callback():
    """æµ‹è¯•è®­ç»ƒå›è°ƒå‡½æ•°"""
    print("\næµ‹è¯•è®­ç»ƒå›è°ƒå‡½æ•°...")
    
    try:
        import tempfile
        from train_ppo_enhanced_tracking import TrainingCallback, EnhancedRewardTracker
        from config_1 import Config
        
        config = Config()
        
        with tempfile.TemporaryDirectory() as temp_dir:
            tracker = EnhancedRewardTracker(temp_dir, config)
            callback = TrainingCallback(
                reward_tracker=tracker,
                eval_freq=100,
                verbose=0
            )
            print("âœ“ è®­ç»ƒå›è°ƒå‡½æ•°åˆ›å»ºæˆåŠŸ")
            
            # æ¨¡æ‹Ÿå›è°ƒè°ƒç”¨
            result = callback._on_step()
            print(f"âœ“ å›è°ƒå‡½æ•°æ‰§è¡ŒæˆåŠŸ: è¿”å› {result}")
            
        return True
        
    except Exception as e:
        print(f"âœ— è®­ç»ƒå›è°ƒå‡½æ•°æµ‹è¯•å¤±è´¥: {e}")
        return False

def run_quick_training_test():
    """è¿è¡Œå¿«é€Ÿè®­ç»ƒæµ‹è¯•"""
    print("\nè¿è¡Œå¿«é€Ÿè®­ç»ƒæµ‹è¯•...")
    
    try:
        import tempfile
        import torch
        from stable_baselines3 import PPO
        from stable_baselines3.common.vec_env import DummyVecEnv
        from train_ppo_enhanced_tracking import (
            CustomActorCriticPolicy,
            EnhancedRewardTracker,
            TrainingCallback,
            make_env
        )
        from config_1 import Config
        
        config = Config()
        
        with tempfile.TemporaryDirectory() as temp_dir:
            # åˆ›å»ºç®€å•ç¯å¢ƒ
            env_fn = make_env(
                scenario=1,
                n_uavs=2,
                n_users=5,
                user_distribution='uniform',
                channel_model='3gpp-36777',
                max_hops=None,
                render_mode=None,
                rank=0,
                seed=42
            )
            
            vec_env = DummyVecEnv([env_fn])
            
            # åˆ›å»ºå¥–åŠ±è¿½è¸ªå™¨å’Œå›è°ƒ
            tracker = EnhancedRewardTracker(temp_dir, config)
            callback = TrainingCallback(tracker, eval_freq=50, verbose=0)
            
            # åˆ›å»ºPPOæ¨¡å‹
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            model = PPO(
                CustomActorCriticPolicy,
                vec_env,
                learning_rate=3e-4,
                n_steps=32,  # å°å€¼ç”¨äºå¿«é€Ÿæµ‹è¯•
                batch_size=16,
                n_epochs=2,
                gamma=0.99,
                gae_lambda=0.95,
                clip_range=0.2,
                ent_coef=0.01,
                vf_coef=0.5,
                max_grad_norm=0.5,
                verbose=0,
                device=device
            )
            
            print("âœ“ æµ‹è¯•ç¯å¢ƒå’Œæ¨¡å‹è®¾ç½®å®Œæˆ")
            
            # è¿è¡ŒçŸ­æœŸè®­ç»ƒ
            model.learn(
                total_timesteps=128,  # å¾ˆå°çš„æ­¥æ•°ç”¨äºæµ‹è¯•
                callback=callback,
                progress_bar=False
            )
            
            print("âœ“ å¿«é€Ÿè®­ç»ƒæµ‹è¯•æˆåŠŸå®Œæˆ")
            
            # æµ‹è¯•ä¿å­˜å’ŒåŠ è½½
            model_path = os.path.join(temp_dir, "test_model.zip")
            model.save(model_path)
            print("âœ“ æ¨¡å‹ä¿å­˜æˆåŠŸ")
            
            loaded_model = PPO.load(model_path, device=device)
            print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            vec_env.close()
        
        return True
        
    except Exception as e:
        print(f"âœ— å¿«é€Ÿè®­ç»ƒæµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=" * 60)
    print("PPOè®­ç»ƒè„šæœ¬æµ‹è¯•")
    print("=" * 60)
    
    # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
    init_multiproc_logging(
        log_dir="logs",
        log_file=f"test_ppo_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log",
        file_level=logging.INFO,
        console_level=logging.WARNING
    )
    
    logger = get_logger("PPO-Test")
    logger.info("å¼€å§‹PPOè®­ç»ƒè„šæœ¬æµ‹è¯•")
    
    tests = [
        ("å¯¼å…¥æµ‹è¯•", test_ppo_imports),
        ("ç¯å¢ƒåˆ›å»ºæµ‹è¯•", test_environment_creation),
        ("PPOæ¨¡å‹åˆ›å»ºæµ‹è¯•", test_ppo_model_creation),
        ("å¥–åŠ±è¿½è¸ªå™¨æµ‹è¯•", test_reward_tracker),
        ("è®­ç»ƒå›è°ƒå‡½æ•°æµ‹è¯•", test_training_callback),
        ("å¿«é€Ÿè®­ç»ƒæµ‹è¯•", run_quick_training_test)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        try:
            success = test_func()
            results.append((test_name, success))
            if success:
                logger.info(f"{test_name}: é€šè¿‡")
            else:
                logger.error(f"{test_name}: å¤±è´¥")
        except Exception as e:
            print(f"âœ— {test_name} å‡ºç°å¼‚å¸¸: {e}")
            results.append((test_name, False))
            logger.error(f"{test_name}: å¼‚å¸¸ - {e}")
    
    # æ€»ç»“ç»“æœ
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»“æœæ€»ç»“:")
    print("=" * 60)
    
    passed = 0
    failed = 0
    
    for test_name, success in results:
        status = "âœ“ é€šè¿‡" if success else "âœ— å¤±è´¥"
        print(f"{test_name:.<40} {status}")
        if success:
            passed += 1
        else:
            failed += 1
    
    print(f"\næ€»è®¡: {passed} é€šè¿‡, {failed} å¤±è´¥")
    
    if failed == 0:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡äº†ï¼PPOè®­ç»ƒè„šæœ¬å‡†å¤‡å°±ç»ªã€‚")
        logger.info("æ‰€æœ‰æµ‹è¯•é€šè¿‡")
    else:
        print(f"\nâš ï¸  æœ‰ {failed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³é—®é¢˜ã€‚")
        logger.warning(f"æœ‰ {failed} ä¸ªæµ‹è¯•å¤±è´¥")
    
    shutdown_logging()
    return failed == 0

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)

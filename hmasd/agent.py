import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.optim import Adam
from torch.distributions import Categorical
import time
import os
from collections import deque
from torch.utils.tensorboard import SummaryWriter

from logger import main_logger
from hmasd.networks import SkillCoordinator, SkillDiscoverer, TeamDiscriminator, IndividualDiscriminator
from hmasd.utils import ReplayBuffer, StateSkillDataset, compute_gae, compute_ppo_loss, one_hot

def huber_loss(input, target, delta=1.0, reduction='mean'):
    """
    è®¡ç®—Huber Lossï¼ˆä¹Ÿç§°ä¸ºSmooth L1 Lossï¼‰
    
    å‚æ•°:
        input: é¢„æµ‹å€¼ [batch_size, ...]
        target: ç›®æ ‡å€¼ [batch_size, ...]
        delta: Huber Lossçš„deltaå‚æ•°ï¼Œæ§åˆ¶L1/L2åˆ‡æ¢ç‚¹
        reduction: 'mean', 'sum', 'none'
        
    è¿”å›:
        loss: HuberæŸå¤±å€¼
    """
    residual = torch.abs(input - target)
    condition = residual < delta
    
    # å½“|residual| < deltaæ—¶ä½¿ç”¨L2æŸå¤±ï¼š0.5 * residual^2 / delta
    # å½“|residual| >= deltaæ—¶ä½¿ç”¨L1æŸå¤±ï¼šresidual - 0.5 * delta
    loss = torch.where(
        condition,
        0.5 * residual.pow(2) / delta,
        residual - 0.5 * delta
    )
    
    if reduction == 'mean':
        return loss.mean()
    elif reduction == 'sum':
        return loss.sum()
    else:
        return loss

class HMASDAgent:
    """
    å±‚æ¬¡åŒ–å¤šæ™ºèƒ½ä½“æŠ€èƒ½å‘ç°ï¼ˆHMASDï¼‰ä»£ç†
    """
    def __init__(self, config, log_dir='logs', device=None, debug=False):
        """
        åˆå§‹åŒ–HMASDä»£ç†
        
        å‚æ•°:
            config: é…ç½®å¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰è¶…å‚æ•°
            log_dir: TensorBoardæ—¥å¿—ç›®å½•
            device: è®¡ç®—è®¾å¤‡ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æ£€æµ‹
            debug: æ˜¯å¦å¯ç”¨è‡ªåŠ¨æ±‚å¯¼å¼‚å¸¸æ£€æµ‹
        """
        # å¯ç”¨å¼‚å¸¸æ£€æµ‹ä»¥å¸®åŠ©è°ƒè¯•
        if debug:
            torch.autograd.set_detect_anomaly(True)
            main_logger.info("å·²å¯ç”¨è‡ªåŠ¨æ±‚å¯¼å¼‚å¸¸æ£€æµ‹")
            
        self.config = config
        self.device = device if device is not None else torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        main_logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # ç¡®ä¿ç¯å¢ƒç»´åº¦å·²è®¾ç½®
        assert config.state_dim is not None, "å¿…é¡»å…ˆè®¾ç½®state_dim"
        assert config.obs_dim is not None, "å¿…é¡»å…ˆè®¾ç½®obs_dim"
        
        # ã€ä¿®å¤E1ã€‘è®¾ç½®loggerå±æ€§ä»¥æ”¯æŒåœ¨å¤šä¸ªæ–¹æ³•ä¸­ä½¿ç”¨
        self.logger = main_logger
        
        # åˆå§‹åŒ–TensorBoard
        self.log_dir = log_dir
        os.makedirs(log_dir, exist_ok=True)
        self.writer = SummaryWriter(log_dir)
        main_logger.debug(f"HMASDAgent.__init__: SummaryWriter created: {self.writer}")
        self.global_step = 0
        
        # è®­ç»ƒæ¨¡å¼æ§åˆ¶
        self.rollout_based_training = getattr(config, 'rollout_based_training', True)
        self.episode_based_training = getattr(config, 'episode_based_training', False)
        self.sync_training_mode = getattr(config, 'sync_training_mode', False)
        
        # ç¡®ä¿åªæœ‰ä¸€ç§è®­ç»ƒæ¨¡å¼è¢«å¯ç”¨
        active_modes = sum([self.rollout_based_training, self.episode_based_training, self.sync_training_mode])
        if active_modes > 1:
            main_logger.warning("æ£€æµ‹åˆ°å¤šä¸ªè®­ç»ƒæ¨¡å¼è¢«å¯ç”¨ï¼Œå°†ä½¿ç”¨rollout_based_trainingä½œä¸ºé»˜è®¤æ¨¡å¼")
            self.rollout_based_training = True
            self.episode_based_training = False
            self.sync_training_mode = False
        elif active_modes == 0:
            main_logger.warning("æ²¡æœ‰è®­ç»ƒæ¨¡å¼è¢«å¯ç”¨ï¼Œå°†ä½¿ç”¨rollout_based_trainingä½œä¸ºé»˜è®¤æ¨¡å¼")
            self.rollout_based_training = True
        
        if self.rollout_based_training:
            # Rollout-basedè®­ç»ƒçŠ¶æ€ç®¡ç†
            self.rollout_length = config.rollout_length
            self.num_parallel_envs = config.num_parallel_envs
            self.ppo_epochs = config.ppo_epochs
            self.num_mini_batch = config.num_mini_batch
            self.steps_collected = 0               # å½“å‰rolloutæ”¶é›†çš„æ­¥æ•°
            self.rollout_count = 0                 # rolloutè®¡æ•°å™¨
            self.total_steps_collected = 0         # æ€»æ”¶é›†æ­¥æ•°
            main_logger.info(f"Rollout-basedè®­ç»ƒæ¨¡å¼å·²å¯ç”¨ï¼Œrollouté•¿åº¦: {self.rollout_length}, å¹¶è¡Œç¯å¢ƒ: {self.num_parallel_envs}")
            
        elif self.episode_based_training:
            # Episode-basedè®­ç»ƒçŠ¶æ€ç®¡ç†
            self.episodes_collected = 0
            self.update_frequency = config.update_frequency
            self.min_episodes_for_update = config.min_episodes_for_update
            self.max_episodes_per_update = config.max_episodes_per_update
            self.min_high_level_samples = config.min_high_level_samples
            self.min_low_level_samples = config.min_low_level_samples
            self.last_update_episode = 0
            main_logger.info(f"Episode-basedè®­ç»ƒæ¨¡å¼å·²å¯ç”¨ï¼Œæ›´æ–°é¢‘ç‡: {self.update_frequency} episodes")
            
        else:  # sync_training_mode
            # åŒæ­¥è®­ç»ƒæ§åˆ¶æœºåˆ¶ï¼ˆå…¼å®¹æ€§ä¿ç•™ï¼‰
            self.sync_mode = True                     # å¯ç”¨åŒæ­¥è®­ç»ƒæ¨¡å¼
            self.collection_enabled = True            # æ•°æ®æ”¶é›†å¼€å…³
            self.policy_version = 0                   # ç­–ç•¥ç‰ˆæœ¬å·
            self.sync_batch_size = config.batch_size  # åŒæ­¥batchå¤§å°ï¼ˆä»é…ç½®è·å–ï¼‰
            self.samples_collected_this_round = 0     # æœ¬è½®æ”¶é›†çš„æ ·æœ¬æ•°
            self.last_sync_step = 0                   # ä¸Šæ¬¡åŒæ­¥æ›´æ–°çš„æ­¥æ•°
            main_logger.info(f"åŒæ­¥è®­ç»ƒæ¨¡å¼å·²å¯ç”¨ï¼ŒåŒæ­¥batchå¤§å°: {self.sync_batch_size}")
        
        # ç¡®ä¿æ‰€æœ‰è®­ç»ƒæ¨¡å¼éƒ½æœ‰å¿…è¦çš„å±æ€§ï¼ˆå‘åå…¼å®¹ï¼‰
        if not hasattr(self, 'sync_mode'):
            self.sync_mode = False  # é»˜è®¤å…³é—­åŒæ­¥æ¨¡å¼
        if not hasattr(self, 'collection_enabled'):
            self.collection_enabled = True  # é»˜è®¤å¯ç”¨æ•°æ®æ”¶é›†
        
        # åˆ›å»ºç½‘ç»œ
        self.skill_coordinator = SkillCoordinator(config).to(self.device)
        self.skill_discoverer = SkillDiscoverer(config, logger=main_logger).to(self.device) # Pass logger
        self.team_discriminator = TeamDiscriminator(config).to(self.device)
        self.individual_discriminator = IndividualDiscriminator(config).to(self.device)
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        self.coordinator_optimizer = Adam(
            self.skill_coordinator.parameters(),
            lr=config.lr_coordinator
        )
        self.discoverer_optimizer = Adam(
            self.skill_discoverer.parameters(),
            lr=config.lr_discoverer
        )
        self.discriminator_optimizer = Adam(
            list(self.team_discriminator.parameters()) + 
            list(self.individual_discriminator.parameters()),
            lr=config.lr_discriminator
        )
        
        # åˆ›å»ºç»éªŒå›æ”¾ç¼“å†²åŒº
        self.high_level_buffer = ReplayBuffer(config.buffer_size)
        self.high_level_buffer_with_logprobs = []  # æ–°å¢ï¼šé«˜å±‚ç»éªŒç¼“å†²åŒºï¼ˆå¸¦log probabilitiesï¼‰
        self.low_level_buffer = ReplayBuffer(config.buffer_size)
        self.state_skill_dataset = StateSkillDataset(config.buffer_size)
        
        # å…¶ä»–åˆå§‹åŒ–
        self.current_team_skill = None  # å½“å‰å›¢é˜ŸæŠ€èƒ½ (ä¿ç•™ç”¨äºå•ç¯å¢ƒå…¼å®¹æ€§)
        self.current_agent_skills = None  # å½“å‰ä¸ªä½“æŠ€èƒ½åˆ—è¡¨ (ä¿ç•™ç”¨äºå•ç¯å¢ƒå…¼å®¹æ€§)
        self.skill_change_timer = 0  # æŠ€èƒ½æ›´æ¢è®¡æ—¶å™¨ (ä¿ç•™ç”¨äºå•ç¯å¢ƒå…¼å®¹æ€§)
        self.current_high_level_reward_sum = 0.0 # å½“å‰æŠ€èƒ½å‘¨æœŸçš„ç´¯ç§¯å¥–åŠ±
        self.env_reward_sums = {}  # ç”¨äºå­˜å‚¨æ¯ä¸ªç¯å¢ƒIDçš„ç´¯ç§¯å¥–åŠ±ï¼Œç”¨äºå¹¶è¡Œè®­ç»ƒ
        self.env_timers = {}  # ç”¨äºå­˜å‚¨æ¯ä¸ªç¯å¢ƒIDçš„æŠ€èƒ½è®¡æ—¶å™¨ï¼Œç”¨äºå¹¶è¡Œè®­ç»ƒ
        
        # æ–°å¢ï¼šç¯å¢ƒç‰¹å®šçš„çŠ¶æ€è·Ÿè¸ª
        self.env_team_skills = {}  # å„ç¯å¢ƒçš„å½“å‰å›¢é˜ŸæŠ€èƒ½
        self.env_agent_skills = {}  # å„ç¯å¢ƒçš„å½“å‰ä¸ªä½“æŠ€èƒ½åˆ—è¡¨
        self.env_log_probs = {}  # å„ç¯å¢ƒçš„log probabilities
        self.env_hidden_states = {}  # å„ç¯å¢ƒçš„GRUéšè—çŠ¶æ€
        
        # é¢„åˆå§‹åŒ–32ä¸ªå¹¶è¡Œç¯å¢ƒçš„å¥–åŠ±ç´¯ç§¯å’ŒæŠ€èƒ½è®¡æ—¶å™¨(ä¸config.num_envs=32å¯¹åº”)
        for i in range(32):
            self.env_reward_sums[i] = 0.0
            self.env_timers[i] = 0
            self.env_team_skills[i] = None
            self.env_agent_skills[i] = None
            self.env_log_probs[i] = None
            self.env_hidden_states[i] = None
        self.accumulated_rewards = 0.0  # ç”¨äºæµ‹è¯•çš„ç´¯ç§¯å¥–åŠ±å±æ€§
        self.episode_rewards = []  # è®°å½•æ¯ä¸ªå®Œæ•´episodeçš„å¥–åŠ±

        # ç”¨äºè®°å½•æ•´ä¸ªepisodeçš„æŠ€èƒ½ä½¿ç”¨è®¡æ•°
        self.episode_team_skill_counts = {}
        # å°†åœ¨ç¬¬ä¸€æ¬¡åˆ†é…æŠ€èƒ½æ—¶æ ¹æ®å®é™…æ™ºèƒ½ä½“æ•°é‡åˆå§‹åŒ–
        self.episode_agent_skill_counts = [] 
        
        # è®­ç»ƒæŒ‡æ ‡
        self.training_info = {
            'high_level_loss': [],
            'low_level_loss': [],
            'discriminator_loss': [],
            'team_skill_entropy': [],
            'agent_skill_entropy': [],
            'action_entropy': [],
            'episode_rewards': [],
            # æ–°å¢ç”¨äºè®°å½•å†…åœ¨å¥–åŠ±ç»„ä»¶å’Œä»·å€¼ä¼°è®¡çš„åˆ—è¡¨
            'intrinsic_reward_env_component': [],
            'intrinsic_reward_team_disc_component': [],
            'intrinsic_reward_ind_disc_component': [],
            'intrinsic_reward_low_level_average': [], # ç”¨äºè®°å½•æ‰¹æ¬¡å¹³å‡å†…åœ¨å¥–åŠ±
            'coordinator_state_value_mean': [],
            'coordinator_agent_value_mean': [],
            'discoverer_value_mean': []
        }
        
        # ç”¨äºå‡å°‘é«˜å±‚ç¼“å†²åŒºè­¦å‘Šæ—¥å¿—çš„è®¡æ•°å™¨
        self.high_level_buffer_warning_counter = 0
        self.last_high_level_buffer_size = 0
        
        # é«˜å±‚ç»éªŒç»Ÿè®¡
        self.high_level_samples_total = 0        # æ€»æ”¶é›†é«˜å±‚æ ·æœ¬æ•°
        self.high_level_samples_by_env = {}      # å„ç¯å¢ƒè´¡çŒ®çš„æ ·æœ¬æ•°
        self.high_level_samples_by_reason = {'æŠ€èƒ½å‘¨æœŸç»“æŸ': 0, 'ç¯å¢ƒç»ˆæ­¢': 0, 'å‘¨æœŸå®Œæˆæ£€æµ‹': 0}  # æ”¶é›†åŸå› ç»Ÿè®¡
        
        # é«˜å±‚ç»éªŒæ”¶é›†å¢å¼º
        self.env_last_contribution = {}          # è·Ÿè¸ªæ¯ä¸ªç¯å¢ƒä¸Šæ¬¡è´¡çŒ®é«˜å±‚æ ·æœ¬çš„æ—¶é—´æ­¥
        self.force_high_level_collection = {}    # å¼ºåˆ¶é‡‡é›†æ ‡å¿—ï¼Œç”¨äºç¡®ä¿æ‰€æœ‰ç¯å¢ƒéƒ½èƒ½è´¡çŒ®æ ·æœ¬
        self.env_reward_thresholds = {}          # ç¯å¢ƒç‰¹å®šçš„å¥–åŠ±é˜ˆå€¼
        
        # è®°å½•å†…åœ¨å¥–åŠ±ç»„æˆéƒ¨åˆ†çš„ç´¯ç§¯å€¼ï¼Œç”¨äºç»Ÿè®¡åˆ†æ
        self.cumulative_env_reward = 0.0
        self.cumulative_team_disc_reward = 0.0
        self.cumulative_ind_disc_reward = 0.0
        self.reward_component_counts = 0
        
        # Huber Lossè‡ªé€‚åº”deltaå‚æ•°
        self.adaptive_coordinator_delta = getattr(config, 'huber_coordinator_delta', 1.0)
        self.adaptive_discoverer_delta = getattr(config, 'huber_discoverer_delta', 1.0)
        self.delta_update_count = 0
    
    def should_sync_update(self):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥è¿›è¡ŒåŒæ­¥æ›´æ–°"""
        if not self.sync_mode:
            return False
        
        return self.samples_collected_this_round >= self.sync_batch_size
    
    def enable_data_collection(self):
        """å¯ç”¨æ•°æ®æ”¶é›†"""
        self.collection_enabled = True
        main_logger.debug(f"æ•°æ®æ”¶é›†å·²å¯ç”¨ï¼Œç­–ç•¥ç‰ˆæœ¬: {self.policy_version}")
    
    def disable_data_collection(self):
        """ç¦ç”¨æ•°æ®æ”¶é›†"""
        self.collection_enabled = False
        main_logger.debug(f"æ•°æ®æ”¶é›†å·²ç¦ç”¨ï¼Œç­–ç•¥ç‰ˆæœ¬: {self.policy_version}")
    
    def force_collect_pending_high_level_experiences(self):
        """
        åœ¨åŒæ­¥æ›´æ–°å‰å¼ºåˆ¶æ”¶é›†æ‰€æœ‰æœªå®ŒæˆæŠ€èƒ½å‘¨æœŸçš„é«˜å±‚ç»éªŒ
        ç¡®ä¿é«˜å±‚ç¼“å†²åŒºæœ‰è¶³å¤Ÿçš„æ ·æœ¬è¿›è¡Œæ›´æ–°
        """
        pending_collections = 0
        
        for env_id in range(32):  # å‡è®¾æœ€å¤š32ä¸ªå¹¶è¡Œç¯å¢ƒ
            timer = self.env_timers.get(env_id, 0)
            reward_sum = self.env_reward_sums.get(env_id, 0.0)
            
            # å¦‚æœè¯¥ç¯å¢ƒæœ‰æœªå®Œæˆçš„æŠ€èƒ½å‘¨æœŸä¸”ç´¯ç§¯å¥–åŠ±ä¸ä¸º0ï¼Œå¼ºåˆ¶æ”¶é›†
            if timer > 0 and timer < self.config.k - 1:
                main_logger.info(f"å¼ºåˆ¶æ”¶é›†ç¯å¢ƒ{env_id}çš„é«˜å±‚ç»éªŒ: timer={timer}, ç´¯ç§¯å¥–åŠ±={reward_sum:.4f}")
                self.force_high_level_collection[env_id] = True
                pending_collections += 1
        
        if pending_collections > 0:
            main_logger.info(f"åŒæ­¥æ›´æ–°å‰å¼ºåˆ¶æ”¶é›†äº† {pending_collections} ä¸ªç¯å¢ƒçš„pendingé«˜å±‚ç»éªŒ")
        
        return pending_collections

    def sync_update(self):
        """
        æ”¹è¿›çš„åŒæ­¥æ›´æ–°æœºåˆ¶ - åŸºäºHMASDè®ºæ–‡ç®—æ³•
        ç¡®ä¿é«˜å±‚ç»éªŒæ”¶é›†ä¸å—ä½å±‚ç¼“å†²åŒºæ¸…ç©ºå½±å“
        
        è¿”å›:
            update_info: æ›´æ–°ä¿¡æ¯å­—å…¸
        """
        # 1. åœæ­¢æ•°æ®æ”¶é›†
        self.disable_data_collection()
        
        # 2. è®°å½•åŒæ­¥æ›´æ–°ä¿¡æ¯
        samples_count = self.samples_collected_this_round
        main_logger.info(f"åŒæ­¥æ›´æ–°å¼€å§‹ - æ”¶é›†äº† {samples_count} ä¸ªæ ·æœ¬ï¼Œç­–ç•¥ç‰ˆæœ¬: {self.policy_version}")
        
        # 3. ã€æ–°å¢ã€‘å¼ºåˆ¶æ”¶é›†æ‰€æœ‰pendingçš„é«˜å±‚ç»éªŒ
        pending_count = self.force_collect_pending_high_level_experiences()
        
        # 4. è®°å½•ç¼“å†²åŒºçŠ¶æ€
        high_level_buffer_size_before = len(self.high_level_buffer)
        low_level_buffer_size_before = len(self.low_level_buffer)
        main_logger.info(f"åŒæ­¥æ›´æ–°å‰ç¼“å†²åŒºçŠ¶æ€ - é«˜å±‚: {high_level_buffer_size_before}, ä½å±‚: {low_level_buffer_size_before}")
        
        # 5. ã€ä¿®æ”¹é¡ºåºã€‘å…ˆæ›´æ–°é«˜å±‚ç­–ç•¥ï¼ˆä½¿ç”¨ç°æœ‰çš„é«˜å±‚ç»éªŒï¼‰
        coordinator_loss, coordinator_policy_loss, coordinator_value_loss, team_skill_entropy, agent_skill_entropy, \
        mean_coord_state_val, mean_coord_agent_val, mean_high_level_reward = self.update_coordinator()
        
        # 6. å†æ›´æ–°ä½å±‚ç­–ç•¥ï¼ˆä¼šæ¸…ç©ºä½å±‚ç¼“å†²åŒºï¼‰
        discoverer_loss, discoverer_policy_loss, discoverer_value_loss, action_entropy, \
        avg_intrinsic_reward, avg_env_comp, avg_team_disc_comp, avg_ind_disc_comp, \
        avg_discoverer_val = self.update_discoverer()
        
        # 7. æœ€åæ›´æ–°åˆ¤åˆ«å™¨
        discriminator_loss = self.update_discriminators()
        
        # 8. è®°å½•ç¼“å†²åŒºçŠ¶æ€å˜åŒ–
        high_level_buffer_size_after = len(self.high_level_buffer)
        low_level_buffer_size_after = len(self.low_level_buffer)
        main_logger.info(f"åŒæ­¥æ›´æ–°åç¼“å†²åŒºçŠ¶æ€ - é«˜å±‚: {high_level_buffer_size_after}, ä½å±‚: {low_level_buffer_size_after}")
        
        # 9. é‡ç½®åŒæ­¥çŠ¶æ€
        self.policy_version += 1
        self.samples_collected_this_round = 0
        self.last_sync_step = self.global_step
        
        # 10. é‡æ–°å¯ç”¨æ•°æ®æ”¶é›†
        self.enable_data_collection()
        
        # 11. è®°å½•åŒæ­¥æ›´æ–°å®Œæˆ
        main_logger.info(f"åŒæ­¥æ›´æ–°å®Œæˆ - ç­–ç•¥ç‰ˆæœ¬æ›´æ–°åˆ°: {self.policy_version}, å·²é‡ç½®æ ·æœ¬è®¡æ•°")
        
        # 12. æ„å»ºæ›´æ–°ä¿¡æ¯
        update_info = {
            'sync_samples_collected': samples_count,
            'policy_version': self.policy_version,
            'is_sync_update': True,
            'pending_high_level_forced': pending_count,
            'discriminator_loss': discriminator_loss,
            'coordinator_loss': coordinator_loss,
            'coordinator_policy_loss': coordinator_policy_loss,
            'coordinator_value_loss': coordinator_value_loss,
            'discoverer_loss': discoverer_loss,
            'discoverer_policy_loss': discoverer_policy_loss,
            'discoverer_value_loss': discoverer_value_loss,
            'team_skill_entropy': team_skill_entropy,
            'agent_skill_entropy': agent_skill_entropy,
            'action_entropy': action_entropy,
            'avg_intrinsic_reward': avg_intrinsic_reward,
            'avg_env_comp': avg_env_comp,
            'avg_team_disc_comp': avg_team_disc_comp,
            'avg_ind_disc_comp': avg_ind_disc_comp,
            'mean_coord_state_val': mean_coord_state_val,
            'mean_coord_agent_val': mean_coord_agent_val,
            'avg_discoverer_val': avg_discoverer_val,
            'mean_high_level_reward': mean_high_level_reward
        }
        
        return update_info
    
    def reset_buffers(self):
        """é‡ç½®æ‰€æœ‰ç»éªŒç¼“å†²åŒº"""
        main_logger.info("é‡ç½®æ‰€æœ‰ç»éªŒç¼“å†²åŒº")
        self.high_level_buffer.clear()
        self.high_level_buffer_with_logprobs = []
        self.low_level_buffer.clear()
        self.state_skill_dataset.clear()
        
        # é‡ç½®è®¡æ•°å™¨å’Œç´¯ç§¯å€¼
        self.current_high_level_reward_sum = 0.0
        self.accumulated_rewards = 0.0
        self.skill_change_timer = 0
        self.high_level_buffer_warning_counter = 0
        self.last_high_level_buffer_size = 0
        
        # é‡ç½®ç¯å¢ƒç‰¹å®šçš„å¥–åŠ±ç´¯ç§¯å­—å…¸å’Œè®¡æ—¶å™¨å­—å…¸
        self.env_reward_sums = {}
        self.env_timers = {}
        
        # é‡ç½®å¥–åŠ±ç»„æˆéƒ¨åˆ†çš„ç´¯ç§¯å€¼
        self.cumulative_env_reward = 0.0
        self.cumulative_team_disc_reward = 0.0
        self.cumulative_ind_disc_reward = 0.0
        self.reward_component_counts = 0
        
        # é‡ç½®æŠ€èƒ½ä½¿ç”¨è®¡æ•°
        self.episode_team_skill_counts = {}
        self.episode_agent_skill_counts = []
    
    def select_action(self, observations, agent_skills=None, deterministic=False, env_id=0):
        """
        ä¸ºæ‰€æœ‰æ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œ
        
        å‚æ•°:
            observations: æ‰€æœ‰æ™ºèƒ½ä½“çš„è§‚æµ‹ [n_agents, obs_dim]
            agent_skills: æ‰€æœ‰æ™ºèƒ½ä½“çš„æŠ€èƒ½ [n_agents]ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰æŠ€èƒ½
            deterministic: æ˜¯å¦ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
            env_id: ç¯å¢ƒIDï¼Œç”¨äºå¤šç¯å¢ƒå¹¶è¡Œè®­ç»ƒ
            
        è¿”å›:
            actions: æ‰€æœ‰æ™ºèƒ½ä½“çš„åŠ¨ä½œ [n_agents, action_dim]
            action_logprobs: æ‰€æœ‰æ™ºèƒ½ä½“çš„åŠ¨ä½œå¯¹æ•°æ¦‚ç‡ [n_agents]
        """
        if agent_skills is None:
            agent_skills = self.env_agent_skills.get(env_id, self.current_agent_skills)
            
        n_agents = observations.shape[0]
        actions = torch.zeros((n_agents, self.config.action_dim), device=self.device)
        action_logprobs = torch.zeros(n_agents, device=self.device)
        
        # åˆå§‹åŒ–æˆ–è·å–ç¯å¢ƒç‰¹å®šçš„GRUéšè—çŠ¶æ€
        if env_id not in self.env_hidden_states or self.env_hidden_states[env_id] is None:
            self.skill_discoverer.init_hidden(batch_size=1)
            self.env_hidden_states[env_id] = self.skill_discoverer.actor_hidden
        else:
            self.skill_discoverer.actor_hidden = self.env_hidden_states[env_id]
        
        with torch.no_grad():
            for i in range(n_agents):
                obs = torch.FloatTensor(observations[i]).unsqueeze(0).to(self.device)
                skill = torch.tensor(agent_skills[i], device=self.device)
                
                action, action_logprob, _ = self.skill_discoverer(obs, skill, deterministic)
                
                actions[i] = action.squeeze(0)
                action_logprobs[i] = action_logprob.squeeze(0)
        
        # ä¿å­˜æ›´æ–°åçš„GRUéšè—çŠ¶æ€
        self.env_hidden_states[env_id] = self.skill_discoverer.actor_hidden
        
        return actions.cpu().numpy(), action_logprobs.cpu().numpy()
    
    def assign_skills(self, state, observations, deterministic=False):
        """
        ä¸ºæ‰€æœ‰æ™ºèƒ½ä½“åˆ†é…æŠ€èƒ½
        
        å‚æ•°:
            state: å…¨å±€çŠ¶æ€ [state_dim]
            observations: æ‰€æœ‰æ™ºèƒ½ä½“çš„è§‚æµ‹ [n_agents, obs_dim]
            deterministic: æ˜¯å¦ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
            
        è¿”å›:
            team_skill: å›¢é˜ŸæŠ€èƒ½ç´¢å¼•
            agent_skills: ä¸ªä½“æŠ€èƒ½ç´¢å¼•åˆ—è¡¨ [n_agents]
            log_probs: åŒ…å«å›¢é˜ŸæŠ€èƒ½å’Œä¸ªä½“æŠ€èƒ½log probabilitiesçš„å­—å…¸
        """
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        obs_tensor = torch.FloatTensor(observations).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            team_skill, agent_skills, Z_logits, z_logits = self.skill_coordinator(
                state_tensor, obs_tensor, deterministic
            )
            
            # è®¡ç®—log probabilities
            Z_dist = torch.distributions.Categorical(logits=Z_logits)
            Z_log_prob = Z_dist.log_prob(team_skill)
            
            z_log_probs = []
            n_agents_actual = agent_skills.size(1)
            for i in range(n_agents_actual):
                zi_dist = torch.distributions.Categorical(logits=z_logits[i])
                zi_log_prob = zi_dist.log_prob(agent_skills[0, i])
                z_log_probs.append(zi_log_prob.item())
            
            log_probs = {
                'team_log_prob': Z_log_prob.item(),
                'agent_log_probs': z_log_probs
            }
        
        return team_skill.item(), agent_skills.squeeze(0).cpu().numpy(), log_probs
    
    def step(self, state, observations, ep_t, deterministic=False, env_id=0):
        """
        æ‰§è¡Œä¸€ä¸ªç¯å¢ƒæ­¥éª¤ - ä¿®å¤ç‰ˆæœ¬
        
        å…³é”®ä¿®å¤ï¼š
        1. ç»Ÿä¸€æŠ€èƒ½å‘¨æœŸåˆ¤æ–­é€»è¾‘
        2. ç¡®ä¿é«˜å±‚ç»éªŒåœ¨skillå®Œæˆæ—¶æ­£ç¡®æ”¶é›†
        3. é¿å…é‡å¤æ”¶é›†å’Œé—æ¼
        
        å‚æ•°:
            state: å…¨å±€çŠ¶æ€ [state_dim]
            observations: æ‰€æœ‰æ™ºèƒ½ä½“çš„è§‚æµ‹ [n_agents, obs_dim]
            ep_t: å½“å‰episodeä¸­çš„æ—¶é—´æ­¥
            deterministic: æ˜¯å¦ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥ï¼ˆç”¨äºè¯„ä¼°ï¼‰
            env_id: ç¯å¢ƒIDï¼Œç”¨äºå¤šç¯å¢ƒå¹¶è¡Œè®­ç»ƒ
            
        è¿”å›:
            actions: æ‰€æœ‰æ™ºèƒ½ä½“çš„åŠ¨ä½œ [n_agents, action_dim]
            info: é¢å¤–ä¿¡æ¯ï¼Œå¦‚å½“å‰æŠ€èƒ½
        """
        # è·å–æˆ–åˆå§‹åŒ–ç¯å¢ƒç‰¹å®šçš„çŠ¶æ€
        current_team_skill = self.env_team_skills.get(env_id, self.current_team_skill)
        current_agent_skills = self.env_agent_skills.get(env_id, self.current_agent_skills)
        env_timer = self.env_timers.get(env_id, 0)
        
        # ã€å…³é”®ä¿®å¤ã€‘ç»Ÿä¸€æŠ€èƒ½å‘¨æœŸåˆ¤æ–­é€»è¾‘
        # ä½¿ç”¨ç¯å¢ƒç‰¹å®šçš„timerè€Œä¸æ˜¯å…¨å±€ep_tæ¥åˆ¤æ–­æŠ€èƒ½å‘¨æœŸ
        skill_cycle_completed = env_timer >= self.config.k
        need_skill_reassignment = skill_cycle_completed or current_team_skill is None
        
        main_logger.debug(f"[SKILL_CYCLE_DEBUG] ç¯å¢ƒ{env_id} æŠ€èƒ½å‘¨æœŸæ£€æŸ¥: "
                          f"ep_t={ep_t}, env_timer={env_timer}, k={self.config.k}, "
                          f"cycle_completed={skill_cycle_completed}, "
                          f"need_reassignment={need_skill_reassignment}, "
                          f"current_team_skill={current_team_skill}")
        
        if need_skill_reassignment:
            # ã€ä¿®å¤ã€‘åœ¨é‡æ–°åˆ†é…æŠ€èƒ½ä¹‹å‰ï¼Œå…ˆæ”¶é›†ä¸Šä¸€å‘¨æœŸçš„é«˜å±‚ç»éªŒï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if skill_cycle_completed and current_team_skill is not None:
                old_reward_sum = self.env_reward_sums.get(env_id, 0.0)
                if old_reward_sum != 0.0:  # åªæœ‰å½“ç´¯ç§¯å¥–åŠ±ä¸ä¸º0æ—¶æ‰æ”¶é›†
                    observations_tensor = torch.FloatTensor(observations).to(self.device)
                    state_tensor = torch.FloatTensor(state).to(self.device)
                    
                    success = self._collect_high_level_experience(
                        env_id, state_tensor, observations_tensor,
                        current_team_skill, current_agent_skills, 
                        reason="æŠ€èƒ½å‘¨æœŸå®Œæˆ"
                    )
                    
                    if success:
                        main_logger.debug(f"[STEP_FIX] ç¯å¢ƒ{env_id}åœ¨æŠ€èƒ½é‡åˆ†é…å‰æˆåŠŸæ”¶é›†é«˜å±‚ç»éªŒ: "
                                        f"ç´¯ç§¯å¥–åŠ±={old_reward_sum:.4f}")
            
            # åˆ†é…æ–°æŠ€èƒ½
            team_skill, agent_skills, log_probs = self.assign_skills(state, observations, deterministic)
            
            # æ›´æ–°ç¯å¢ƒç‰¹å®šçš„çŠ¶æ€
            self.env_team_skills[env_id] = team_skill
            self.env_agent_skills[env_id] = agent_skills
            self.env_log_probs[env_id] = log_probs
            self.env_timers[env_id] = 0  # é‡ç½®è®¡æ—¶å™¨
            
            # é‡ç½®ç¯å¢ƒç‰¹å®šçš„ç´¯ç§¯å¥–åŠ±
            self.env_reward_sums[env_id] = 0.0
            
            # åŒæ—¶æ›´æ–°å…¨å±€çŠ¶æ€ï¼ˆç”¨äºå…¼å®¹æ€§ï¼‰
            if env_id == 0:  # åªæœ‰ç¯å¢ƒ0æ›´æ–°å…¨å±€çŠ¶æ€
                self.current_team_skill = team_skill
                self.current_agent_skills = agent_skills
                self.current_log_probs = log_probs
                self.skill_change_timer = 0
                self.current_high_level_reward_sum = 0.0
                self.accumulated_rewards = 0.0
            
            skill_changed = True
            
            # ã€è°ƒè¯•æ—¥å¿—ã€‘è®°å½•æŠ€èƒ½åˆ†é…ç»“æœ
            main_logger.debug(f"[SKILL_ASSIGN_DEBUG] ç¯å¢ƒ{env_id} æŠ€èƒ½å·²é‡æ–°åˆ†é…: "
                              f"team_skill={team_skill}, agent_skills={agent_skills}, "
                              f"timeré‡ç½®: {env_timer}â†’0, "
                              f"å¥–åŠ±ç´¯ç§¯é‡ç½®")

            # æ›´æ–°æŠ€èƒ½ä½¿ç”¨è®¡æ•°ï¼ˆåªæœ‰ç¯å¢ƒ0æ›´æ–°å…¨å±€è®¡æ•°ï¼‰
            if env_id == 0:
                # åˆå§‹åŒ– agent skill counts åˆ—è¡¨ï¼ˆå¦‚æœå°šæœªåˆå§‹åŒ–æˆ–æ™ºèƒ½ä½“æ•°é‡å·²æ›´æ”¹ï¼‰
                if not self.episode_agent_skill_counts or len(self.episode_agent_skill_counts) != len(agent_skills):
                    self.episode_agent_skill_counts = [{} for _ in range(len(agent_skills))]

                # è®°å½•å›¢é˜ŸæŠ€èƒ½
                self.episode_team_skill_counts[team_skill] = self.episode_team_skill_counts.get(team_skill, 0) + 1
                # è®°å½•ä¸ªä½“æŠ€èƒ½
                for i, agent_skill in enumerate(agent_skills):
                    self.episode_agent_skill_counts[i][agent_skill] = self.episode_agent_skill_counts[i].get(agent_skill, 0) + 1
        else:
            self.env_timers[env_id] += 1
            # åŒæ—¶æ›´æ–°å…¨å±€è®¡æ—¶å™¨ï¼ˆç”¨äºå…¼å®¹æ€§ï¼‰
            if env_id == 0:
                self.skill_change_timer += 1
            skill_changed = False
            main_logger.debug(f"ç¯å¢ƒ{env_id}æŠ€èƒ½æœªæ›´æ–°: timerå¢åŠ åˆ°{self.env_timers[env_id]}")
            
        # é€‰æ‹©åŠ¨ä½œï¼Œä½¿ç”¨ç¯å¢ƒç‰¹å®šçš„æŠ€èƒ½
        actions, action_logprobs = self.select_action(observations, self.env_agent_skills[env_id], deterministic, env_id)
        
        info = {
            'team_skill': self.env_team_skills[env_id],
            'agent_skills': self.env_agent_skills[env_id],
            'action_logprobs': action_logprobs,
            'skill_changed': skill_changed,
            'skill_timer': self.env_timers[env_id],
            'log_probs': self.env_log_probs[env_id],
            'env_id': env_id
        }
        
        return actions, info
    
    def collect_episode(self, env, max_steps=1000):
        """
        æ”¶é›†å®Œæ•´episodeçš„æ•°æ®ï¼ˆepisode-basedè®­ç»ƒæ¨¡å¼ï¼‰
        
        å‚æ•°:
            env: ç¯å¢ƒå®ä¾‹
            max_steps: æœ€å¤§æ­¥æ•°é™åˆ¶
            
        è¿”å›:
            episode_info: episodeä¿¡æ¯å­—å…¸
        """
        if not self.episode_based_training:
            raise ValueError("collect_episodeåªèƒ½åœ¨episode_based_trainingæ¨¡å¼ä¸‹ä½¿ç”¨")
        
        episode_reward = 0.0
        episode_steps = 0
        episode_start_time = time.time()
        
        # é‡ç½®ç¯å¢ƒ
        state, observations = env.reset()
        done = False
        ep_t = 0
        
        main_logger.info(f"å¼€å§‹æ”¶é›†Episode {self.episodes_collected + 1}")
        
        while not done and ep_t < max_steps:
            # æ‰§è¡Œstepï¼Œæ”¶é›†æ•°æ®ä½†ä¸è§¦å‘æ›´æ–°
            actions, info = self.step(state, observations, ep_t, deterministic=False, env_id=0)
            
            # ç¯å¢ƒäº¤äº’
            next_state, next_observations, rewards, dones = env.step(actions)
            
            # å­˜å‚¨ç»éªŒï¼ˆä»…æ”¶é›†ï¼Œä¸è§¦å‘æ›´æ–°ï¼‰
            success = self.store_transition(
                state, next_state, observations, next_observations,
                actions, rewards, dones, info['team_skill'], info['agent_skills'],
                info['action_logprobs'], info['log_probs'], info['skill_timer'], env_id=0
            )
            
            if not success:
                main_logger.warning(f"Episode {self.episodes_collected + 1}: æ­¥éª¤ {ep_t} å­˜å‚¨ç»éªŒå¤±è´¥")
            
            # æ›´æ–°çŠ¶æ€
            episode_reward += rewards if isinstance(rewards, (int, float)) else rewards.item()
            state, observations = next_state, next_observations
            done = dones if isinstance(dones, bool) else dones.any()
            ep_t += 1
            episode_steps += 1
            
            # æ¯100æ­¥è®°å½•ä¸€æ¬¡è¿›åº¦
            if ep_t % 100 == 0:
                main_logger.debug(f"Episode {self.episodes_collected + 1}: æ­¥éª¤ {ep_t}, ç´¯ç§¯å¥–åŠ±: {episode_reward:.4f}")
        
        # Episodeç»“æŸ
        episode_duration = time.time() - episode_start_time
        self.episodes_collected += 1
        
        # è®°å½•episodeä¿¡æ¯
        episode_info = {
            'episode_id': self.episodes_collected,
            'episode_reward': episode_reward,
            'episode_steps': episode_steps,
            'episode_duration': episode_duration,
            'termination_reason': 'done' if done else 'max_steps',
            'high_level_buffer_size': len(self.high_level_buffer),
            'low_level_buffer_size': len(self.low_level_buffer)
        }
        
        # è®°å½•åˆ°è®­ç»ƒä¿¡æ¯
        self.training_info['episode_rewards'].append(episode_reward)
        
        # è®°å½•åˆ°TensorBoard
        if hasattr(self, 'writer'):
            self.writer.add_scalar('Episodes/Reward', episode_reward, self.episodes_collected)
            self.writer.add_scalar('Episodes/Steps', episode_steps, self.episodes_collected)
            self.writer.add_scalar('Episodes/Duration', episode_duration, self.episodes_collected)
            self.writer.add_scalar('Episodes/BufferSizes/HighLevel', len(self.high_level_buffer), self.episodes_collected)
            self.writer.add_scalar('Episodes/BufferSizes/LowLevel', len(self.low_level_buffer), self.episodes_collected)
        
        main_logger.info(f"Episode {self.episodes_collected} å®Œæˆ: "
                        f"å¥–åŠ±={episode_reward:.4f}, æ­¥æ•°={episode_steps}, "
                        f"è€—æ—¶={episode_duration:.2f}s, åŸå› ={episode_info['termination_reason']}")
        
        return episode_info
    
    def collect_rollout_step(self, envs, env_states, env_observations, env_dones=None):
        """
        æ”¶é›†å•æ­¥rolloutæ•°æ®ï¼ˆrollout-basedè®­ç»ƒæ¨¡å¼ï¼‰
        
        å‚æ•°:
            envs: å¹¶è¡Œç¯å¢ƒï¼ˆSubprocVecEnvå¯¹è±¡ï¼‰
            env_states: å„ç¯å¢ƒçš„å½“å‰çŠ¶æ€ [num_envs, state_dim]
            env_observations: å„ç¯å¢ƒçš„å½“å‰è§‚æµ‹ [num_envs, n_agents, obs_dim]
            env_dones: å„ç¯å¢ƒçš„ç»ˆæ­¢çŠ¶æ€ [num_envs] (å¯é€‰)
            
        è¿”å›:
            rollout_data: å•æ­¥rolloutæ•°æ®
        """
        if not self.rollout_based_training:
            raise ValueError("collect_rollout_stepåªèƒ½åœ¨rollout_based_trainingæ¨¡å¼ä¸‹ä½¿ç”¨")
        
        # ä¿®å¤ï¼šä½¿ç”¨ SubprocVecEnv çš„ num_envs å±æ€§è€Œä¸æ˜¯ len()
        num_envs = envs.num_envs
        actions_all = []
        infos_all = []
        
        # ä¸ºæ¯ä¸ªç¯å¢ƒæ‰§è¡Œstep
        for env_id in range(num_envs):
            actions, info = self.step(
                env_states[env_id], 
                env_observations[env_id], 
                self.steps_collected,  # ä½¿ç”¨å…¨å±€æ­¥æ•°è®¡æ•°å™¨
                deterministic=False, 
                env_id=env_id
            )
            actions_all.append(actions)
            infos_all.append(info)
        
        return {
            'actions': actions_all,
            'infos': infos_all,
            'step_count': self.steps_collected
        }
    
    def should_rollout_update(self):
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿›è¡Œrolloutæ›´æ–° - ä¿®å¤ç‰ˆæœ¬
        
        è¿”å›:
            bool: æ˜¯å¦åº”è¯¥æ›´æ–°
        """
        if not self.rollout_based_training:
            return False
        
        # ã€ä¿®å¤C1ã€‘ä½¿ç”¨æ­£ç¡®çš„ç›®æ ‡æ­¥æ•°è®¡ç®—ï¼šrollout_length Ã— num_parallel_envs
        target_steps = self.rollout_length * self.num_parallel_envs
        
        # ã€ä¿®å¤C2ã€‘æ¯100æ­¥è®°å½•ä¸€æ¬¡è¿›åº¦ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
        if self.steps_collected % 100 == 0 or not hasattr(self, '_last_debug_step'):
            progress_percent = (self.steps_collected / target_steps) * 100
            main_logger.info(f"[ROLLOUT_UPDATE_CHECK] å½“å‰è¿›åº¦: {self.steps_collected}/{target_steps} "
                           f"({progress_percent:.1f}%) - rollout_length={self.rollout_length}, "
                           f"num_parallel_envs={self.num_parallel_envs}")
            self._last_debug_step = self.steps_collected
        
        should_update = self.steps_collected >= target_steps
        
        # ã€ä¿®å¤C3ã€‘è®°å½•æ›´æ–°å†³ç­–çš„è¯¦ç»†ä¿¡æ¯
        if should_update:
            main_logger.info(f"ğŸ”„ æ»¡è¶³rolloutæ›´æ–°æ¡ä»¶: æ”¶é›†æ­¥æ•°={self.steps_collected}, "
                           f"ç›®æ ‡æ­¥æ•°={target_steps}, è¶…å‡º={self.steps_collected - target_steps}")
        
        # ã€æ–°å¢C4ã€‘å¦‚æœæ¥è¿‘ç›®æ ‡ä½†è¿˜æ²¡è¾¾åˆ°ï¼Œè®°å½•è¯¦ç»†çŠ¶æ€
        elif self.steps_collected >= target_steps * 0.9:  # 90%ä»¥ä¸Šæ—¶è®°å½•
            remaining = target_steps - self.steps_collected
            main_logger.info(f"â³ æ¥è¿‘æ›´æ–°æ¡ä»¶: è¿˜éœ€{remaining}æ­¥ "
                           f"({self.steps_collected}/{target_steps})")
        
        return should_update
    
    def rollout_update(self):
        """
        æ‰§è¡Œrollout-basedæ‰¹é‡æ›´æ–°ï¼ˆä¸¥æ ¼æŒ‰ç…§è®ºæ–‡Algorithm 1å®ç°ï¼‰
        
        è®­ç»ƒæµç¨‹ï¼š
        1. å¹¶è¡Œæ”¶é›†: 32ç¯å¢ƒ Ã— 128æ­¥ = 4096æ ·æœ¬ â†’ B_h, B_l, D
        2. è®­ç»ƒé˜¶æ®µ: PPOç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒ15è½®ï¼Œåˆ¤åˆ«å™¨ä»Dé‡‡æ ·è®­ç»ƒ
        3. æ¸…ç©ºç¼“å†²åŒº: B_hå’ŒB_læ¸…ç©ºï¼ŒDä¿ç•™
        4. é‡å¤å¾ªç¯
        
        è¿”å›:
            update_info: æ›´æ–°ä¿¡æ¯å­—å…¸
        """
        if not self.rollout_based_training:
            raise ValueError("rollout_updateåªèƒ½åœ¨rollout_based_trainingæ¨¡å¼ä¸‹ä½¿ç”¨")
        
        update_start_time = time.time()
        steps_for_update = self.steps_collected
        target_samples = self.rollout_length * self.num_parallel_envs
        
        main_logger.info(f"ğŸ”„ å¼€å§‹Rolloutæ›´æ–° #{self.rollout_count + 1}")
        main_logger.info(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: æ”¶é›†æ­¥æ•°={steps_for_update}, ç›®æ ‡æ ·æœ¬={target_samples}, "
                        f"å¹¶è¡Œç¯å¢ƒ={self.num_parallel_envs}")
        
        # ã€å…³é”®ä¿®å¤ã€‘åœ¨è®­ç»ƒå‰å¼ºåˆ¶æ”¶é›†æ‰€æœ‰pendingçš„é«˜å±‚ç»éªŒ
        #main_logger.info("ğŸ” Rolloutç»“æŸï¼Œå¼ºåˆ¶æ”¶é›†æ‰€æœ‰pendingé«˜å±‚ç»éªŒ...")
        #pending_collections = self._force_collect_all_pending_high_level_experiences()
        #main_logger.info(f"âœ… å¼ºåˆ¶æ”¶é›†å®Œæˆï¼Œæ–°å¢ {pending_collections} ä¸ªé«˜å±‚ç»éªŒ")
        
        # è®°å½•æ›´æ–°å‰çš„ç¼“å†²åŒºçŠ¶æ€
        high_level_size_before = len(self.high_level_buffer)
        low_level_size_before = len(self.low_level_buffer)
        state_skill_size_before = len(self.state_skill_dataset)
        
        # ã€è°ƒè¯•æ—¥å¿—ã€‘è¯¦ç»†è®°å½•ç¼“å†²åŒºçŠ¶æ€å’Œé«˜å±‚ç»éªŒæ”¶é›†æƒ…å†µ
        main_logger.warning(f"[ROLLOUT_BUFFER_DEBUG] æ›´æ–°å‰ç¼“å†²åŒºè¯¦ç»†çŠ¶æ€:")
        main_logger.warning(f"   - B_h (é«˜å±‚): {high_level_size_before} (ç›®æ ‡: {self.config.high_level_batch_size})")
        main_logger.warning(f"   - B_l (ä½å±‚): {low_level_size_before} (ç›®æ ‡: {self.config.batch_size})")
        main_logger.warning(f"   - D (åˆ¤åˆ«å™¨): {state_skill_size_before}")
        main_logger.warning(f"   - é«˜å±‚æ ·æœ¬ç»Ÿè®¡: æ€»è®¡={self.high_level_samples_total}, "
                           f"ç¯å¢ƒè´¡çŒ®={self.high_level_samples_by_env}, "
                           f"åŸå› ç»Ÿè®¡={self.high_level_samples_by_reason}")
        
        # ã€è°ƒè¯•æ—¥å¿—ã€‘æ£€æŸ¥å„ç¯å¢ƒçš„æŠ€èƒ½è®¡æ—¶å™¨çŠ¶æ€
        env_timer_status = {}
        for env_id in range(self.num_parallel_envs):
            timer = self.env_timers.get(env_id, 0)
            reward_sum = self.env_reward_sums.get(env_id, 0.0)
            env_timer_status[env_id] = {'timer': timer, 'reward_sum': reward_sum}
        
        main_logger.warning(f"[ROLLOUT_TIMER_DEBUG] å„ç¯å¢ƒæŠ€èƒ½è®¡æ—¶å™¨çŠ¶æ€: {env_timer_status}")
        
        # éªŒè¯æ•°æ®æ”¶é›†çš„å®Œæ•´æ€§
        if steps_for_update != self.rollout_length:
            main_logger.warning(f"âš ï¸ æ”¶é›†æ­¥æ•°({steps_for_update})ä¸ç›®æ ‡({self.rollout_length})ä¸åŒ¹é…")
        
        # æ‰§è¡Œ15è½®PPOè®­ç»ƒï¼ˆä¸¥æ ¼æŒ‰ç…§è®ºæ–‡è®¾ç½®ï¼‰
        main_logger.info(f"ğŸ¯ å¼€å§‹{self.ppo_epochs}è½®PPOè®­ç»ƒï¼ˆä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼‰")
        
        coordinator_losses = []
        discoverer_losses = []
        discriminator_losses = []
        
        for epoch in range(self.ppo_epochs):
            epoch_start_time = time.time()
            main_logger.debug(f"   è½®æ¬¡ {epoch + 1}/{self.ppo_epochs}")
            
            # 1. æ›´æ–°é«˜å±‚ç­–ç•¥ï¼ˆPPOï¼Œä½¿ç”¨B_hå…¨éƒ¨æ•°æ®ï¼‰
            coordinator_info = self._rollout_update_coordinator()
            coordinator_losses.append(coordinator_info)
            
            # 2. æ›´æ–°ä½å±‚ç­–ç•¥ï¼ˆPPOï¼Œä½¿ç”¨B_lå…¨éƒ¨æ•°æ®ï¼‰
            discoverer_info = self._rollout_update_discoverer()
            discoverer_losses.append(discoverer_info)
            
            # 3. æ›´æ–°åˆ¤åˆ«å™¨ï¼ˆç›‘ç£å­¦ä¹ ï¼Œä»Dé‡‡æ ·ï¼‰
            discriminator_loss = self.update_discriminators()
            discriminator_losses.append(discriminator_loss)
            
            epoch_time = time.time() - epoch_start_time
            
            if epoch % 5 == 0 or epoch == self.ppo_epochs - 1:
                main_logger.debug(f"   è½®æ¬¡ {epoch + 1} å®Œæˆï¼Œè€—æ—¶: {epoch_time:.3f}s")
        
        # ã€å…³é”®ã€‘ä¸¥æ ¼æŒ‰ç…§è®ºæ–‡è¦æ±‚æ¸…ç©ºPPOç¼“å†²åŒº
        main_logger.info("ğŸ§¹ æ¸…ç©ºPPOç¼“å†²åŒºï¼ˆä¿æŒon-policyç‰¹æ€§ï¼‰")
        
        # è®°å½•æ¸…ç©ºå‰çš„ç»Ÿè®¡ä¿¡æ¯
        total_high_level_samples_used = high_level_size_before
        total_low_level_samples_used = low_level_size_before
        
        # æ¸…ç©ºB_hå’ŒB_lï¼ˆPPOè¦æ±‚ï¼‰
        self.high_level_buffer.clear()
        self.low_level_buffer.clear()
        
        # Dï¼ˆåˆ¤åˆ«å™¨æ•°æ®é›†ï¼‰ä¿ç•™ï¼ˆç›‘ç£å­¦ä¹ å¯é‡å¤ä½¿ç”¨ï¼‰
        # self.state_skill_dataset ä¸æ¸…ç©º
        
        # éªŒè¯æ¸…ç©ºæ˜¯å¦æˆåŠŸ
        high_level_size_after = len(self.high_level_buffer)
        low_level_size_after = len(self.low_level_buffer)
        state_skill_size_after = len(self.state_skill_dataset)
        
        if high_level_size_after != 0 or low_level_size_after != 0:
            main_logger.error(f"âŒ ç¼“å†²åŒºæ¸…ç©ºå¤±è´¥ï¼B_h={high_level_size_after}, B_l={low_level_size_after}")
        else:
            main_logger.info(f"âœ… PPOç¼“å†²åŒºæ¸…ç©ºæˆåŠŸ")
        
        main_logger.info(f"ğŸ“¦ æ›´æ–°åç¼“å†²åŒºçŠ¶æ€:")
        main_logger.info(f"   - B_h (é«˜å±‚): {high_level_size_before} â†’ {high_level_size_after}")
        main_logger.info(f"   - B_l (ä½å±‚): {low_level_size_before} â†’ {low_level_size_after}")
        main_logger.info(f"   - D (åˆ¤åˆ«å™¨): {state_skill_size_before} â†’ {state_skill_size_after} (ä¿ç•™)")
        
        # ã€ä¿®å¤D1ã€‘é‡ç½®rolloutçŠ¶æ€ - ç¡®ä¿å®Œæ•´é‡ç½®
        steps_before_reset = self.steps_collected
        self.steps_collected = 0
        self.rollout_count += 1
        self.total_steps_collected += steps_for_update
        update_duration = time.time() - update_start_time
        
        # ã€ä¿®å¤D2ã€‘éªŒè¯æ­¥æ•°é‡ç½®æ˜¯å¦æˆåŠŸ
        if self.steps_collected != 0:
            main_logger.error(f"âŒ æ­¥æ•°é‡ç½®å¤±è´¥ï¼steps_collected={self.steps_collected}")
        else:
            main_logger.debug(f"âœ… æ­¥æ•°é‡ç½®æˆåŠŸ: {steps_before_reset} â†’ {self.steps_collected}")
        
        # ã€ä¿®å¤D3ã€‘é‡ç½®ç¯å¢ƒç›¸å…³çš„è®¡æ•°å™¨å’ŒçŠ¶æ€
        # é‡ç½®æ‰€æœ‰ç¯å¢ƒçš„æŠ€èƒ½è®¡æ—¶å™¨å’Œå¥–åŠ±ç´¯ç§¯
        for env_id in range(self.num_parallel_envs):
            if env_id in self.env_timers:
                self.env_timers[env_id] = 0
            if env_id in self.env_reward_sums:
                self.env_reward_sums[env_id] = 0.0
        
        main_logger.debug(f"ğŸ”„ æ‰€æœ‰ç¯å¢ƒçŠ¶æ€å·²é‡ç½®: timerså’Œreward_sumså·²æ¸…é›¶")
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_coordinator_info = self._average_update_info(coordinator_losses)
        avg_discoverer_info = self._average_update_info(discoverer_losses)
        avg_discriminator_loss = np.mean(discriminator_losses) if discriminator_losses else 0.0
        
        # è®¡ç®—æ ·æœ¬ä½¿ç”¨æ•ˆç‡
        samples_per_second = target_samples / update_duration if update_duration > 0 else 0
        
        main_logger.info(f"ğŸ‰ Rolloutæ›´æ–° #{self.rollout_count} å®Œæˆ")
        main_logger.info(f"â±ï¸ è€—æ—¶: {update_duration:.2f}s, æ•ˆç‡: {samples_per_second:.0f} æ ·æœ¬/ç§’")
        main_logger.info(f"ğŸ“ˆ ç´¯è®¡: rollouts={self.rollout_count}, æ€»æ­¥æ•°={self.total_steps_collected:,}")
        
        # æ„å»ºè¯¦ç»†çš„æ›´æ–°ä¿¡æ¯
        update_info = {
            'update_type': 'rollout_batch',
            'rollout_count': self.rollout_count,
            'steps_used': steps_for_update,
            'target_samples': target_samples,
            'total_steps': self.total_steps_collected,
            'ppo_epochs': self.ppo_epochs,
            'num_parallel_envs': self.num_parallel_envs,
            'update_duration': update_duration,
            'samples_per_second': samples_per_second,
            'buffer_changes': {
                'high_level': (high_level_size_before, high_level_size_after),
                'low_level': (low_level_size_before, low_level_size_after),
                'state_skill': (state_skill_size_before, state_skill_size_after)
            },
            'samples_used': {
                'high_level': total_high_level_samples_used,
                'low_level': total_low_level_samples_used,
                'discriminator': state_skill_size_before
            },
            'coordinator': avg_coordinator_info,
            'discoverer': avg_discoverer_info,
            'discriminator': {'discriminator_loss': avg_discriminator_loss},
            'buffer_cleared': high_level_size_after == 0 and low_level_size_after == 0,
            'algorithm_compliance': {
                'ppo_epochs_executed': self.ppo_epochs,
                'buffers_cleared': high_level_size_after == 0 and low_level_size_after == 0,
                'discriminator_preserved': state_skill_size_after > 0
            }
        }
        
        # è®°å½•åˆ°TensorBoard
        if hasattr(self, 'writer'):
            # åŸºæœ¬æŒ‡æ ‡
            self.writer.add_scalar('Rollout/UpdateDuration', update_duration, self.rollout_count)
            self.writer.add_scalar('Rollout/StepsUsed', steps_for_update, self.rollout_count)
            self.writer.add_scalar('Rollout/TotalSteps', self.total_steps_collected, self.rollout_count)
            self.writer.add_scalar('Rollout/SamplesPerSecond', samples_per_second, self.rollout_count)
            
            # ç¼“å†²åŒºçŠ¶æ€
            self.writer.add_scalar('Rollout/BufferSizeBefore/HighLevel', high_level_size_before, self.rollout_count)
            self.writer.add_scalar('Rollout/BufferSizeBefore/LowLevel', low_level_size_before, self.rollout_count)
            self.writer.add_scalar('Rollout/BufferSizeAfter/HighLevel', high_level_size_after, self.rollout_count)
            self.writer.add_scalar('Rollout/BufferSizeAfter/LowLevel', low_level_size_after, self.rollout_count)
            
            # ç®—æ³•åˆè§„æ€§
            self.writer.add_scalar('Rollout/Algorithm/BuffersCleared', 
                                  1.0 if update_info['buffer_cleared'] else 0.0, self.rollout_count)
            self.writer.add_scalar('Rollout/Algorithm/PPOEpochs', self.ppo_epochs, self.rollout_count)
            
            # æŸå¤±è®°å½•
            if avg_coordinator_info:
                self.writer.add_scalar('Rollout/AvgCoordinatorLoss', 
                                      avg_coordinator_info.get('coordinator_loss', 0), self.rollout_count)
                self.writer.add_scalar('Rollout/AvgCoordinatorPolicyLoss', 
                                      avg_coordinator_info.get('coordinator_policy_loss', 0), self.rollout_count)
                self.writer.add_scalar('Rollout/AvgCoordinatorValueLoss', 
                                      avg_coordinator_info.get('coordinator_value_loss', 0), self.rollout_count)
            
            if avg_discoverer_info:
                self.writer.add_scalar('Rollout/AvgDiscovererLoss', 
                                      avg_discoverer_info.get('discoverer_loss', 0), self.rollout_count)
                self.writer.add_scalar('Rollout/AvgDiscovererPolicyLoss', 
                                      avg_discoverer_info.get('discoverer_policy_loss', 0), self.rollout_count)
                self.writer.add_scalar('Rollout/AvgDiscovererValueLoss', 
                                      avg_discoverer_info.get('discoverer_value_loss', 0), self.rollout_count)
            
            self.writer.add_scalar('Rollout/AvgDiscriminatorLoss', avg_discriminator_loss, self.rollout_count)
        
        return update_info
    
    def _force_collect_all_pending_high_level_experiences(self):
        """
        å¼ºåˆ¶æ”¶é›†æ‰€æœ‰ç¯å¢ƒä¸­pendingçš„é«˜å±‚ç»éªŒ
        è§£å†³rolloutç»“æŸæ—¶éƒ¨åˆ†ç¯å¢ƒæŠ€èƒ½å‘¨æœŸæœªå®Œæˆçš„é—®é¢˜
        
        è¿”å›:
            int: æ–°æ”¶é›†çš„é«˜å±‚ç»éªŒæ•°é‡
        """
        pending_collections = 0
        
        for env_id in range(self.num_parallel_envs):
            timer = self.env_timers.get(env_id, 0)
            reward_sum = self.env_reward_sums.get(env_id, 0.0)
            
            # å¦‚æœè¯¥ç¯å¢ƒæœ‰æœªå®Œæˆçš„æŠ€èƒ½å‘¨æœŸï¼ˆtimer > 0ä¸”è¿˜æ²¡åˆ°k-1ï¼‰ï¼Œå¼ºåˆ¶æ”¶é›†
            if timer > 0:
                main_logger.info(f"ğŸ”§ å¼ºåˆ¶æ”¶é›†ç¯å¢ƒ{env_id}çš„pendingé«˜å±‚ç»éªŒ: "
                               f"timer={timer}/{self.config.k-1}, ç´¯ç§¯å¥–åŠ±={reward_sum:.4f}")
                
                # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„é«˜å±‚ç»éªŒï¼ˆä½¿ç”¨å½“å‰ç´¯ç§¯çš„å¥–åŠ±ï¼‰
                # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬éœ€è¦æ¨¡æ‹Ÿstore_transitionä¸­çš„é«˜å±‚ç»éªŒæ”¶é›†é€»è¾‘
                if reward_sum != 0.0 or timer >= self.config.k // 2:  # åªæ”¶é›†æœ‰æ„ä¹‰çš„ç»éªŒ
                    # è·å–ç¯å¢ƒçš„å½“å‰æŠ€èƒ½çŠ¶æ€
                    team_skill = self.env_team_skills.get(env_id, 0)
                    agent_skills = self.env_agent_skills.get(env_id, [0] * self.config.n_agents)
                    
                    # åˆ›å»ºè™šæ‹Ÿçš„çŠ¶æ€å’Œè§‚æµ‹ï¼ˆä½¿ç”¨é›¶å‘é‡ä½œä¸ºå ä½ç¬¦ï¼‰
                    state_tensor = torch.zeros(self.config.state_dim, device=self.device)
                    team_skill_tensor = torch.tensor(team_skill, device=self.device)
                    observations_tensor = torch.zeros(self.config.n_agents, self.config.obs_dim, device=self.device)
                    agent_skills_tensor = torch.tensor(agent_skills[:self.config.n_agents], device=self.device)
                    
                    # åˆ›å»ºé«˜å±‚ç»éªŒå…ƒç»„
                    high_level_experience = (
                        state_tensor,                  # å…¨å±€çŠ¶æ€s
                        team_skill_tensor,             # å›¢é˜ŸæŠ€èƒ½Z
                        observations_tensor,           # æ‰€æœ‰æ™ºèƒ½ä½“è§‚æµ‹o
                        agent_skills_tensor,           # æ‰€æœ‰ä¸ªä½“æŠ€èƒ½z
                        torch.tensor(reward_sum, device=self.device) # ç´¯ç§¯å¥–åŠ±
                    )
                    
                    # å­˜å‚¨é«˜å±‚ç»éªŒ
                    self.high_level_buffer.push(high_level_experience)
                    
                    # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                    self.high_level_samples_total += 1
                    self.high_level_samples_by_env[env_id] = self.high_level_samples_by_env.get(env_id, 0) + 1
                    self.high_level_samples_by_reason['Rolloutç»“æŸå¼ºåˆ¶æ”¶é›†'] = self.high_level_samples_by_reason.get('Rolloutç»“æŸå¼ºåˆ¶æ”¶é›†', 0) + 1
                    
                    pending_collections += 1
                    
                    main_logger.info(f"âœ… ç¯å¢ƒ{env_id}é«˜å±‚ç»éªŒå·²å¼ºåˆ¶æ”¶é›†: "
                                   f"ç´¯ç§¯å¥–åŠ±={reward_sum:.4f}, æ–°ç¼“å†²åŒºå¤§å°={len(self.high_level_buffer)}")
                
                # é‡ç½®è¯¥ç¯å¢ƒçš„çŠ¶æ€
                self.env_reward_sums[env_id] = 0.0
                self.env_timers[env_id] = 0
        
        if pending_collections > 0:
            main_logger.info(f"ğŸ¯ Rolloutç»“æŸå¼ºåˆ¶æ”¶é›†æ€»ç»“: æ–°å¢ {pending_collections} ä¸ªé«˜å±‚ç»éªŒ, "
                           f"é«˜å±‚ç¼“å†²åŒº: {len(self.high_level_buffer)}")
        
        return pending_collections
    
    def _rollout_update_coordinator(self):
        """rolloutæ¨¡å¼ä¸‹çš„é«˜å±‚ç­–ç•¥æ›´æ–°ï¼ˆä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼Œä¸é‡‡æ ·ï¼‰"""
        if len(self.high_level_buffer) == 0:
            return self._get_default_coordinator_info()
        
        # ä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼ˆnum_mini_batch=1çš„å«ä¹‰ï¼‰
        return self._update_coordinator_with_all_buffer()
    
    def _rollout_update_discoverer(self):
        """rolloutæ¨¡å¼ä¸‹çš„ä½å±‚ç­–ç•¥æ›´æ–°ï¼ˆä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼Œä¸é‡‡æ ·ï¼‰"""
        if len(self.low_level_buffer) == 0:
            return self._get_default_discoverer_info()
        
        # ä½¿ç”¨å…¨éƒ¨æ•°æ®è¿›è¡Œè®­ç»ƒ
        return self._update_discoverer_with_all_buffer()
    
    def _update_coordinator_with_all_buffer(self):
        """ä½¿ç”¨ç¼“å†²åŒºä¸­çš„å…¨éƒ¨æ•°æ®æ›´æ–°åè°ƒå™¨"""
        # å¤ç”¨ç°æœ‰çš„update_coordinatoré€»è¾‘
        coordinator_loss, coordinator_policy_loss, coordinator_value_loss, team_skill_entropy, agent_skill_entropy, \
        mean_coord_state_val, mean_coord_agent_val, mean_high_level_reward = self.update_coordinator()
        
        return {
            'coordinator_loss': coordinator_loss,
            'coordinator_policy_loss': coordinator_policy_loss,
            'coordinator_value_loss': coordinator_value_loss,
            'team_skill_entropy': team_skill_entropy,
            'agent_skill_entropy': agent_skill_entropy,
            'mean_coord_state_val': mean_coord_state_val,
            'mean_coord_agent_val': mean_coord_agent_val,
            'mean_high_level_reward': mean_high_level_reward
        }
    
    def _update_discoverer_with_all_buffer(self):
        """ä½¿ç”¨ç¼“å†²åŒºä¸­çš„å…¨éƒ¨æ•°æ®æ›´æ–°å‘ç°å™¨"""
        # å¤ç”¨ç°æœ‰çš„update_discovereré€»è¾‘
        discoverer_loss, discoverer_policy_loss, discoverer_value_loss, action_entropy, \
        avg_intrinsic_reward, avg_env_comp, avg_team_disc_comp, avg_ind_disc_comp, \
        avg_discoverer_val = self.update_discoverer()
        
        return {
            'discoverer_loss': discoverer_loss,
            'discoverer_policy_loss': discoverer_policy_loss,
            'discoverer_value_loss': discoverer_value_loss,
            'action_entropy': action_entropy,
            'avg_intrinsic_reward': avg_intrinsic_reward,
            'avg_env_comp': avg_env_comp,
            'avg_team_disc_comp': avg_team_disc_comp,
            'avg_ind_disc_comp': avg_ind_disc_comp,
            'avg_discoverer_val': avg_discoverer_val
        }
    
    def _average_update_info(self, info_list):
        """è®¡ç®—å¤šæ¬¡æ›´æ–°ä¿¡æ¯çš„å¹³å‡å€¼"""
        if not info_list:
            return {}
        
        # è¿‡æ»¤æ‰ç©ºçš„info
        valid_infos = [info for info in info_list if info and isinstance(info, dict)]
        if not valid_infos:
            return {}
        
        avg_info = {}
        for key in valid_infos[0].keys():
            if isinstance(valid_infos[0][key], (int, float)):
                avg_info[key] = np.mean([info.get(key, 0) for info in valid_infos])
            else:
                avg_info[key] = valid_infos[0][key]  # ä¿ç•™éæ•°å€¼ç±»å‹çš„ç¬¬ä¸€ä¸ªå€¼
        
        return avg_info
    
    def step_rollout_counter(self):
        """
        å¢åŠ rolloutæ­¥æ•°è®¡æ•°å™¨
        åœ¨æ¯æ¬¡ç¯å¢ƒstepåè°ƒç”¨
        """
        if self.rollout_based_training:
            self.steps_collected += 1
    
    def should_update(self):
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿›è¡Œæ‰¹é‡è®­ç»ƒæ›´æ–°
        
        è¿”å›:
            bool: æ˜¯å¦åº”è¯¥æ›´æ–°
        """
        if not self.episode_based_training:
            return False
        
        # æ¡ä»¶1ï¼šæ”¶é›†äº†è¶³å¤Ÿçš„episodes
        episodes_since_last_update = self.episodes_collected - self.last_update_episode
        enough_episodes = episodes_since_last_update >= self.update_frequency
        
        # æ¡ä»¶2ï¼šè¾¾åˆ°æœ€å°‘episodeè¦æ±‚
        min_episodes_met = self.episodes_collected >= self.min_episodes_for_update
        
        # æ¡ä»¶3ï¼šç¼“å†²åŒºæœ‰è¶³å¤Ÿçš„æ•°æ®
        enough_high_level_data = len(self.high_level_buffer) >= self.min_high_level_samples
        enough_low_level_data = len(self.low_level_buffer) >= self.min_low_level_samples
        
        should_update = enough_episodes and min_episodes_met and enough_high_level_data and enough_low_level_data
        
        if episodes_since_last_update > 0 and episodes_since_last_update % 5 == 0:  # æ¯5ä¸ªepisodeè®°å½•ä¸€æ¬¡
            main_logger.debug(f"æ›´æ–°æ£€æŸ¥: episodes_since_last={episodes_since_last_update}, "
                             f"enough_episodes={enough_episodes}, min_episodes_met={min_episodes_met}, "
                             f"enough_high_level={enough_high_level_data}({len(self.high_level_buffer)}/{self.min_high_level_samples}), "
                             f"enough_low_level={enough_low_level_data}({len(self.low_level_buffer)}/{self.min_low_level_samples}), "
                             f"should_update={should_update}")
        
        return should_update
    
    def batch_update(self):
        """
        æ‰¹é‡æ›´æ–°æ‰€æœ‰ç½‘ç»œï¼ˆä¸¥æ ¼æŒ‰ç…§è®ºæ–‡Algorithm 1çš„episode-basedæ¨¡å¼ï¼‰
        
        è¿”å›:
            update_info: æ›´æ–°ä¿¡æ¯å­—å…¸
        """
        if not self.episode_based_training:
            raise ValueError("batch_updateåªèƒ½åœ¨episode_based_trainingæ¨¡å¼ä¸‹ä½¿ç”¨")
        
        update_start_time = time.time()
        episodes_for_update = self.episodes_collected - self.last_update_episode
        
        main_logger.info(f"å¼€å§‹æ‰¹é‡æ›´æ–° - Episode {self.episodes_collected}, "
                        f"ä½¿ç”¨è¿‡å» {episodes_for_update} ä¸ªepisodesçš„æ•°æ®")
        
        # è®°å½•æ›´æ–°å‰çš„ç¼“å†²åŒºçŠ¶æ€
        high_level_size_before = len(self.high_level_buffer)
        low_level_size_before = len(self.low_level_buffer)
        state_skill_size_before = len(self.state_skill_dataset)
        
        main_logger.info(f"æ›´æ–°å‰ç¼“å†²åŒºçŠ¶æ€ - é«˜å±‚: {high_level_size_before}, "
                        f"ä½å±‚: {low_level_size_before}, åˆ¤åˆ«å™¨: {state_skill_size_before}")
        
        # 1. æ›´æ–°é«˜å±‚ç­–ç•¥ï¼ˆPPO + æ¸…ç©ºç¼“å†²åŒºï¼‰
        coordinator_info = self.update_coordinator_batch()
        
        # 2. æ›´æ–°ä½å±‚ç­–ç•¥ï¼ˆPPO + æ¸…ç©ºç¼“å†²åŒºï¼‰  
        discoverer_info = self.update_discoverer_batch()
        
        # 3. æ›´æ–°åˆ¤åˆ«å™¨ï¼ˆç›‘ç£å­¦ä¹ ï¼Œä¿ç•™éƒ¨åˆ†æ•°æ®ï¼‰
        discriminator_info = self.update_discriminators_batch()
        
        # è®°å½•æ›´æ–°åçš„ç¼“å†²åŒºçŠ¶æ€
        high_level_size_after = len(self.high_level_buffer)
        low_level_size_after = len(self.low_level_buffer) 
        state_skill_size_after = len(self.state_skill_dataset)
        
        # æ›´æ–°çŠ¶æ€
        self.last_update_episode = self.episodes_collected
        update_duration = time.time() - update_start_time
        
        main_logger.info(f"æ‰¹é‡æ›´æ–°å®Œæˆ - è€—æ—¶: {update_duration:.2f}s, "
                        f"ç¼“å†²åŒºå˜åŒ–: é«˜å±‚({high_level_size_before}â†’{high_level_size_after}), "
                        f"ä½å±‚({low_level_size_before}â†’{low_level_size_after}), "
                        f"åˆ¤åˆ«å™¨({state_skill_size_before}â†’{state_skill_size_after})")
        
        # æ„å»ºæ›´æ–°ä¿¡æ¯
        update_info = {
            'update_type': 'episode_batch',
            'episodes_used': episodes_for_update,
            'total_episodes': self.episodes_collected,
            'update_duration': update_duration,
            'buffer_changes': {
                'high_level': (high_level_size_before, high_level_size_after),
                'low_level': (low_level_size_before, low_level_size_after),
                'state_skill': (state_skill_size_before, state_skill_size_after)
            },
            'coordinator': coordinator_info,
            'discoverer': discoverer_info,
            'discriminator': discriminator_info
        }
        
        # è®°å½•åˆ°TensorBoard
        if hasattr(self, 'writer'):
            self.writer.add_scalar('Updates/Duration', update_duration, self.episodes_collected)
            self.writer.add_scalar('Updates/EpisodesUsed', episodes_for_update, self.episodes_collected)
            self.writer.add_scalar('Updates/BufferSizeBefore/HighLevel', high_level_size_before, self.episodes_collected)
            self.writer.add_scalar('Updates/BufferSizeBefore/LowLevel', low_level_size_before, self.episodes_collected)
            self.writer.add_scalar('Updates/BufferSizeAfter/HighLevel', high_level_size_after, self.episodes_collected)
            self.writer.add_scalar('Updates/BufferSizeAfter/LowLevel', low_level_size_after, self.episodes_collected)
        
        return update_info
    
    def update_coordinator_batch(self):
        """æ‰¹é‡æ›´æ–°é«˜å±‚ç­–ç•¥ï¼ˆä¸¥æ ¼PPOï¼Œepisode-basedï¼‰"""
        if len(self.high_level_buffer) < self.min_high_level_samples:
            main_logger.warning(f"é«˜å±‚ç¼“å†²åŒºæ•°æ®ä¸è¶³ï¼Œéœ€è¦{self.min_high_level_samples}ä¸ªæ ·æœ¬ï¼Œ"
                               f"ä½†åªæœ‰{len(self.high_level_buffer)}ä¸ªã€‚è·³è¿‡æ›´æ–°ã€‚")
            return self._get_default_coordinator_info()
        
        # ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ•°æ®è¿›è¡Œè®­ç»ƒï¼ˆè€Œä¸æ˜¯é‡‡æ ·ï¼‰
        all_data = list(self.high_level_buffer.buffer)
        actual_batch_size = len(all_data)
        
        main_logger.info(f"é«˜å±‚ç­–ç•¥æ‰¹é‡æ›´æ–° - ä½¿ç”¨å…¨éƒ¨ {actual_batch_size} ä¸ªæ ·æœ¬")
        
        # æ‰§è¡ŒPPOæ›´æ–°ï¼ˆä½¿ç”¨ç°æœ‰çš„update_coordinatoré€»è¾‘ï¼Œä½†ä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼‰
        update_info = self._update_coordinator_with_all_data(all_data)
        
        # ã€å…³é”®ã€‘æ¸…ç©ºç¼“å†²åŒºï¼ˆPPOè¦æ±‚ï¼‰
        buffer_size_before = len(self.high_level_buffer)
        self.high_level_buffer.clear()
        self.high_level_buffer_with_logprobs = []
        
        main_logger.info(f"é«˜å±‚ç­–ç•¥æ›´æ–°å®Œæˆï¼Œç¼“å†²åŒºå·²æ¸…ç©º: {buffer_size_before}â†’0 (ç¬¦åˆPPO on-policyè¦æ±‚)")
        
        update_info['samples_used'] = actual_batch_size
        update_info['buffer_cleared'] = True
        
        return update_info
    
    def update_discoverer_batch(self):
        """æ‰¹é‡æ›´æ–°ä½å±‚ç­–ç•¥ï¼ˆä¸¥æ ¼PPOï¼Œepisode-basedï¼‰"""
        if len(self.low_level_buffer) < self.min_low_level_samples:
            main_logger.warning(f"ä½å±‚ç¼“å†²åŒºæ•°æ®ä¸è¶³ï¼Œéœ€è¦{self.min_low_level_samples}ä¸ªæ ·æœ¬ï¼Œ"
                               f"ä½†åªæœ‰{len(self.low_level_buffer)}ä¸ªã€‚è·³è¿‡æ›´æ–°ã€‚")
            return self._get_default_discoverer_info()
        
        # ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ•°æ®è¿›è¡Œè®­ç»ƒ
        actual_batch_size = len(self.low_level_buffer)
        main_logger.info(f"ä½å±‚ç­–ç•¥æ‰¹é‡æ›´æ–° - ä½¿ç”¨å…¨éƒ¨ {actual_batch_size} ä¸ªæ ·æœ¬")
        
        # æ‰§è¡ŒPPOæ›´æ–°ï¼ˆä½¿ç”¨ç°æœ‰çš„update_discovereré€»è¾‘ï¼‰
        update_info = self._update_discoverer_with_all_data()
        
        # ã€å…³é”®ã€‘æ¸…ç©ºç¼“å†²åŒºï¼ˆPPOè¦æ±‚ï¼‰- è¿™ä¸ªå·²ç»åœ¨åŸæœ‰çš„update_discovererä¸­å®ç°äº†
        
        update_info['samples_used'] = actual_batch_size
        update_info['buffer_cleared'] = True
        
        return update_info
    
    def update_discriminators_batch(self):
        """æ‰¹é‡æ›´æ–°åˆ¤åˆ«å™¨ï¼ˆç›‘ç£å­¦ä¹ ï¼Œå¯ä»¥ä¿ç•™éƒ¨åˆ†æ•°æ®ï¼‰"""
        if len(self.state_skill_dataset) < self.config.batch_size:
            main_logger.warning(f"åˆ¤åˆ«å™¨æ•°æ®é›†ä¸è¶³ï¼Œéœ€è¦{self.config.batch_size}ä¸ªæ ·æœ¬ï¼Œ"
                               f"ä½†åªæœ‰{len(self.state_skill_dataset)}ä¸ªã€‚è·³è¿‡æ›´æ–°ã€‚")
            return {'discriminator_loss': 0.0, 'samples_used': 0}
        
        # åˆ¤åˆ«å™¨ä½¿ç”¨ç›‘ç£å­¦ä¹ ï¼Œå¯ä»¥å¤šæ¬¡ä½¿ç”¨æ•°æ®ï¼Œå› æ­¤ä¸éœ€è¦æ¸…ç©º
        discriminator_loss = self.update_discriminators()
        
        return {
            'discriminator_loss': discriminator_loss,
            'samples_used': len(self.state_skill_dataset),
            'note': 'åˆ¤åˆ«å™¨ä½¿ç”¨ç›‘ç£å­¦ä¹ ï¼Œæ•°æ®é›†æœªæ¸…ç©º'
        }
    
    def _update_coordinator_with_all_data(self, all_data):
        """ä½¿ç”¨æ‰€æœ‰é«˜å±‚æ•°æ®æ›´æ–°åè°ƒå™¨ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰"""
        # å¤ç”¨ç°æœ‰çš„update_coordinatoré€»è¾‘ï¼Œä½†ä¼ å…¥æ‰€æœ‰æ•°æ®
        # è¿™é‡Œæš‚æ—¶è¿”å›ç°æœ‰update_coordinatorçš„ç»“æœ
        coordinator_loss, coordinator_policy_loss, coordinator_value_loss, team_skill_entropy, agent_skill_entropy, \
        mean_coord_state_val, mean_coord_agent_val, mean_high_level_reward = self.update_coordinator()
        
        return {
            'coordinator_loss': coordinator_loss,
            'coordinator_policy_loss': coordinator_policy_loss,
            'coordinator_value_loss': coordinator_value_loss,
            'team_skill_entropy': team_skill_entropy,
            'agent_skill_entropy': agent_skill_entropy,
            'mean_coord_state_val': mean_coord_state_val,
            'mean_coord_agent_val': mean_coord_agent_val,
            'mean_high_level_reward': mean_high_level_reward
        }
    
    def _update_discoverer_with_all_data(self):
        """ä½¿ç”¨æ‰€æœ‰ä½å±‚æ•°æ®æ›´æ–°å‘ç°å™¨ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰"""
        # å¤ç”¨ç°æœ‰çš„update_discovereré€»è¾‘
        discoverer_loss, discoverer_policy_loss, discoverer_value_loss, action_entropy, \
        avg_intrinsic_reward, avg_env_comp, avg_team_disc_comp, avg_ind_disc_comp, \
        avg_discoverer_val = self.update_discoverer()
        
        return {
            'discoverer_loss': discoverer_loss,
            'discoverer_policy_loss': discoverer_policy_loss,
            'discoverer_value_loss': discoverer_value_loss,
            'action_entropy': action_entropy,
            'avg_intrinsic_reward': avg_intrinsic_reward,
            'avg_env_comp': avg_env_comp,
            'avg_team_disc_comp': avg_team_disc_comp,
            'avg_ind_disc_comp': avg_ind_disc_comp,
            'avg_discoverer_val': avg_discoverer_val
        }
    
    def _get_default_coordinator_info(self):
        """è·å–é»˜è®¤çš„åè°ƒå™¨æ›´æ–°ä¿¡æ¯ï¼ˆå½“è·³è¿‡æ›´æ–°æ—¶ï¼‰"""
        return {
            'coordinator_loss': 0.0,
            'coordinator_policy_loss': 0.0,
            'coordinator_value_loss': 0.0,
            'team_skill_entropy': 0.0,
            'agent_skill_entropy': 0.0,
            'mean_coord_state_val': 0.0,
            'mean_coord_agent_val': 0.0,
            'mean_high_level_reward': 0.0,
            'samples_used': 0,
            'buffer_cleared': False,
            'skipped': True
        }
    
    def _get_default_discoverer_info(self):
        """è·å–é»˜è®¤çš„å‘ç°å™¨æ›´æ–°ä¿¡æ¯ï¼ˆå½“è·³è¿‡æ›´æ–°æ—¶ï¼‰"""
        return {
            'discoverer_loss': 0.0,
            'discoverer_policy_loss': 0.0,
            'discoverer_value_loss': 0.0,
            'action_entropy': 0.0,
            'avg_intrinsic_reward': 0.0,
            'avg_env_comp': 0.0,
            'avg_team_disc_comp': 0.0,
            'avg_ind_disc_comp': 0.0,
            'avg_discoverer_val': 0.0,
            'samples_used': 0,
            'buffer_cleared': False,
            'skipped': True
        }

    def _collect_high_level_experience(self, env_id, state_tensor, observations_tensor, 
                                     team_skill, agent_skills, reason="æŠ€èƒ½å‘¨æœŸç»“æŸ"):
        """
        ç»Ÿä¸€çš„é«˜å±‚ç»éªŒæ”¶é›†å…¥å£
        
        å‚æ•°:
            env_id: ç¯å¢ƒID
            state_tensor: å…¨å±€çŠ¶æ€å¼ é‡
            observations_tensor: æ‰€æœ‰æ™ºèƒ½ä½“è§‚æµ‹å¼ é‡
            team_skill: å›¢é˜ŸæŠ€èƒ½ç´¢å¼•
            agent_skills: ä¸ªä½“æŠ€èƒ½ç´¢å¼•åˆ—è¡¨
            reason: æ”¶é›†åŸå› ï¼Œç”¨äºæ—¥å¿—è®°å½•
            
        è¿”å›:
            bool: æ˜¯å¦æˆåŠŸæ”¶é›†
        """
        # è·å–å½“å‰ç¯å¢ƒçš„ç´¯ç§¯å¥–åŠ±
        env_accumulated_reward = self.env_reward_sums.get(env_id, 0.0)
        
        # åˆ›å»ºé«˜å±‚ç»éªŒå…ƒç»„
        team_skill_tensor = torch.tensor(team_skill, device=self.device)
        agent_skills_tensor = torch.tensor(agent_skills, device=self.device)
        
        high_level_experience = (
            state_tensor,                                                    # å…¨å±€çŠ¶æ€s
            team_skill_tensor,                                               # å›¢é˜ŸæŠ€èƒ½Z
            observations_tensor,                                             # æ‰€æœ‰æ™ºèƒ½ä½“è§‚æµ‹o
            agent_skills_tensor,                                             # æ‰€æœ‰ä¸ªä½“æŠ€èƒ½z
            torch.tensor(env_accumulated_reward, device=self.device)         # ç´¯ç§¯å¥–åŠ±
        )
        
        # å­˜å‚¨é«˜å±‚ç»éªŒ
        self.high_level_buffer.push(high_level_experience)
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.high_level_samples_total += 1
        self.high_level_samples_by_env[env_id] = self.high_level_samples_by_env.get(env_id, 0) + 1
        self.high_level_samples_by_reason[reason] = self.high_level_samples_by_reason.get(reason, 0) + 1
        
        # æ›´æ–°ç¯å¢ƒæœ€åè´¡çŒ®æ—¶é—´
        self.env_last_contribution[env_id] = self.global_step
        
        # é‡ç½®å¼ºåˆ¶æ”¶é›†æ ‡å¿—
        if env_id in self.force_high_level_collection:
            self.force_high_level_collection[env_id] = False
        
        # è®°å½•æˆåŠŸæ”¶é›†çš„infoçº§åˆ«æ—¥å¿—
        current_buffer_size = len(self.high_level_buffer)
        main_logger.debug(f"âœ… é«˜å±‚ç»éªŒæ”¶é›†æˆåŠŸ: ç¯å¢ƒID={env_id}, step={self.global_step}, "
                        f"ç¼“å†²åŒºå¤§å°: {current_buffer_size}, ç´¯ç§¯å¥–åŠ±: {env_accumulated_reward:.4f}, "
                        f"åŸå› : {reason}")
        
        # é‡ç½®è¯¥ç¯å¢ƒçš„å¥–åŠ±ç´¯ç§¯å’ŒæŠ€èƒ½è®¡æ—¶å™¨
        self.env_reward_sums[env_id] = 0.0
        self.env_timers[env_id] = 0
        
        return True

    def store_high_level_transition(self, state, team_skill, observations, agent_skills, 
                                   accumulated_reward, skill_log_probs=None, worker_id=0):
        """
        å­˜å‚¨é«˜å±‚ç»éªŒï¼ˆä¸“é—¨ç”¨äºå¤šçº¿ç¨‹è®­ç»ƒï¼‰
        
        å‚æ•°:
            state: å…¨å±€çŠ¶æ€ [state_dim]
            team_skill: å›¢é˜ŸæŠ€èƒ½ç´¢å¼•
            observations: æ‰€æœ‰æ™ºèƒ½ä½“çš„è§‚æµ‹ [n_agents, obs_dim]  
            agent_skills: ä¸ªä½“æŠ€èƒ½ç´¢å¼•åˆ—è¡¨ [n_agents]
            accumulated_reward: kæ­¥ç´¯ç§¯å¥–åŠ±
            skill_log_probs: æŠ€èƒ½çš„log probabilitieså­—å…¸
            worker_id: worker IDï¼ˆç”¨ä½œenv_idï¼‰
            
        è¿”å›:
            bool: æ˜¯å¦æˆåŠŸå­˜å‚¨
        """
        try:
            # è½¬æ¢ä¸ºtensoræ ¼å¼
            state_tensor = torch.FloatTensor(state).to(self.device)
            team_skill_tensor = torch.tensor(team_skill, device=self.device)
            observations_tensor = torch.FloatTensor(observations).to(self.device)
            agent_skills_tensor = torch.tensor(agent_skills, device=self.device)
            
            # åˆ›å»ºé«˜å±‚ç»éªŒå…ƒç»„
            high_level_experience = (
                state_tensor,                                                    # å…¨å±€çŠ¶æ€s
                team_skill_tensor,                                               # å›¢é˜ŸæŠ€èƒ½Z
                observations_tensor,                                             # æ‰€æœ‰æ™ºèƒ½ä½“è§‚æµ‹o
                agent_skills_tensor,                                             # æ‰€æœ‰ä¸ªä½“æŠ€èƒ½z
                torch.tensor(accumulated_reward, device=self.device)             # ç´¯ç§¯å¥–åŠ±
            )
            
            # å­˜å‚¨é«˜å±‚ç»éªŒ
            self.high_level_buffer.push(high_level_experience)
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.high_level_samples_total += 1
            self.high_level_samples_by_env[worker_id] = self.high_level_samples_by_env.get(worker_id, 0) + 1
            self.high_level_samples_by_reason['å¤šçº¿ç¨‹å­˜å‚¨'] = self.high_level_samples_by_reason.get('å¤šçº¿ç¨‹å­˜å‚¨', 0) + 1
            
            # å­˜å‚¨å¸¦log probabilitiesçš„ç»éªŒ
            if skill_log_probs is not None:
                self.high_level_buffer_with_logprobs.append({
                    'state': state_tensor.clone(),
                    'team_skill': team_skill,
                    'observations': observations_tensor.clone(),
                    'agent_skills': agent_skills_tensor.clone(),
                    'reward': accumulated_reward,
                    'team_log_prob': skill_log_probs.get('team_log_prob', 0.0),
                    'agent_log_probs': skill_log_probs.get('agent_log_probs', [0.0] * len(agent_skills))
                })
                
                # ä¿æŒç¼“å†²åŒºå¤§å°ä¸è¶…è¿‡config.buffer_size
                if len(self.high_level_buffer_with_logprobs) > self.config.buffer_size:
                    self.high_level_buffer_with_logprobs = self.high_level_buffer_with_logprobs[-self.config.buffer_size:]
            
            main_logger.debug(f"é«˜å±‚ç»éªŒå­˜å‚¨æˆåŠŸ: worker_id={worker_id}, ç´¯ç§¯å¥–åŠ±={accumulated_reward:.4f}, "
                            f"ç¼“å†²åŒºå¤§å°={len(self.high_level_buffer)}")
            
            return True
            
        except Exception as e:
            main_logger.error(f"å­˜å‚¨é«˜å±‚ç»éªŒå¤±è´¥: {e}")
            return False
    
    def store_low_level_transition(self, state, next_state, observations, next_observations,
                                 actions, rewards, dones, team_skill, agent_skills, 
                                 action_logprobs, skill_log_probs=None, worker_id=0):
        """
        å­˜å‚¨ä½å±‚ç»éªŒï¼ˆä¸“é—¨ç”¨äºå¤šçº¿ç¨‹è®­ç»ƒï¼‰
        
        å‚æ•°:
            state: å…¨å±€çŠ¶æ€ [state_dim]
            next_state: ä¸‹ä¸€å…¨å±€çŠ¶æ€ [state_dim] 
            observations: æ‰€æœ‰æ™ºèƒ½ä½“çš„è§‚æµ‹ [n_agents, obs_dim]
            next_observations: æ‰€æœ‰æ™ºèƒ½ä½“çš„ä¸‹ä¸€è§‚æµ‹ [n_agents, obs_dim]
            actions: æ‰€æœ‰æ™ºèƒ½ä½“çš„åŠ¨ä½œ [n_agents, action_dim]
            rewards: ç¯å¢ƒå¥–åŠ±
            dones: æ˜¯å¦ç»“æŸ
            team_skill: å›¢é˜ŸæŠ€èƒ½ç´¢å¼•
            agent_skills: ä¸ªä½“æŠ€èƒ½ç´¢å¼•åˆ—è¡¨ [n_agents]
            action_logprobs: åŠ¨ä½œå¯¹æ•°æ¦‚ç‡ [n_agents]
            skill_log_probs: æŠ€èƒ½çš„log probabilitieså­—å…¸
            worker_id: worker IDï¼ˆç”¨ä½œenv_idï¼‰
            
        è¿”å›:
            bool: æ˜¯å¦æˆåŠŸå­˜å‚¨
        """
        try:
            n_agents = len(agent_skills)
            state_tensor = torch.FloatTensor(state).to(self.device)
            next_state_tensor = torch.FloatTensor(next_state).to(self.device)
            team_skill_tensor = torch.tensor(team_skill, device=self.device)
            
            # ç¡®ä¿rewardsæ˜¯æ•°å€¼ç±»å‹
            current_reward = rewards if isinstance(rewards, (int, float)) else rewards.item()
            
            # è®¡ç®—å›¢é˜ŸæŠ€èƒ½åˆ¤åˆ«å™¨è¾“å‡º
            with torch.no_grad():
                team_disc_logits = self.team_discriminator(next_state_tensor.unsqueeze(0))
                team_disc_log_probs = F.log_softmax(team_disc_logits, dim=-1)
                team_skill_log_prob = team_disc_log_probs[0, team_skill]
            
            # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“å­˜å‚¨ä½å±‚ç»éªŒ
            for i in range(n_agents):
                obs = torch.FloatTensor(observations[i]).to(self.device)
                next_obs = torch.FloatTensor(next_observations[i]).to(self.device)
                action = torch.FloatTensor(actions[i]).to(self.device)
                done = dones if isinstance(dones, bool) else dones[i] if isinstance(dones, list) else dones
                
                # è®¡ç®—ä¸ªä½“æŠ€èƒ½åˆ¤åˆ«å™¨è¾“å‡º
                with torch.no_grad():
                    agent_disc_logits = self.individual_discriminator(
                        next_obs.unsqueeze(0), 
                        team_skill_tensor
                    )
                    agent_disc_log_probs = F.log_softmax(agent_disc_logits, dim=-1)
                    agent_skill_log_prob = agent_disc_log_probs[0, agent_skills[i]]
                    
                # è®¡ç®—ä½å±‚å¥–åŠ±ï¼ˆEq. 4ï¼‰åŠå…¶ç»„æˆéƒ¨åˆ†
                env_reward_component = self.config.lambda_e * current_reward
                team_disc_component = self.config.lambda_D * team_skill_log_prob.item()
                ind_disc_component = self.config.lambda_d * agent_skill_log_prob.item()
                
                intrinsic_reward = env_reward_component + team_disc_component + ind_disc_component
                
                # å­˜å‚¨ä½å±‚ç»éªŒ
                low_level_experience = (
                    state_tensor,                           # å…¨å±€çŠ¶æ€s
                    team_skill_tensor,                      # å›¢é˜ŸæŠ€èƒ½Z
                    obs,                                    # æ™ºèƒ½ä½“è§‚æµ‹o_i
                    torch.tensor(agent_skills[i], device=self.device),  # ä¸ªä½“æŠ€èƒ½z_i
                    action,                                 # åŠ¨ä½œa_i
                    torch.tensor(intrinsic_reward, device=self.device),  # æ€»å†…åœ¨å¥–åŠ±r_i
                    torch.tensor(done, dtype=torch.float, device=self.device),  # æ˜¯å¦ç»“æŸ
                    torch.tensor(action_logprobs[i], device=self.device),  # åŠ¨ä½œå¯¹æ•°æ¦‚ç‡
                    torch.tensor(env_reward_component, device=self.device), # ç¯å¢ƒå¥–åŠ±éƒ¨åˆ†
                    torch.tensor(team_disc_component, device=self.device),  # å›¢é˜Ÿåˆ¤åˆ«å™¨éƒ¨åˆ†
                    torch.tensor(ind_disc_component, device=self.device)   # ä¸ªä½“åˆ¤åˆ«å™¨éƒ¨åˆ†
                )
                self.low_level_buffer.push(low_level_experience)
            
            # å­˜å‚¨æŠ€èƒ½åˆ¤åˆ«å™¨è®­ç»ƒæ•°æ®
            observations_tensor = torch.FloatTensor(next_observations).to(self.device)
            agent_skills_tensor = torch.tensor(agent_skills, device=self.device)
            self.state_skill_dataset.push(
                next_state_tensor,
                team_skill_tensor,
                observations_tensor,
                agent_skills_tensor
            )
            
            main_logger.debug(f"ä½å±‚ç»éªŒå­˜å‚¨æˆåŠŸ: worker_id={worker_id}, n_agents={n_agents}, "
                            f"å¥–åŠ±={current_reward:.4f}, ç¼“å†²åŒºå¤§å°={len(self.low_level_buffer)}")
            
            return True
            
        except Exception as e:
            main_logger.error(f"å­˜å‚¨ä½å±‚ç»éªŒå¤±è´¥: {e}")
            return False

    def store_transition(self, state, next_state, observations, next_observations,
                         actions, rewards, dones, team_skill, agent_skills, action_logprobs, log_probs=None, 
                         skill_timer_for_env=None, env_id=0):
        """
        å­˜å‚¨ç¯å¢ƒäº¤äº’ç»éªŒï¼ˆæ”¯æŒåŒæ­¥è®­ç»ƒï¼‰
        
        å‚æ•°:
            state: å…¨å±€çŠ¶æ€ [state_dim]
            next_state: ä¸‹ä¸€å…¨å±€çŠ¶æ€ [state_dim]
            observations: æ‰€æœ‰æ™ºèƒ½ä½“çš„è§‚æµ‹ [n_agents, obs_dim]
            next_observations: æ‰€æœ‰æ™ºèƒ½ä½“çš„ä¸‹ä¸€è§‚æµ‹ [n_agents, obs_dim]
            actions: æ‰€æœ‰æ™ºèƒ½ä½“çš„åŠ¨ä½œ [n_agents, action_dim]
            rewards: ç¯å¢ƒå¥–åŠ±
            dones: æ˜¯å¦ç»“æŸ [n_agents]
            team_skill: å›¢é˜ŸæŠ€èƒ½ç´¢å¼•
            agent_skills: ä¸ªä½“æŠ€èƒ½ç´¢å¼•åˆ—è¡¨ [n_agents]
            action_logprobs: åŠ¨ä½œå¯¹æ•°æ¦‚ç‡ [n_agents]
            log_probs: æŠ€èƒ½çš„log probabilitieså­—å…¸ï¼ŒåŒ…å«'team_log_prob'å’Œ'agent_log_probs'
            skill_timer_for_env: å½“å‰ç¯å¢ƒçš„æŠ€èƒ½è®¡æ—¶å™¨å€¼ï¼Œç”¨äºå¤šç¯å¢ƒå¹¶è¡Œè®­ç»ƒ
            env_id: ç¯å¢ƒIDï¼Œç”¨äºå¤šç¯å¢ƒå¹¶è¡Œè®­ç»ƒ
            
        è¿”å›:
            bool: æ˜¯å¦æˆåŠŸå­˜å‚¨ï¼ˆåŒæ­¥æ¨¡å¼ä¸‹å¯èƒ½æ‹’ç»å­˜å‚¨ï¼‰
        """
        # ä¿®å¤ï¼šåˆ†ç¦»ä½å±‚å’Œé«˜å±‚ç»éªŒçš„åŒæ­¥æ§åˆ¶
        # Episode-basedæ¨¡å¼ä¸‹å§‹ç»ˆå…è®¸æ•°æ®æ”¶é›†
        # åŒæ­¥æ¨¡å¼ä¸‹æ‰å—collection_enabledæ§åˆ¶
        low_level_collection_allowed = True
        if not self.episode_based_training and hasattr(self, 'sync_mode') and self.sync_mode and not self.collection_enabled:
            low_level_collection_allowed = False
            main_logger.debug(f"åŒæ­¥æ¨¡å¼ï¼šä½å±‚æ•°æ®æ”¶é›†å·²ç¦ç”¨ï¼Œç¯å¢ƒ{env_id}åªæ”¶é›†é«˜å±‚ç»éªŒ")
        
        n_agents = len(agent_skills)
        state_tensor = torch.FloatTensor(state).to(self.device)
        next_state_tensor = torch.FloatTensor(next_state).to(self.device)
        team_skill_tensor = torch.tensor(team_skill, device=self.device)
        
        # ç´¯åŠ å½“å‰æ­¥çš„å›¢é˜Ÿå¥–åŠ±
        # ç¡®ä¿rewardsæ˜¯æ•°å€¼ç±»å‹
        current_reward = rewards if isinstance(rewards, (int, float)) else rewards.item()
        
        # ä½¿ç”¨ç¯å¢ƒIDä¸ºé”®åˆ›å»ºæˆ–æ›´æ–°ç¯å¢ƒç‰¹å®šçš„å¥–åŠ±ç´¯ç§¯
        if env_id not in self.env_reward_sums:
            self.env_reward_sums[env_id] = 0.0
        
        self.env_reward_sums[env_id] += current_reward
        
        # è®°å½•é«˜å±‚å¥–åŠ±ç´¯ç§¯æƒ…å†µï¼ˆå¢åŠ total_stepå’Œskill_timerä¿¡æ¯ï¼‰
        main_logger.debug(f"store_transition: ç¯å¢ƒID={env_id}, step={self.global_step}, skill_timer={skill_timer_for_env}, "
                          f"å½“å‰æ­¥å¥–åŠ±={current_reward:.4f}, æ­¤ç¯å¢ƒç´¯ç§¯é«˜å±‚å¥–åŠ±={self.env_reward_sums[env_id]:.4f}")
        
        # è®¡ç®—å›¢é˜ŸæŠ€èƒ½åˆ¤åˆ«å™¨è¾“å‡º
        with torch.no_grad():
            team_disc_logits = self.team_discriminator(next_state_tensor.unsqueeze(0))
            team_disc_log_probs = F.log_softmax(team_disc_logits, dim=-1)
            team_skill_log_prob = team_disc_log_probs[0, team_skill]
        
        # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“å­˜å‚¨ä½å±‚ç»éªŒï¼ˆä»…åœ¨æ•°æ®æ”¶é›†å…è®¸æ—¶ï¼‰
        if low_level_collection_allowed:
            for i in range(n_agents):
                obs = torch.FloatTensor(observations[i]).to(self.device)
                next_obs = torch.FloatTensor(next_observations[i]).to(self.device)
                action = torch.FloatTensor(actions[i]).to(self.device)
                done = dones[i] if isinstance(dones, list) else dones
                
                # è®¡ç®—ä¸ªä½“æŠ€èƒ½åˆ¤åˆ«å™¨è¾“å‡º
                with torch.no_grad():
                    agent_disc_logits = self.individual_discriminator(
                        next_obs.unsqueeze(0), 
                        team_skill_tensor
                    )
                    agent_disc_log_probs = F.log_softmax(agent_disc_logits, dim=-1)
                    agent_skill_log_prob = agent_disc_log_probs[0, agent_skills[i]]
                    
                # è®¡ç®—ä½å±‚å¥–åŠ±ï¼ˆEq. 4ï¼‰åŠå…¶ç»„æˆéƒ¨åˆ†
                env_reward_component = self.config.lambda_e * current_reward # ä½¿ç”¨ current_reward
                team_disc_component = self.config.lambda_D * team_skill_log_prob.item()
                ind_disc_component = self.config.lambda_d * agent_skill_log_prob.item()
                
                intrinsic_reward = env_reward_component + team_disc_component + ind_disc_component
                
                # å­˜å‚¨ä½å±‚ç»éªŒ
                low_level_experience = (
                    state_tensor,                           # å…¨å±€çŠ¶æ€s
                    team_skill_tensor,                      # å›¢é˜ŸæŠ€èƒ½Z
                    obs,                                    # æ™ºèƒ½ä½“è§‚æµ‹o_i
                    torch.tensor(agent_skills[i], device=self.device),  # ä¸ªä½“æŠ€èƒ½z_i
                    action,                                 # åŠ¨ä½œa_i
                    torch.tensor(intrinsic_reward, device=self.device),  # æ€»å†…åœ¨å¥–åŠ±r_i
                    torch.tensor(done, dtype=torch.float, device=self.device),  # æ˜¯å¦ç»“æŸ
                    torch.tensor(action_logprobs[i], device=self.device),  # åŠ¨ä½œå¯¹æ•°æ¦‚ç‡
                    torch.tensor(env_reward_component, device=self.device), # ç¯å¢ƒå¥–åŠ±éƒ¨åˆ†
                    torch.tensor(team_disc_component, device=self.device),  # å›¢é˜Ÿåˆ¤åˆ«å™¨éƒ¨åˆ†
                    torch.tensor(ind_disc_component, device=self.device)   # ä¸ªä½“åˆ¤åˆ«å™¨éƒ¨åˆ†
                )
                self.low_level_buffer.push(low_level_experience)
                
                # åœ¨åŒæ­¥æ¨¡å¼ä¸‹ï¼Œå¢åŠ æ ·æœ¬è®¡æ•°
                if self.sync_mode:
                    self.samples_collected_this_round += 1
        else:
            main_logger.debug(f"ç¯å¢ƒ{env_id}: è·³è¿‡ä½å±‚ç»éªŒå­˜å‚¨ï¼ˆåŒæ­¥æ¨¡å¼æ•°æ®æ”¶é›†å·²ç¦ç”¨ï¼‰")
            
        # å­˜å‚¨æŠ€èƒ½åˆ¤åˆ«å™¨è®­ç»ƒæ•°æ®
        observations_tensor = torch.FloatTensor(next_observations).to(self.device)
        agent_skills_tensor = torch.tensor(agent_skills, device=self.device)
        self.state_skill_dataset.push(
            next_state_tensor,
            team_skill_tensor,
            observations_tensor,
            agent_skills_tensor
        )
        
        # è·å–æˆ–åˆå§‹åŒ–å½“å‰ç¯å¢ƒçš„æŠ€èƒ½è®¡æ—¶å™¨
        if env_id not in self.env_timers:
            self.env_timers[env_id] = 0
        
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„æŠ€èƒ½è®¡æ—¶å™¨å€¼ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç¯å¢ƒä¸“ç”¨è®¡æ—¶å™¨
        skill_timer = skill_timer_for_env if skill_timer_for_env is not None else self.env_timers[env_id]
        
        # è®°å½•å½“å‰æŠ€èƒ½è®¡æ—¶å™¨çŠ¶æ€
        main_logger.debug(f"store_transition: ç¯å¢ƒID={env_id}, skill_timer={skill_timer}, k={self.config.k}, æ¡ä»¶åˆ¤æ–­={skill_timer == self.config.k - 1}")
        
        # è·å–æˆ–åˆå§‹åŒ–ç¯å¢ƒçš„æœ€åè´¡çŒ®æ—¶é—´
        if env_id not in self.env_last_contribution:
            self.env_last_contribution[env_id] = 0
        
        # è·å–æˆ–åˆå§‹åŒ–ç¯å¢ƒç‰¹å®šçš„å¥–åŠ±é˜ˆå€¼
        if env_id not in self.env_reward_thresholds:
            self.env_reward_thresholds[env_id] = 0.0  # å°†é»˜è®¤é˜ˆå€¼è®¾ä¸º0ï¼Œç¡®ä¿å§‹ç»ˆèƒ½å­˜å‚¨é«˜å±‚ç»éªŒ
        
        # åˆ¤æ–­è¯¥ç¯å¢ƒæ˜¯å¦éœ€è¦å¼ºåˆ¶æ”¶é›†é«˜å±‚æ ·æœ¬
        force_collection = self.force_high_level_collection.get(env_id, False)
        
        # ç®€åŒ–é€»è¾‘ï¼šå–æ¶ˆæ‰€æœ‰å¥–åŠ±é˜ˆå€¼ï¼Œç¡®ä¿å§‹ç»ˆæ”¶é›†é«˜å±‚æ ·æœ¬
        self.env_reward_thresholds[env_id] = 0.0
        
        # å¯¹é•¿æ—¶é—´æœªè´¡çŒ®çš„ç¯å¢ƒå¼ºåˆ¶æ”¶é›†
        steps_since_contribution = self.global_step - self.env_last_contribution.get(env_id, 0)
        if steps_since_contribution > 500:  # é™ä½æ£€æŸ¥é—´éš”è‡³500æ­¥
            self.force_high_level_collection[env_id] = True
            if steps_since_contribution % 500 == 0:  # é¿å…æ—¥å¿—è¿‡å¤š
                main_logger.info(f"ç¯å¢ƒID={env_id}å·²{steps_since_contribution}æ­¥æœªè´¡çŒ®é«˜å±‚æ ·æœ¬ï¼Œå°†å¼ºåˆ¶æ”¶é›†")
        
        # ã€ä¿®å¤ã€‘ç®€åŒ–é«˜å±‚ç»éªŒæ”¶é›†é€»è¾‘ï¼Œåªä½¿ç”¨ç¯å¢ƒç‰¹å®šçš„timeråˆ¤æ–­
        # åºŸå¼ƒglobal_stepåˆ¤æ–­ï¼Œè§£å†³å¤šç¯å¢ƒå¹¶è¡Œæ—¶çš„å†²çªé—®é¢˜
        timer_completed_cycle = skill_timer == self.config.k - 1
        
        # å­˜å‚¨é«˜å±‚ç»éªŒçš„æ¡ä»¶ï¼š
        # 1. æŠ€èƒ½å‘¨æœŸå®Œæˆï¼ˆåŸºäºç¯å¢ƒç‰¹å®štimerï¼‰
        # 2. ç¯å¢ƒç»ˆæ­¢
        # 3. å¼ºåˆ¶æ”¶é›†
        should_store_high_level = timer_completed_cycle or dones or force_collection
        
        # ã€è°ƒè¯•æ—¥å¿—ã€‘è®°å½•é«˜å±‚ç»éªŒæ”¶é›†çš„è¯¦ç»†åˆ¤æ–­è¿‡ç¨‹
        main_logger.debug(f"[HIGH_LEVEL_DEBUG] ç¯å¢ƒ{env_id} é«˜å±‚å­˜å‚¨æ£€æŸ¥: "
                          f"skill_timer={skill_timer}, k-1={self.config.k-1}, "
                          f"timer_completed_cycle={timer_completed_cycle}, "
                          f"dones={dones}, force_collection={force_collection}, "
                          f"should_store={should_store_high_level}, "
                          f"ç´¯ç§¯å¥–åŠ±={self.env_reward_sums.get(env_id, 0.0):.4f}, "
                          f"global_step={self.global_step}")
        
        if should_store_high_level:
            # è·å–å½“å‰ç¯å¢ƒçš„ç´¯ç§¯å¥–åŠ±
            env_accumulated_reward = self.env_reward_sums.get(env_id, 0.0)
            
            # è®°å½•é«˜å±‚ç»éªŒå­˜å‚¨æ£€æŸ¥ä¿¡æ¯
            reason = "æœªçŸ¥åŸå› "
            if timer_completed_cycle:
                reason = "å‘¨æœŸå®Œæˆæ£€æµ‹"
                main_logger.debug(f"ç¯å¢ƒID={env_id}æŠ€èƒ½å‘¨æœŸå®Œæˆæ£€æµ‹: ç´¯ç§¯å¥–åŠ±={env_accumulated_reward:.4f}, "
                               f"global_step={self.global_step}, k={self.config.k}")
            elif skill_timer == self.config.k - 1:
                reason = "æŠ€èƒ½å‘¨æœŸç»“æŸ"
                main_logger.debug(f"ç¯å¢ƒID={env_id}æŠ€èƒ½å‘¨æœŸç»“æŸ: ç´¯ç§¯å¥–åŠ±={env_accumulated_reward:.4f}, "
                               f"ç¦»ä¸Šæ¬¡è´¡çŒ®={steps_since_contribution}æ­¥, k={self.config.k}")
            elif dones:
                reason = "ç¯å¢ƒç»ˆæ­¢"
                # è®°å½•episodeç»“æŸä¿¡æ¯ï¼ŒåŒ…å«ç¯å¢ƒIDå’Œè¯¦ç»†ä¿¡æ¯
                episode_info = {
                    'env_id': env_id,
                    'total_reward': env_accumulated_reward,
                    'skill_timer': skill_timer,
                    'team_skill': team_skill,
                    'agent_skills': agent_skills
                }
                
                # ä»ç¯å¢ƒä¿¡æ¯ä¸­æå–é¢å¤–çš„episodeç»Ÿè®¡ä¿¡æ¯
                if hasattr(next_state, '__len__') and len(next_state) > 0:
                    episode_info['final_state_norm'] = float(torch.norm(torch.tensor(next_state)).item())
                
                # è®°å½•episodeç»“æŸçš„è¯¦ç»†ä¿¡æ¯
                main_logger.info(f"Episodeç»“æŸ - ç¯å¢ƒID: {env_id}, "
                               f"æ€»å¥–åŠ±: {env_accumulated_reward:.4f}, "
                               f"æŠ€èƒ½è®¡æ—¶å™¨: {skill_timer}, "
                               f"å›¢é˜ŸæŠ€èƒ½: {team_skill}, "
                               f"ä¸ªä½“æŠ€èƒ½: {agent_skills}")
            elif force_collection:
                reason = "å¼ºåˆ¶æ”¶é›†"
                main_logger.info(f"ç¯å¢ƒID={env_id}å¼ºåˆ¶æ”¶é›†: ç´¯ç§¯å¥–åŠ±={env_accumulated_reward:.4f}, æŠ€èƒ½è®¡æ—¶å™¨={skill_timer}")
            # åˆ›å»ºé«˜å±‚ç»éªŒå…ƒç»„
            high_level_experience = (
                state_tensor,                  # å…¨å±€çŠ¶æ€s
                team_skill_tensor,             # å›¢é˜ŸæŠ€èƒ½Z
                observations_tensor,           # æ‰€æœ‰æ™ºèƒ½ä½“è§‚æµ‹o
                agent_skills_tensor,           # æ‰€æœ‰ä¸ªä½“æŠ€èƒ½z
                torch.tensor(env_accumulated_reward, device=self.device) # å­˜å‚¨è¯¥ç¯å¢ƒçš„kæ­¥ç´¯ç§¯å¥–åŠ±
            )
            
            # å­˜å‚¨é«˜å±‚ç»éªŒ
            self.high_level_buffer.push(high_level_experience)
        
            # æ— è®ºbufferé•¿åº¦æ˜¯å¦å˜åŒ–ï¼Œéƒ½è®¤ä¸ºæˆåŠŸæ·»åŠ äº†ä¸€ä¸ªæ ·æœ¬
            # é¿å…åœ¨bufferæ»¡æ—¶å› ä¸ºé•¿åº¦ä¸å˜è€Œè¯¯åˆ¤ä¸ºæœªæ·»åŠ æ ·æœ¬
            samples_added = 1
            self.high_level_samples_total += samples_added
            # è®°å½•ç¯å¢ƒè´¡çŒ®
            self.high_level_samples_by_env[env_id] = self.high_level_samples_by_env.get(env_id, 0) + 1
            # è®°å½•åŸå› ç»Ÿè®¡
            self.high_level_samples_by_reason[reason] = self.high_level_samples_by_reason.get(reason, 0) + 1
            
            # æ›´æ–°ç¯å¢ƒæœ€åè´¡çŒ®æ—¶é—´
            self.env_last_contribution[env_id] = self.global_step
            
            # é‡ç½®å¼ºåˆ¶æ”¶é›†æ ‡å¿—
            if force_collection:
                self.force_high_level_collection[env_id] = False
            
            # æ¯æ”¶é›†5ä¸ªæ ·æœ¬è®°å½•ä¸€æ¬¡ç»Ÿè®¡ä¿¡æ¯ï¼ˆä»10æ”¹ä¸º5ï¼Œå¢åŠ åé¦ˆé¢‘ç‡ï¼‰
            if self.high_level_samples_total % 5 == 0:
                main_logger.debug(f"[HIGH_LEVEL_COLLECT] é«˜å±‚ç»éªŒç»Ÿè®¡ - æ€»æ ·æœ¬: {self.high_level_samples_total}, ç¯å¢ƒè´¡çŒ®: {self.high_level_samples_by_env}, åŸå› ç»Ÿè®¡: {self.high_level_samples_by_reason}")
                
                # è®°å½•åˆ°TensorBoard
                if hasattr(self, 'writer'):
                    self.writer.add_scalar('Buffer/high_level_samples_total', self.high_level_samples_total, self.global_step)
                    # è®°å½•å„ç¯å¢ƒçš„æ ·æœ¬è´¡çŒ®æ¯”ä¾‹
                    for e_id, count in self.high_level_samples_by_env.items():
                        self.writer.add_scalar(f'Buffer/env_{e_id}_contribution', count, self.global_step)
        
            # å¢åŠ æ—¥å¿—ä»¥ä¾¿è·Ÿè¸ªé«˜å±‚ç»éªŒæ·»åŠ çŠ¶æ€
            current_buffer_size = len(self.high_level_buffer)
            main_logger.debug(f"[HIGH_LEVEL_COLLECT] âœ“ é«˜å±‚ç»éªŒå·²æ·»åŠ : ç¯å¢ƒID={env_id}, step={self.global_step}, "
                           f"ç¼“å†²åŒºå¤§å°: {current_buffer_size}/{self.config.high_level_batch_size}, "
                           f"ç´¯ç§¯å¥–åŠ±: {env_accumulated_reward:.4f}, åŸå› : {reason}")
            
            # å°†å¸¦æœ‰log probabilitiesçš„ç»éªŒå­˜å‚¨åˆ°ä¸“ç”¨ç¼“å†²åŒº
            if log_probs is not None:
                self.high_level_buffer_with_logprobs.append({
                    'state': state_tensor.clone(),
                    'team_skill': team_skill,
                    'observations': observations_tensor.clone(),
                    'agent_skills': agent_skills_tensor.clone(),
                    'reward': env_accumulated_reward,  # ä½¿ç”¨ç¯å¢ƒç‰¹å®šçš„ç´¯ç§¯å¥–åŠ±
                    'team_log_prob': log_probs['team_log_prob'],
                    'agent_log_probs': log_probs['agent_log_probs']
                })
                
                # ä¿æŒç¼“å†²åŒºå¤§å°ä¸è¶…è¿‡config.buffer_size
                if len(self.high_level_buffer_with_logprobs) > self.config.buffer_size:
                    self.high_level_buffer_with_logprobs = self.high_level_buffer_with_logprobs[-self.config.buffer_size:]
            
            # é‡ç½®è¯¥ç¯å¢ƒçš„å¥–åŠ±ç´¯ç§¯
            self.env_reward_sums[env_id] = 0.0
            
            # é‡ç½®è¯¥ç¯å¢ƒçš„æŠ€èƒ½è®¡æ—¶å™¨
            self.env_timers[env_id] = 0
            
        else:
            # å¦‚æœä¸åˆ°æŠ€èƒ½å‘¨æœŸç»“æŸæ—¶é—´ï¼Œå¢åŠ è¯¥ç¯å¢ƒçš„æŠ€èƒ½è®¡æ—¶å™¨ï¼Œä½†ç¡®ä¿ä¸è¶…è¿‡k-1
            if self.env_timers[env_id] < self.config.k - 1:
                self.env_timers[env_id] += 1
        
        # è¿”å›æˆåŠŸå­˜å‚¨
        return True
    
    def update_coordinator(self):
        """æ›´æ–°é«˜å±‚æŠ€èƒ½åè°ƒå™¨ç½‘ç»œ"""
        # è®°å½•é«˜å±‚ç¼“å†²åŒºçŠ¶æ€
        buffer_len = len(self.high_level_buffer)
        required_batch_size = self.config.high_level_batch_size
        main_logger.info(f"[BUFFER_STATUS] é«˜å±‚ç¼“å†²åŒºçŠ¶æ€: {buffer_len}/{required_batch_size} (å½“å‰/æ‰€éœ€)")
        
        if buffer_len < required_batch_size:
            # å¦‚æœç¼“å†²åŒºä¸è¶³ï¼Œä½¿ç”¨è®¡æ•°å™¨å‡å°‘è­¦å‘Šæ—¥å¿—é¢‘ç‡
            # åªæœ‰å½“ç¼“å†²åŒºå¤§å°å˜åŒ–æˆ–è€…æ¯10æ¬¡æ›´æ–°æ‰è®°å½•ä¸€æ¬¡è­¦å‘Š
            if buffer_len != self.last_high_level_buffer_size or self.high_level_buffer_warning_counter % 10 == 0:
                main_logger.info(f"Training: æ”¶é›†ä¸­... é«˜å±‚ç¼“å†²åŒº: {buffer_len}/{required_batch_size} æ ·æœ¬")
            else:
                main_logger.debug(f"[BUFFER_STATUS] é«˜å±‚ç¼“å†²åŒºæ ·æœ¬ä¸è¶³ï¼Œéœ€è¦{required_batch_size}ä¸ªæ ·æœ¬ï¼Œä½†åªæœ‰{buffer_len}ä¸ªã€‚è·³è¿‡æ›´æ–°ã€‚")
            
            # æ›´æ–°è®¡æ•°å™¨å’Œä¸Šæ¬¡ç¼“å†²åŒºå¤§å°
            self.high_level_buffer_warning_counter += 1
            self.last_high_level_buffer_size = buffer_len
            
            # ä¿æŒä¸å‡½æ•°æ­£å¸¸è¿”å›å€¼ç›¸åŒæ•°é‡çš„å…ƒç´ 
            return 0, 0, 0, 0, 0, 0, 0, 0
        
        # ç¼“å†²åŒºå·²æ»¡ï¼Œç»§ç»­æ›´æ–°
        main_logger.info(f"[HIGH_LEVEL_UPDATE] é«˜å±‚ç¼“å†²åŒºæ»¡è¶³æ›´æ–°æ¡ä»¶ï¼Œä»{buffer_len}ä¸ªæ ·æœ¬ä¸­é‡‡æ ·{required_batch_size}ä¸ª")
            
        # ä»ç¼“å†²åŒºé‡‡æ ·æ•°æ®
        batch = self.high_level_buffer.sample(self.config.high_level_batch_size)
        states, team_skills, observations, agent_skills, rewards = zip(*batch)
        
        states = torch.stack(states)
        team_skills = torch.stack(team_skills)
        observations = torch.stack(observations)
        agent_skills = torch.stack(agent_skills)
        rewards = torch.stack(rewards) # rewardsç°åœ¨æ˜¯ç´¯ç§¯çš„kæ­¥å¥–åŠ±r_h
        
        # è®°å½•é«˜å±‚å¥–åŠ±çš„ç»Ÿè®¡ä¿¡æ¯
        reward_mean = rewards.mean().item()
        reward_std = rewards.std().item()
        reward_min = rewards.min().item()
        reward_max = rewards.max().item()
        main_logger.info(f"[HIGH_LEVEL_UPDATE] é«˜å±‚å¥–åŠ±ç»Ÿè®¡: å‡å€¼={reward_mean:.4f}, æ ‡å‡†å·®={reward_std:.4f}, æœ€å°å€¼={reward_min:.4f}, æœ€å¤§å€¼={reward_max:.4f}")
        
        # è·å–å½“å‰çŠ¶æ€ä»·å€¼
        state_values, agent_values = self.skill_coordinator.get_value(states, observations)
        
        # ç”±äºæˆ‘ä»¬å‡è®¾æ¯ä¸ªé«˜å±‚ç»éªŒéƒ½æ˜¯ä¸€ä¸ªkæ­¥åºåˆ—çš„ç«¯ç‚¹ï¼Œ
        # æ‰€ä»¥æˆ‘ä»¬å¯ä»¥å‡è®¾ä¸‹ä¸€çŠ¶æ€ä»·å€¼ä¸º0ï¼ˆæˆ–è€…å¯ä»¥ä»æ–°çš„çŠ¶æ€è®¡ç®—ï¼‰
        next_values = torch.zeros_like(state_values)
        
        # åœ¨è®¡ç®—GAEä¹‹å‰è¯¦ç»†è®°å½•å¥–åŠ±å’Œä»·å€¼çš„ç»Ÿè®¡ä¿¡æ¯
        rewards_mean = rewards.mean().item()
        rewards_std = rewards.std().item()
        rewards_min = rewards.min().item()
        rewards_max = rewards.max().item()
        state_values_mean = state_values.mean().item()
        state_values_std = state_values.std().item()
        state_values_min = state_values.min().item()
        state_values_max = state_values.max().item()
        
        main_logger.debug(f"GAEè¾“å…¥ç»Ÿè®¡:")
        main_logger.debug(f"  rewards: å‡å€¼={rewards_mean:.4f}, æ ‡å‡†å·®={rewards_std:.4f}, æœ€å°å€¼={rewards_min:.4f}, æœ€å¤§å€¼={rewards_max:.4f}")
        main_logger.debug(f"  state_values: å‡å€¼={state_values_mean:.4f}, æ ‡å‡†å·®={state_values_std:.4f}, æœ€å°å€¼={state_values_min:.4f}, æœ€å¤§å€¼={state_values_max:.4f}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼
        rewards_has_nan = torch.isnan(rewards).any().item()
        rewards_has_inf = torch.isinf(rewards).any().item()
        values_has_nan = torch.isnan(state_values).any().item()
        values_has_inf = torch.isinf(state_values).any().item()
        
        if rewards_has_nan or rewards_has_inf:
            main_logger.error(f"å¥–åŠ±ä¸­å­˜åœ¨NaNæˆ–Inf: NaN={rewards_has_nan}, Inf={rewards_has_inf}")
            # å°è¯•ä¿®å¤NaN/Infå€¼ï¼Œä»¥é¿å…æ•´ä¸ªè®­ç»ƒä¸­æ–­
            rewards = torch.nan_to_num(rewards, nan=0.0, posinf=10.0, neginf=-10.0)
            main_logger.info("å·²å°†å¥–åŠ±ä¸­çš„NaN/Infå€¼æ›¿æ¢ä¸ºæœ‰é™å€¼")
        
        if values_has_nan or values_has_inf:
            main_logger.error(f"çŠ¶æ€ä»·å€¼ä¸­å­˜åœ¨NaNæˆ–Inf: NaN={values_has_nan}, Inf={values_has_inf}")
            # å°è¯•ä¿®å¤NaN/Infå€¼
            state_values = torch.nan_to_num(state_values, nan=0.0, posinf=10.0, neginf=-10.0)
            main_logger.info("å·²å°†çŠ¶æ€ä»·å€¼ä¸­çš„NaN/Infå€¼æ›¿æ¢ä¸ºæœ‰é™å€¼")
        
        # è®¡ç®—GAE
        dones = torch.zeros_like(rewards)  # å‡è®¾é«˜å±‚ç»éªŒä¸åŒ…å«ç»ˆæ­¢ä¿¡æ¯
        # ç¡®ä¿ä¼ é€’ç»™compute_gaeçš„valuesæ˜¯1Dï¼Œä½¿ç”¨cloneé¿å…åŸåœ°æ“ä½œ
        try:
            advantages, returns = compute_gae(rewards.clone(), state_values.squeeze(-1).clone(), 
                                            next_values.squeeze(-1).clone(), dones.clone(), 
                                            self.config.gamma, self.config.gae_lambda)
            # advantages å’Œ returns éƒ½æ˜¯ [batch_size]ï¼Œåˆ†ç¦»è®¡ç®—å›¾
            advantages = advantages.detach()
            returns = returns.detach()
            
            # æ£€æŸ¥ advantages å’Œ returns çš„ç»Ÿè®¡ä¿¡æ¯
            adv_mean = advantages.mean().item()
            adv_std = advantages.std().item()
            adv_min = advantages.min().item()
            adv_max = advantages.max().item()
            ret_mean = returns.mean().item()
            ret_std = returns.std().item()
            ret_min = returns.min().item()
            ret_max = returns.max().item()
            
            main_logger.debug(f"GAEè¾“å‡ºç»Ÿè®¡:")
            main_logger.debug(f"  Advantages: å‡å€¼={adv_mean:.4f}, æ ‡å‡†å·®={adv_std:.4f}, æœ€å°å€¼={adv_min:.4f}, æœ€å¤§å€¼={adv_max:.4f}")
            main_logger.debug(f"  Returns: å‡å€¼={ret_mean:.4f}, æ ‡å‡†å·®={ret_std:.4f}, æœ€å°å€¼={ret_min:.4f}, æœ€å¤§å€¼={ret_max:.4f}")
            
            # æ£€æŸ¥GAEè¾“å‡ºæ˜¯å¦æœ‰å¼‚å¸¸å€¼
            adv_has_nan = torch.isnan(advantages).any().item()
            adv_has_inf = torch.isinf(advantages).any().item()
            ret_has_nan = torch.isnan(returns).any().item()
            ret_has_inf = torch.isinf(returns).any().item()
            
            if adv_has_nan or adv_has_inf:
                main_logger.error(f"advantagesä¸­å­˜åœ¨NaNæˆ–Inf: NaN={adv_has_nan}, Inf={adv_has_inf}")
                # å°è¯•ä¿®å¤NaN/Infå€¼
                advantages = torch.nan_to_num(advantages, nan=0.0, posinf=10.0, neginf=-10.0)
                main_logger.info("å·²å°†advantagesä¸­çš„NaN/Infå€¼æ›¿æ¢ä¸ºæœ‰é™å€¼")
            
            if ret_has_nan or ret_has_inf:
                main_logger.error(f"returnsä¸­å­˜åœ¨NaNæˆ–Inf: NaN={ret_has_nan}, Inf={ret_has_inf}")
                # å°è¯•ä¿®å¤NaN/Infå€¼
                returns = torch.nan_to_num(returns, nan=0.0, posinf=10.0, neginf=-10.0)
                main_logger.info("å·²å°†returnsä¸­çš„NaN/Infå€¼æ›¿æ¢ä¸ºæœ‰é™å€¼")
                
            # å½’ä¸€åŒ–advantagesï¼Œæœ‰åŠ©äºç¨³å®šè®­ç»ƒ
            if adv_std > 0:
                advantages = (advantages - adv_mean) / (adv_std + 1e-8)
                main_logger.debug("å·²å¯¹advantagesè¿›è¡Œå½’ä¸€åŒ–å¤„ç†")
                
        except Exception as e:
            main_logger.error(f"è®¡ç®—GAEæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            # ä½¿ç”¨å®‰å…¨çš„é»˜è®¤å€¼
            advantages = torch.zeros_like(rewards)
            returns = rewards.clone()  # åœ¨ç¼ºä¹æ›´å¥½é€‰æ‹©çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨åŸå§‹å¥–åŠ±ä½œä¸ºè¿”å›å€¼
            main_logger.info("ç”±äºGAEè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨å®‰å…¨çš„é»˜è®¤å€¼ä½œä¸ºæ›¿ä»£")
        
        # è·å–å½“å‰ç­–ç•¥
        try:
            Z, z, Z_logits, z_logits = self.skill_coordinator(states, observations)
            
            # åœ¨ä½¿ç”¨logitså‰æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼
            Z_logits_has_nan = torch.isnan(Z_logits).any().item()
            Z_logits_has_inf = torch.isinf(Z_logits).any().item()
            
            if Z_logits_has_nan or Z_logits_has_inf:
                main_logger.error(f"Z_logitsä¸­å­˜åœ¨NaNæˆ–Inf: NaN={Z_logits_has_nan}, Inf={Z_logits_has_inf}")
                # å°è¯•ä¿®å¤NaN/Infå€¼
                Z_logits = torch.nan_to_num(Z_logits, nan=0.0, posinf=10.0, neginf=-10.0)
                main_logger.info("å·²å°†Z_logitsä¸­çš„NaN/Infå€¼æ›¿æ¢ä¸ºæœ‰é™å€¼")
            
            # é‡æ–°è®¡ç®—å›¢é˜ŸæŠ€èƒ½æ¦‚ç‡åˆ†å¸ƒ
            team_skills_detached = team_skills.clone().detach()  # åˆ†ç¦»è®¡ç®—å›¾ï¼Œé˜²æ­¢åŸåœ°æ“ä½œ
            Z_dist = Categorical(logits=Z_logits)
            Z_log_probs = Z_dist.log_prob(team_skills_detached)
            Z_entropy = Z_dist.entropy().mean()
            
            # è®°å½•å›¢é˜ŸæŠ€èƒ½ç†µçš„ç»Ÿè®¡ä¿¡æ¯
            main_logger.debug(f"å›¢é˜ŸæŠ€èƒ½ç†µ: {Z_entropy.item():.4f}")
            
        except Exception as e:
            main_logger.error(f"åœ¨è®¡ç®—ç­–ç•¥åˆ†å¸ƒæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            # ä½¿ç”¨å®‰å…¨çš„é»˜è®¤å€¼
            batch_size = states.size(0)
            Z = torch.zeros(batch_size, dtype=torch.long, device=self.device)
            z = torch.zeros(batch_size, self.config.n_agents, dtype=torch.long, device=self.device)
            Z_logits = torch.zeros((batch_size, self.config.n_Z), device=self.device)
            z_logits = [torch.zeros((batch_size, self.config.n_z), device=self.device) for _ in range(self.config.n_agents)]
            Z_log_probs = torch.zeros(batch_size, device=self.device)
            Z_entropy = torch.tensor(0.0, device=self.device)
            main_logger.info("ç”±äºé”™è¯¯ï¼Œä½¿ç”¨å®‰å…¨çš„é»˜è®¤å€¼è¿›è¡Œè®¡ç®—")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¸¦log probabilitiesçš„é«˜å±‚ç»éªŒ
        use_stored_logprobs = len(self.high_level_buffer_with_logprobs) >= self.config.high_level_batch_size
        
        try:
            # è®¡ç®—é«˜å±‚ç­–ç•¥æŸå¤±
            if use_stored_logprobs:
                # ä½¿ç”¨å­˜å‚¨çš„log probabilitiesè®¡ç®—æ›´å‡†ç¡®çš„PPO ratio
                
                # ä»å¸¦log probabilitiesçš„ç¼“å†²åŒºä¸­éšæœºé€‰æ‹©æ ·æœ¬
                indices = torch.randperm(len(self.high_level_buffer_with_logprobs))[:self.config.high_level_batch_size]
                old_team_log_probs = [self.high_level_buffer_with_logprobs[i]['team_log_prob'] for i in indices]
                old_team_log_probs_tensor = torch.tensor(old_team_log_probs, device=self.device).detach()  # ä½¿ç”¨detach()é˜²æ­¢æ±‚å¯¼é”™è¯¯
                
                # æ£€æŸ¥old_team_log_probs_tensoræ˜¯å¦æœ‰å¼‚å¸¸å€¼
                old_log_probs_has_nan = torch.isnan(old_team_log_probs_tensor).any().item()
                old_log_probs_has_inf = torch.isinf(old_team_log_probs_tensor).any().item()
                
                if old_log_probs_has_nan or old_log_probs_has_inf:
                    main_logger.error(f"old_team_log_probs_tensorä¸­å­˜åœ¨NaNæˆ–Inf: NaN={old_log_probs_has_nan}, Inf={old_log_probs_has_inf}")
                    # å°è¯•ä¿®å¤NaN/Infå€¼
                    old_team_log_probs_tensor = torch.nan_to_num(old_team_log_probs_tensor, nan=0.0, posinf=0.0, neginf=0.0)
                    main_logger.info("å·²å°†old_team_log_probs_tensorä¸­çš„NaN/Infå€¼æ›¿æ¢ä¸º0")
                
                # è®°å½•log_probsçš„ç»Ÿè®¡ä¿¡æ¯
                main_logger.debug(f"å½“å‰log_probsç»Ÿè®¡: å‡å€¼={Z_log_probs.mean().item():.4f}, æ ‡å‡†å·®={Z_log_probs.std().item():.4f}")
                main_logger.debug(f"å†å²log_probsç»Ÿè®¡: å‡å€¼={old_team_log_probs_tensor.mean().item():.4f}, æ ‡å‡†å·®={old_team_log_probs_tensor.std().item():.4f}")
                
                # å®‰å…¨è®¡ç®—PPO ratioï¼Œé¿å…æ•°å€¼ä¸Šæº¢
                log_ratio = Z_log_probs - old_team_log_probs_tensor
                # è£å‰ªlog_ratioä»¥é¿å…expæ“ä½œå¯¼è‡´æ•°å€¼æº¢å‡º
                log_ratio = torch.clamp(log_ratio, -10.0, 10.0)
                Z_ratio = torch.exp(log_ratio)
                
                # è®°å½•ratioçš„ç»Ÿè®¡ä¿¡æ¯
                ratio_mean = Z_ratio.mean().item()
                ratio_std = Z_ratio.std().item()
                ratio_min = Z_ratio.min().item()
                ratio_max = Z_ratio.max().item()
                main_logger.debug(f"PPO ratioç»Ÿè®¡: å‡å€¼={ratio_mean:.4f}, æ ‡å‡†å·®={ratio_std:.4f}, æœ€å°å€¼={ratio_min:.4f}, æœ€å¤§å€¼={ratio_max:.4f}")
                
                # æ‰“å°debugä¿¡æ¯
                main_logger.debug(f"ä½¿ç”¨å­˜å‚¨çš„log probabilitiesè¿›è¡ŒPPOæ›´æ–°ï¼Œå…±æœ‰{len(self.high_level_buffer_with_logprobs)}ä¸ªæ ·æœ¬")
            else:
                # å¦‚æœæ²¡æœ‰å­˜å‚¨log probabilitiesï¼Œåˆ™å‡è®¾old_log_probs=0
                # åŒæ ·éœ€è¦è£å‰ªä»¥é¿å…æ•°å€¼æº¢å‡º
                log_ratio = torch.clamp(Z_log_probs, -10.0, 10.0)
                Z_ratio = torch.exp(log_ratio)
                main_logger.warning("æœªä½¿ç”¨å­˜å‚¨çš„log probabilitiesï¼Œå‡è®¾old_log_probs=0")
            
            # æ£€æŸ¥ratioæ˜¯å¦æœ‰å¼‚å¸¸å€¼
            ratio_has_nan = torch.isnan(Z_ratio).any().item()
            ratio_has_inf = torch.isinf(Z_ratio).any().item()
            
            if ratio_has_nan or ratio_has_inf:
                main_logger.error(f"Z_ratioä¸­å­˜åœ¨NaNæˆ–Inf: NaN={ratio_has_nan}, Inf={ratio_has_inf}")
                # å°è¯•ä¿®å¤NaN/Infå€¼
                Z_ratio = torch.nan_to_num(Z_ratio, nan=1.0, posinf=2.0, neginf=0.5)
                main_logger.info("å·²å°†Z_ratioä¸­çš„NaN/Infå€¼æ›¿æ¢ä¸ºæœ‰é™å€¼")
            
            # è®¡ç®—å¸¦è£å‰ªçš„ç›®æ ‡å‡½æ•°
            Z_surr1 = Z_ratio * advantages
            Z_surr2 = torch.clamp(Z_ratio, 1.0 - self.config.clip_epsilon, 1.0 + self.config.clip_epsilon) * advantages
            Z_policy_loss = -torch.min(Z_surr1, Z_surr2).mean()
            
            # æ£€æŸ¥æŸå¤±æ˜¯å¦æœ‰å¼‚å¸¸å€¼
            if torch.isnan(Z_policy_loss).any().item() or torch.isinf(Z_policy_loss).any().item():
                main_logger.error(f"Z_policy_lossåŒ…å«NaNæˆ–Infå€¼: {Z_policy_loss.item()}")
                # ä½¿ç”¨ä¸€ä¸ªå®‰å…¨çš„é»˜è®¤æŸå¤±å€¼
                Z_policy_loss = torch.tensor(0.1, device=self.device, requires_grad=True)
                main_logger.info("å·²å°†Z_policy_lossæ›¿æ¢ä¸ºå®‰å…¨çš„é»˜è®¤å€¼0.1")
                
        except Exception as e:
            main_logger.error(f"è®¡ç®—é«˜å±‚ç­–ç•¥æŸå¤±æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            # ä½¿ç”¨å®‰å…¨çš„é»˜è®¤æŸå¤±å€¼
            Z_policy_loss = torch.tensor(0.1, device=self.device, requires_grad=True)
            main_logger.info("ç”±äºé”™è¯¯ï¼Œä½¿ç”¨å®‰å…¨çš„é»˜è®¤å€¼0.1ä½œä¸ºZ_policy_loss")
        
        try:
            # è®¡ç®—é«˜å±‚ä»·å€¼æŸå¤± - ä½¿ç”¨é…ç½®åŒ–çš„Huber Lossæé«˜é²æ£’æ€§
            state_values = state_values.float() # Shape [batch_size, 1]
            # returns æ˜¯ [batch_size], éœ€è¦ unsqueeze åŒ¹é… state_values
            returns = returns.float().unsqueeze(-1) # Shape [batch_size, 1]
            
            # æ ¹æ®é…ç½®é€‰æ‹©æŸå¤±å‡½æ•°
            if getattr(self.config, 'use_huber_loss', True):
                # ä½¿ç”¨è‡ªé€‚åº”æˆ–é…ç½®çš„Huber Loss
                if getattr(self.config, 'huber_adaptive_delta', False):
                    delta = self.adaptive_coordinator_delta
                    main_logger.debug(f"ä½¿ç”¨è‡ªé€‚åº”Huber Lossè®¡ç®—åè°ƒå™¨ä»·å€¼æŸå¤±ï¼Œdelta={delta:.4f}")
                else:
                    delta = getattr(self.config, 'huber_coordinator_delta', 1.0)
                    main_logger.debug(f"ä½¿ç”¨å›ºå®šHuber Lossè®¡ç®—åè°ƒå™¨ä»·å€¼æŸå¤±ï¼Œdelta={delta}")
                Z_value_loss = huber_loss(state_values, returns, delta=delta)
            else:
                # ä½¿ç”¨ä¼ ç»Ÿçš„MSE Loss
                Z_value_loss = F.mse_loss(state_values, returns)
                main_logger.debug("ä½¿ç”¨MSE Lossè®¡ç®—åè°ƒå™¨ä»·å€¼æŸå¤±")
            
            # æ£€æŸ¥ä»·å€¼æŸå¤±æ˜¯å¦æœ‰å¼‚å¸¸å€¼
            if torch.isnan(Z_value_loss).any().item() or torch.isinf(Z_value_loss).any().item():
                main_logger.error(f"Z_value_lossåŒ…å«NaNæˆ–Infå€¼: {Z_value_loss.item()}")
                # ä½¿ç”¨å®‰å…¨çš„é»˜è®¤æŸå¤±å€¼
                Z_value_loss = torch.tensor(0.1, device=self.device, requires_grad=True)
                main_logger.info("å·²å°†Z_value_lossæ›¿æ¢ä¸ºå®‰å…¨çš„é»˜è®¤å€¼0.1")
            
        except Exception as e:
            main_logger.error(f"è®¡ç®—é«˜å±‚ä»·å€¼æŸå¤±æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            # ä½¿ç”¨å®‰å…¨çš„é»˜è®¤æŸå¤±å€¼
            Z_value_loss = torch.tensor(0.1, device=self.device, requires_grad=True)
            main_logger.info("ç”±äºé”™è¯¯ï¼Œä½¿ç”¨å®‰å…¨çš„é»˜è®¤å€¼0.1ä½œä¸ºZ_value_loss")
        
        # åˆå§‹åŒ–æ™ºèƒ½ä½“ç­–ç•¥æŸå¤±
        z_policy_losses = []
        z_entropy_losses = []
        z_value_losses = []
        
        # å¤„ç†æ¯ä¸ªæ™ºèƒ½ä½“çš„ä¸ªä½“æŠ€èƒ½æŸå¤±
        # ä½¿ç”¨å®é™…æ™ºèƒ½ä½“æ•°é‡ï¼Œç”±æ™ºèƒ½ä½“æŠ€èƒ½å½¢çŠ¶å†³å®šï¼Œè€Œä¸æ˜¯é…ç½®ä¸­çš„n_agents
        n_agents_actual = agent_skills.shape[1]  # ä»é‡‡æ ·çš„agent_skillsä¸­è·å–å®é™…æ™ºèƒ½ä½“æ•°é‡
        for i in range(n_agents_actual):
            agent_skills_i = agent_skills[:, i].clone().detach()  # åˆ†ç¦»è®¡ç®—å›¾ï¼Œé˜²æ­¢åŸåœ°æ“ä½œ
            zi_dist = Categorical(logits=z_logits[i])
            zi_log_probs = zi_dist.log_prob(agent_skills_i)
            zi_entropy = zi_dist.entropy().mean()
            
            if use_stored_logprobs:
                # ä½¿ç”¨å­˜å‚¨çš„agent log probabilities
                old_agent_log_probs = [self.high_level_buffer_with_logprobs[j]['agent_log_probs'][i] 
                                      for j in indices 
                                      if i < len(self.high_level_buffer_with_logprobs[j]['agent_log_probs'])]
                
                if len(old_agent_log_probs) == len(zi_log_probs):
                    old_agent_log_probs_tensor = torch.tensor(old_agent_log_probs, device=self.device).detach()  # ä½¿ç”¨detach()é˜²æ­¢æ±‚å¯¼é”™è¯¯
                    zi_ratio = torch.exp(zi_log_probs - old_agent_log_probs_tensor)
                else:
                    # å¦‚æœé•¿åº¦ä¸åŒ¹é…ï¼ˆä¾‹å¦‚æ™ºèƒ½ä½“æ•°é‡å˜åŒ–ï¼‰ï¼Œåˆ™é€€å›åˆ°å‡è®¾old_log_probs=0
                    zi_ratio = torch.exp(zi_log_probs)
            else:
                # å¦‚æœæ²¡æœ‰å­˜å‚¨çš„log probabilitiesï¼Œåˆ™å‡è®¾old_log_probs=0
                zi_ratio = torch.exp(zi_log_probs)
                
            zi_surr1 = zi_ratio * advantages
            zi_surr2 = torch.clamp(zi_ratio, 1.0 - self.config.clip_epsilon, 1.0 + self.config.clip_epsilon) * advantages
            zi_policy_loss = -torch.min(zi_surr1, zi_surr2).mean()
            
            z_policy_losses.append(zi_policy_loss)
            z_entropy_losses.append(zi_entropy)
            
            if i < len(agent_values):
                # ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
                agent_value = agent_values[i].float() # Shape [128, 1]
                # returns å·²ç»æ˜¯ [128, 1]
                returns_i = returns.float() 
                
                # ä½¿ç”¨Huber Loss (smooth_l1_loss) æ›¿ä»£MSE Loss
                zi_value_loss = F.smooth_l1_loss(agent_value, returns_i)
                z_value_losses.append(zi_value_loss)
        
        # åˆå¹¶æ‰€æœ‰æ™ºèƒ½ä½“çš„æŸå¤±
        z_policy_loss = torch.stack(z_policy_losses).mean()
        z_entropy = torch.stack(z_entropy_losses).mean()
        
        if z_value_losses:
            z_value_loss = torch.stack(z_value_losses).mean()
        else:
            z_value_loss = torch.tensor(0.0, device=self.device)
        
        try:
            # æ€»ç­–ç•¥æŸå¤±
            policy_loss = Z_policy_loss + z_policy_loss
            
            # æ€»ä»·å€¼æŸå¤±
            value_loss = Z_value_loss + z_value_loss
            
            # æ€»ç†µæŸå¤±
            entropy_loss = -(Z_entropy + z_entropy) * self.config.lambda_h
            
            # æ€»æŸå¤±
            loss = policy_loss + self.config.value_loss_coef * value_loss + entropy_loss
            
            # æ£€æŸ¥æ€»æŸå¤±æ˜¯å¦æœ‰å¼‚å¸¸å€¼
            if torch.isnan(loss).any().item() or torch.isinf(loss).any().item():
                main_logger.error(f"æ€»æŸå¤±åŒ…å«NaNæˆ–Infå€¼: {loss.item()}")
                # åˆ†ææŸå¤±ç»„æˆéƒ¨åˆ†
                main_logger.error(f"æŸå¤±ç»„æˆéƒ¨åˆ†: policy_loss={policy_loss.item()}, value_loss={value_loss.item()}, entropy_loss={entropy_loss.item()}")
                
                # å°è¯•åˆ›å»ºä¸€ä¸ªæ–°çš„ã€å®‰å…¨çš„æŸå¤±
                policy_loss_safe = torch.tensor(0.1, device=self.device, requires_grad=True)
                value_loss_safe = torch.tensor(0.1, device=self.device, requires_grad=True)
                entropy_loss_safe = torch.tensor(-0.1, device=self.device, requires_grad=True)
                loss = policy_loss_safe + self.config.value_loss_coef * value_loss_safe + entropy_loss_safe
                main_logger.info("å·²å°†æ€»æŸå¤±æ›¿æ¢ä¸ºå®‰å…¨çš„é»˜è®¤å€¼")
            
            # è®°å½•æŸå¤±å€¼
            main_logger.debug(f"æŸå¤±ç»Ÿè®¡: æ€»æŸå¤±={loss.item():.6f}, ç­–ç•¥æŸå¤±={policy_loss.item():.6f}, ä»·å€¼æŸå¤±={value_loss.item():.6f}, ç†µæŸå¤±={entropy_loss.item():.6f}")
            
            # æ›´æ–°ç½‘ç»œ
            self.coordinator_optimizer.zero_grad()
            loss.backward()
            
        except Exception as e:
            main_logger.error(f"è®¡ç®—æ€»æŸå¤±æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            # åˆ›å»ºä¸€ä¸ªæ–°çš„ã€å®‰å…¨çš„æŸå¤±
            loss = torch.tensor(0.3, device=self.device, requires_grad=True)
            policy_loss = torch.tensor(0.1, device=self.device)
            value_loss = torch.tensor(0.1, device=self.device)
            entropy_loss = torch.tensor(-0.1, device=self.device)
            
            main_logger.info("ç”±äºé”™è¯¯ï¼Œä½¿ç”¨å®‰å…¨çš„é»˜è®¤å€¼ä½œä¸ºæŸå¤±")
            
            # æ›´æ–°ç½‘ç»œ
            self.coordinator_optimizer.zero_grad()
            loss.backward()
        
        # æ£€æŸ¥lossæ˜¯å¦æ­£ç¡®è¿æ¥åˆ°è®¡ç®—å›¾
        main_logger.debug(f"æŸå¤±è¿æ¥çŠ¶æ€: requires_grad={loss.requires_grad}, grad_fn={loss.grad_fn}")
        
        # æ£€æŸ¥coordinatorå‚æ•°æ˜¯å¦æ­£ç¡®è®¾ç½®requires_grad
        params_requiring_grad = 0
        for name, param in self.skill_coordinator.named_parameters():
            if param.requires_grad:
                params_requiring_grad += 1
                main_logger.debug(f"å‚æ•° {name} requires_grad=True")
        main_logger.debug(f"Coordinatorä¸­éœ€è¦æ¢¯åº¦çš„å‚æ•°æ•°é‡: {params_requiring_grad}")
        
        # è¯¦ç»†è®°å½•æ¢¯åº¦ä¿¡æ¯
        params_with_grads = [p for p in self.skill_coordinator.parameters() if p.grad is not None]
        if params_with_grads:
            # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦åŒ…å«NaNæˆ–Inf
            has_nan_grad = any(torch.isnan(p.grad).any().item() for p in params_with_grads)
            has_inf_grad = any(torch.isinf(p.grad).any().item() for p in params_with_grads)
            
            if has_nan_grad or has_inf_grad:
                main_logger.error(f"æ¢¯åº¦ä¸­åŒ…å«NaNæˆ–Infå€¼: NaN={has_nan_grad}, Inf={has_inf_grad}")
                # å°è¯•ä¿®å¤æ¢¯åº¦ä¸­çš„NaN/Infå€¼
                for p in params_with_grads:
                    if torch.isnan(p.grad).any() or torch.isinf(p.grad).any():
                        p.grad.data = torch.nan_to_num(p.grad.data, nan=0.0, posinf=1.0, neginf=-1.0)
                main_logger.info("å·²å°†æ¢¯åº¦ä¸­çš„NaNå’ŒInfå€¼æ›¿æ¢ä¸ºæœ‰é™å€¼")
            
            # è®¡ç®—æ¢¯åº¦çš„ç»Ÿè®¡ä¿¡æ¯
            grad_norms = [torch.norm(p.grad.detach()).item() for p in params_with_grads]
            mean_norm = np.mean(grad_norms)
            max_norm = max(grad_norms)
            min_norm = min(grad_norms)
            std_norm = np.std(grad_norms)
            total_norm = torch.sqrt(sum(p.grad.detach().pow(2).sum() for p in params_with_grads)).item()
            
            main_logger.debug(f"æ¢¯åº¦ç»Ÿè®¡ (è£å‰ªå‰): æ€»èŒƒæ•°={total_norm:.6f}, å‡å€¼={mean_norm:.6f}, "
                             f"æ ‡å‡†å·®={std_norm:.6f}, æœ€å¤§={max_norm:.6f}, æœ€å°={min_norm:.6f}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¾ƒå¤§æ¢¯åº¦
            large_grad_threshold = 10.0
            large_grads = [(name, torch.norm(param.grad).item()) 
                           for name, param in self.skill_coordinator.named_parameters() 
                           if param.grad is not None and torch.norm(param.grad).item() > large_grad_threshold]
            
            if large_grads:
                main_logger.warning(f"æ£€æµ‹åˆ°{len(large_grads)}ä¸ªå‚æ•°å…·æœ‰è¾ƒå¤§æ¢¯åº¦ (>{large_grad_threshold}):")
                for name, norm in large_grads[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    main_logger.warning(f"  å‚æ•° {name}: æ¢¯åº¦èŒƒæ•° = {norm:.6f}")
                if len(large_grads) > 5:
                    main_logger.warning(f"  ... è¿˜æœ‰{len(large_grads)-5}ä¸ªå‚æ•°æœ‰è¾ƒå¤§æ¢¯åº¦")
            
            # æ¢¯åº¦è£å‰ª
            try:
                torch.nn.utils.clip_grad_norm_(self.skill_coordinator.parameters(), self.config.max_grad_norm)
                
                # è®°å½•è£å‰ªåçš„æ¢¯åº¦ä¿¡æ¯
                params_with_grads_after = [p for p in self.skill_coordinator.parameters() if p.grad is not None]
                if params_with_grads_after:
                    grad_norms_after = [torch.norm(p.grad.detach()).item() for p in params_with_grads_after]
                    mean_norm_after = np.mean(grad_norms_after)
                    max_norm_after = max(grad_norms_after)
                    min_norm_after = min(grad_norms_after)
                    std_norm_after = np.std(grad_norms_after)
                    total_norm_after = torch.sqrt(sum(p.grad.detach().pow(2).sum() for p in params_with_grads_after)).item()
                    
                    main_logger.debug(f"æ¢¯åº¦ç»Ÿè®¡ (è£å‰ªå): æ€»èŒƒæ•°={total_norm_after:.6f}, å‡å€¼={mean_norm_after:.6f}, "
                                     f"æ ‡å‡†å·®={std_norm_after:.6f}, æœ€å¤§={max_norm_after:.6f}, æœ€å°={min_norm_after:.6f}")
            except Exception as e:
                main_logger.error(f"æ¢¯åº¦è£å‰ªå¤±è´¥: {e}")
                
        else:
            main_logger.warning("æ²¡æœ‰å‚æ•°æ¥æ”¶åˆ°æ¢¯åº¦! æ£€æŸ¥loss.backward()æ˜¯å¦æ­£ç¡®ä¼ æ’­æ¢¯åº¦ã€‚")
            
            # è¯¦ç»†æ£€æŸ¥æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦çŠ¶æ€
            grad_status = {}
            for name, param in self.skill_coordinator.named_parameters():
                if param.grad is None:
                    grad_status[name] = "None"
                else:
                    norm = torch.norm(param.grad).item()
                    has_nan = torch.isnan(param.grad).any().item()
                    has_inf = torch.isinf(param.grad).any().item()
                    grad_status[name] = f"æœ‰æ¢¯åº¦ï¼ŒèŒƒæ•°: {norm:.6f}, NaN: {has_nan}, Inf: {has_inf}"
            
            # è®°å½•æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦çŠ¶æ€
            main_logger.debug("è¯¦ç»†çš„å‚æ•°æ¢¯åº¦çŠ¶æ€:")
            for name, status in grad_status.items():
                main_logger.debug(f"å‚æ•° {name} æ¢¯åº¦çŠ¶æ€: {status}")
        
        # è®°å½•å‚æ•°æ›´æ–°å‰çš„å¤šä¸ªç½‘ç»œå‚æ•°æ ·æœ¬
        sample_params = {}
        for name, param in list(self.skill_coordinator.named_parameters())[:5]:  # åªå–å‰5ä¸ªå‚æ•°ä½œä¸ºæ ·æœ¬
            if param.requires_grad and param.numel() > 0:
                sample_params[name] = param.clone().detach()
                main_logger.debug(f"å‚æ•° {name} æ›´æ–°å‰: å‡å€¼={param.mean().item():.6f}, æ ‡å‡†å·®={param.std().item():.6f}")
        
        try:
            self.coordinator_optimizer.step()
            
            # è®°å½•å‚æ•°æ›´æ–°åçš„å˜åŒ–
            for name, old_param in sample_params.items():
                for curr_name, curr_param in self.skill_coordinator.named_parameters():
                    if curr_name == name:
                        param_mean_diff = (curr_param.detach().mean() - old_param.mean()).item()
                        param_abs_diff = torch.mean(torch.abs(curr_param.detach() - old_param)).item()
                        main_logger.debug(f"å‚æ•° {name} æ›´æ–°å: å‡å€¼å˜åŒ–={param_mean_diff:.6f}, å¹³å‡ç»å¯¹å˜åŒ–={param_abs_diff:.6f}")
                        break
                        
        except Exception as e:
            main_logger.error(f"ä¼˜åŒ–å™¨stepå¤±è´¥: {e}")
            # è¿™ç§æƒ…å†µä¸‹æˆ‘ä»¬æ— æ³•ç»§ç»­ï¼Œä½†è‡³å°‘è®°å½•äº†é”™è¯¯
        
        # è®¡ç®—å¹³å‡ä»·å€¼ä¼°è®¡
        mean_state_value = state_values.mean().item()
        mean_agent_value = 0.0
        if agent_values and len(agent_values) > 0:
            # agent_values æ˜¯ä¸€ä¸ªåˆ—è¡¨çš„å¼ é‡ï¼Œæ¯ä¸ªå¼ é‡æ˜¯ [batch_size, 1]
            # æˆ‘ä»¬éœ€è¦å°†å®ƒä»¬å †å èµ·æ¥ï¼Œç„¶åè®¡ç®—å‡å€¼
            stacked_agent_values = torch.stack(agent_values, dim=0) # Shape [n_agents, batch_size, 1]
            mean_agent_value = stacked_agent_values.mean().item()
        
        # rewards æ˜¯ç´¯ç§¯çš„kæ­¥ç¯å¢ƒå¥–åŠ± r_h
        mean_high_level_reward = rewards.mean().item()
            
        # è¿”å›ï¼šæ€»æŸå¤±, ç­–ç•¥æŸå¤±, ä»·å€¼æŸå¤±, å›¢é˜Ÿç†µ, ä¸ªä½“ç†µ, çŠ¶æ€ä»·å€¼å‡å€¼, æ™ºèƒ½ä½“ä»·å€¼å‡å€¼, é«˜å±‚å¥–åŠ±å‡å€¼
        return loss.item(), policy_loss.item(), value_loss.item(), \
               Z_entropy.item(), z_entropy.item(), \
               mean_state_value, mean_agent_value, mean_high_level_reward
    
    def update_discoverer(self):
        """æ›´æ–°ä½å±‚æŠ€èƒ½å‘ç°å™¨ç½‘ç»œ"""
        if len(self.low_level_buffer) < self.config.batch_size:
            return 0, 0, 0, 0, 0, 0, 0, 0, 0 # å¢åŠ è¿”å›æ•°é‡ä»¥åŒ¹é…æœŸæœ›ï¼ˆ9ä¸ªå€¼ï¼‰
        
        # ä»ç¼“å†²åŒºé‡‡æ ·æ•°æ®ï¼ŒåŒ…å«å†…åœ¨å¥–åŠ±çš„ä¸‰ä¸ªç»„æˆéƒ¨åˆ†
        batch = self.low_level_buffer.sample(self.config.batch_size)
        states, team_skills, observations, agent_skills, actions, rewards, dones, old_log_probs, \
        env_rewards_comp, team_disc_rewards_comp, ind_disc_rewards_comp = zip(*batch)
        
        states = torch.stack(states)
        team_skills = torch.stack(team_skills)
        observations = torch.stack(observations)
        agent_skills = torch.stack(agent_skills)
        actions = torch.stack(actions)
        rewards = torch.stack(rewards)
        dones = torch.stack(dones)
        old_log_probs = torch.stack(old_log_probs)
        
        # åˆå§‹åŒ–GRUéšè—çŠ¶æ€
        self.skill_discoverer.init_hidden(batch_size=self.config.batch_size)
        
        # è·å–å½“å‰çŠ¶æ€ä»·å€¼
        values = self.skill_discoverer.get_value(states, team_skills)
        
        # æ„é€ ä¸‹ä¸€çŠ¶æ€çš„å ä½ç¬¦
        next_values = torch.zeros_like(values)  # å®é™…åº”ç”¨ä¸­åº”è¯¥ä½¿ç”¨çœŸå®ä¸‹ä¸€çŠ¶æ€è®¡ç®—
        
        # è®¡ç®—GAE
        # ç¡®ä¿ä¼ é€’ç»™compute_gaeçš„valuesæ˜¯1Dï¼Œä½¿ç”¨cloneé¿å…åŸåœ°æ“ä½œ
        advantages, returns = compute_gae(rewards.clone(), values.squeeze(-1).clone(), 
                                         next_values.squeeze(-1).clone(), dones.clone(), 
                                         self.config.gamma, self.config.gae_lambda)
        # advantages å’Œ returns éƒ½æ˜¯ [batch_size]ï¼Œåˆ†ç¦»è®¡ç®—å›¾
        advantages = advantages.detach()
        returns = returns.detach()
        
        # é‡æ–°åˆå§‹åŒ–GRUéšè—çŠ¶æ€
        self.skill_discoverer.init_hidden(batch_size=self.config.batch_size)
        
        # è·å–å½“å‰ç­–ç•¥
        _, action_log_probs, action_dist = self.skill_discoverer(observations, agent_skills)
        
        # è®¡ç®—ç­–ç•¥æ¯”ç‡ï¼Œä½¿ç”¨detach()é˜²æ­¢æ±‚å¯¼é”™è¯¯
        old_log_probs_detached = old_log_probs.clone().detach()
        ratios = torch.exp(action_log_probs - old_log_probs_detached)
        
        # é™åˆ¶ç­–ç•¥æ¯”ç‡
        surr1 = ratios * advantages
        surr2 = torch.clamp(ratios, 1.0 - self.config.clip_epsilon, 1.0 + self.config.clip_epsilon) * advantages
        
        # è®¡ç®—ç­–ç•¥æŸå¤±
        policy_loss = -torch.min(surr1, surr2).mean()
        
        # é‡æ–°åˆå§‹åŒ–GRUéšè—çŠ¶æ€
        self.skill_discoverer.init_hidden(batch_size=self.config.batch_size)
        
        # è®¡ç®—ä»·å€¼æŸå¤± - ä½¿ç”¨é…ç½®åŒ–çš„Huber Lossæé«˜é²æ£’æ€§
        current_values = self.skill_discoverer.get_value(states, team_skills) # Shape [128, 1]
        # ç¡®ä¿ç»´åº¦åŒ¹é…å¹¶è½¬æ¢ä¸ºfloat32ç±»å‹
        current_values = current_values.float()
        # returns æ˜¯ [128], éœ€è¦ unsqueeze åŒ¹é… current_values
        returns = returns.float().unsqueeze(-1) # Shape [128, 1]
        
        # æ ¹æ®é…ç½®é€‰æ‹©æŸå¤±å‡½æ•°
        if getattr(self.config, 'use_huber_loss', True):
            # ä½¿ç”¨è‡ªé€‚åº”æˆ–é…ç½®çš„Huber Loss
            if getattr(self.config, 'huber_adaptive_delta', False):
                delta = self.adaptive_discoverer_delta
                main_logger.debug(f"ä½¿ç”¨è‡ªé€‚åº”Huber Lossè®¡ç®—å‘ç°å™¨ä»·å€¼æŸå¤±ï¼Œdelta={delta:.4f}")
            else:
                delta = getattr(self.config, 'huber_discoverer_delta', 1.0)
                main_logger.debug(f"ä½¿ç”¨å›ºå®šHuber Lossè®¡ç®—å‘ç°å™¨ä»·å€¼æŸå¤±ï¼Œdelta={delta}")
            value_loss = huber_loss(current_values, returns, delta=delta)
        else:
            # ä½¿ç”¨ä¼ ç»Ÿçš„MSE Loss
            value_loss = F.mse_loss(current_values, returns)
            main_logger.debug("ä½¿ç”¨MSE Lossè®¡ç®—å‘ç°å™¨ä»·å€¼æŸå¤±")
        
        # è®¡ç®—ç†µæŸå¤±
        entropy_loss = -action_dist.entropy().mean() * self.config.lambda_l
        
        # æ€»æŸå¤±
        loss = policy_loss + self.config.value_loss_coef * value_loss + entropy_loss
        
        # æ›´æ–°ç½‘ç»œ
        self.discoverer_optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.skill_discoverer.parameters(), self.config.max_grad_norm)
        self.discoverer_optimizer.step()
        
        # æ¸…ç©ºä½å±‚ç¼“å†²åŒºï¼Œç¡®ä¿on-policyè®­ç»ƒ
        buffer_size_before = len(self.low_level_buffer)
        self.low_level_buffer.clear()
        main_logger.info(f"åº•å±‚ç­–ç•¥æ›´æ–°å®Œæˆï¼Œå·²æ¸…ç©ºlow_level_bufferï¼ˆä¹‹å‰å¤§å°: {buffer_size_before}ï¼‰")
        
        # è®¡ç®—å†…åœ¨å¥–åŠ±å„éƒ¨åˆ†çš„å¹³å‡å€¼
        avg_intrinsic_reward = rewards.mean().item()
        avg_env_reward_comp = torch.stack(env_rewards_comp).mean().item()
        avg_team_disc_reward_comp = torch.stack(team_disc_rewards_comp).mean().item()
        avg_ind_disc_reward_comp = torch.stack(ind_disc_rewards_comp).mean().item()
        avg_discoverer_value = current_values.mean().item() # ä½¿ç”¨æ›´æ–°å‰çš„ current_values
        
        action_entropy_val = -entropy_loss.item() / self.config.lambda_l if self.config.lambda_l > 0 else 0.0

        return loss.item(), policy_loss.item(), value_loss.item(), action_entropy_val, \
               avg_intrinsic_reward, avg_env_reward_comp, avg_team_disc_reward_comp, avg_ind_disc_reward_comp, avg_discoverer_value
    
    def update_discriminators(self):
        """æ›´æ–°æŠ€èƒ½åˆ¤åˆ«å™¨ç½‘ç»œ"""
        if len(self.state_skill_dataset) < self.config.batch_size:
            return 0
        
        # ä»æ•°æ®é›†é‡‡æ ·æ•°æ®
        batch = self.state_skill_dataset.sample(self.config.batch_size)
        states, team_skills, observations, agent_skills = zip(*batch)
        
        states = torch.stack(states)
        team_skills = torch.stack(team_skills)
        observations = torch.stack(observations)
        agent_skills = torch.stack(agent_skills)
        
        # æ›´æ–°å›¢é˜ŸæŠ€èƒ½åˆ¤åˆ«å™¨
        team_disc_logits = self.team_discriminator(states)
        team_disc_loss = F.cross_entropy(team_disc_logits, team_skills)
        
        # æ›´æ–°ä¸ªä½“æŠ€èƒ½åˆ¤åˆ«å™¨
        batch_size, n_agents = agent_skills.shape
        
        # æ‰å¹³åŒ–å¤„ç†
        observations_flat = observations.reshape(-1, observations.size(-1))
        agent_skills_flat = agent_skills.reshape(-1)
        team_skills_expanded = team_skills.unsqueeze(1).expand(-1, n_agents).reshape(-1)
        
        agent_disc_logits = self.individual_discriminator(observations_flat, team_skills_expanded)
        agent_disc_loss = F.cross_entropy(agent_disc_logits, agent_skills_flat)
        
        # æ€»æŠ€èƒ½åˆ¤åˆ«å™¨æŸå¤±
        disc_loss = team_disc_loss + agent_disc_loss
        
        # æ›´æ–°ç½‘ç»œ
        self.discriminator_optimizer.zero_grad()
        disc_loss.backward()
        self.discriminator_optimizer.step()
        
        return disc_loss.item()
    
    def update_adaptive_delta(self, coordinator_value_loss, discoverer_value_loss):
        """
        è‡ªé€‚åº”è°ƒæ•´Huber Lossçš„deltaå‚æ•°
        
        å‚æ•°:
            coordinator_value_loss: åè°ƒå™¨ä»·å€¼æŸå¤±
            discoverer_value_loss: å‘ç°å™¨ä»·å€¼æŸå¤±
        """
        if not getattr(self.config, 'huber_adaptive_delta', False):
            return
        
        self.delta_update_count += 1
        
        # æ¯100æ¬¡æ›´æ–°è°ƒæ•´ä¸€æ¬¡delta
        if self.delta_update_count % 100 == 0:
            decay_rate = getattr(self.config, 'huber_delta_decay', 0.999)
            min_delta = getattr(self.config, 'huber_min_delta', 0.1)
            
            # æ ¹æ®æŸå¤±å¤§å°è°ƒæ•´delta
            if coordinator_value_loss > 1.0:  # æŸå¤±è¾ƒå¤§æ—¶å¢åŠ delta
                self.adaptive_coordinator_delta = min(self.adaptive_coordinator_delta * 1.1, 2.0)
            elif coordinator_value_loss < 0.1:  # æŸå¤±è¾ƒå°æ—¶å‡å°‘delta
                self.adaptive_coordinator_delta = max(self.adaptive_coordinator_delta * decay_rate, min_delta)
            
            if discoverer_value_loss > 1.0:
                self.adaptive_discoverer_delta = min(self.adaptive_discoverer_delta * 1.1, 2.0)
            elif discoverer_value_loss < 0.1:
                self.adaptive_discoverer_delta = max(self.adaptive_discoverer_delta * decay_rate, min_delta)
            
            main_logger.debug(f"è‡ªé€‚åº”deltaæ›´æ–°: coordinator_delta={self.adaptive_coordinator_delta:.4f}, "
                             f"discoverer_delta={self.adaptive_discoverer_delta:.4f}")
            
            # è®°å½•åˆ°TensorBoard
            if hasattr(self, 'writer'):
                self.writer.add_scalar('HuberLoss/AdaptiveCoordinatorDelta', self.adaptive_coordinator_delta, self.global_step)
                self.writer.add_scalar('HuberLoss/AdaptiveDiscovererDelta', self.adaptive_discoverer_delta, self.global_step)

    def update(self):
        """æ›´æ–°æ‰€æœ‰ç½‘ç»œ"""
        # æ›´æ–°å…¨å±€æ­¥æ•°
        self.global_step += 1
        main_logger.debug(f"HMASDAgent.update (step {self.global_step}): self.writer object: {self.writer}")
        
        # æ›´é¢‘ç¹åœ°æ£€æŸ¥ç¯å¢ƒè´¡çŒ®æƒ…å†µï¼ˆä»1000æ­¥é™è‡³200æ­¥ï¼‰
        if self.global_step % 200 == 0:
            # è·å–æ‰€æœ‰ç¯å¢ƒçš„è´¡çŒ®æƒ…å†µ
            env_contributions = {}
            for env_id in range(32):  # å‡è®¾æœ€å¤š32ä¸ªå¹¶è¡Œç¯å¢ƒ
                env_contributions[env_id] = self.high_level_samples_by_env.get(env_id, 0)
            
            # æ‰¾å‡ºè´¡çŒ®è¾ƒå°‘çš„ç¯å¢ƒï¼Œé™ä½è´¡çŒ®é˜ˆå€¼ä½¿æ›´å¤šç¯å¢ƒè¢«æ ‡è®°
            low_contribution_envs = {env_id: count for env_id, count in env_contributions.items() if count < 3}
            if low_contribution_envs:
                main_logger.info(f"ä»¥ä¸‹ç¯å¢ƒè´¡çŒ®æ ·æœ¬è¾ƒå°‘ï¼Œå°†å¼ºåˆ¶å…¶åœ¨ä¸‹ä¸€ä¸ªæŠ€èƒ½å‘¨æœŸç»“æŸæ—¶è´¡çŒ®: {low_contribution_envs}")
                # æ ‡è®°è¿™äº›ç¯å¢ƒåœ¨ä¸‹ä¸€ä¸ªæŠ€èƒ½å‘¨æœŸç»“æŸæ—¶å¼ºåˆ¶è´¡çŒ®æ ·æœ¬
                for env_id in low_contribution_envs:
                    self.force_high_level_collection[env_id] = True
                    # åŒæ—¶å°†è¿™äº›ç¯å¢ƒçš„å¥–åŠ±é˜ˆå€¼é‡ç½®ä¸º0
                    self.env_reward_thresholds[env_id] = 0.0
            
            # è®°å½•é«˜å±‚ç¼“å†²åŒºçŠ¶æ€
            high_level_buffer_size = len(self.high_level_buffer)
            main_logger.debug(f"å½“å‰é«˜å±‚ç¼“å†²åŒºå¤§å°: {high_level_buffer_size}/{self.config.high_level_batch_size} (å½“å‰/æ‰€éœ€)")
            
            # å¦‚æœé«˜å±‚ç¼“å†²åŒºå¢é•¿è¿‡æ…¢ï¼Œå¼ºåˆ¶æ‰€æœ‰ç¯å¢ƒè¿›è¡Œè´¡çŒ®
            if high_level_buffer_size < self.config.high_level_batch_size * 0.5 and self.global_step > 5000:
                main_logger.warning(f"é«˜å±‚ç¼“å†²åŒºå¢é•¿è¿‡æ…¢ ({high_level_buffer_size}/{self.config.high_level_batch_size})ï¼Œå¼ºåˆ¶æ‰€æœ‰ç¯å¢ƒè´¡çŒ®æ ·æœ¬")
                for env_id in range(32):
                    self.force_high_level_collection[env_id] = True
                    self.env_reward_thresholds[env_id] = 0.0
            
            # è®°å½•ç¯å¢ƒè´¡çŒ®åˆ†å¸ƒåˆ°TensorBoard
            if hasattr(self, 'writer'):
                contrib_data = np.zeros(32)
                for env_id, count in env_contributions.items():
                    contrib_data[env_id] = count
                # è®°å½•è´¡çŒ®æ ‡å‡†å·®ï¼Œè¡¡é‡æ˜¯å¦å¹³è¡¡
                contrib_std = np.std(contrib_data)
                self.writer.add_scalar('Buffer/contribution_stddev', contrib_std, self.global_step)
                # è®°å½•æœ‰æ•ˆè´¡çŒ®ç¯å¢ƒæ•°é‡
                contrib_envs = np.sum(contrib_data > 0)
                self.writer.add_scalar('Buffer/contributing_envs_count', contrib_envs, self.global_step)
        
        # æ›´æ–°æŠ€èƒ½åˆ¤åˆ«å™¨
        discriminator_loss = self.update_discriminators()
        
        # æ›´æ–°é«˜å±‚æŠ€èƒ½åè°ƒå™¨
        coordinator_loss, coordinator_policy_loss, coordinator_value_loss, team_skill_entropy, agent_skill_entropy, \
        mean_coord_state_val, mean_coord_agent_val, mean_high_level_reward = self.update_coordinator()
        
        # æ›´æ–°ä½å±‚æŠ€èƒ½å‘ç°å™¨
        discoverer_loss, discoverer_policy_loss, discoverer_value_loss, action_entropy, \
        avg_intrinsic_reward, avg_env_comp, avg_team_disc_comp, avg_ind_disc_comp, \
        avg_discoverer_val = self.update_discoverer()
        
        # æ›´æ–°è‡ªé€‚åº”Huber Loss deltaå‚æ•°
        self.update_adaptive_delta(coordinator_value_loss, discoverer_value_loss)
        
        # æ›´æ–°è®­ç»ƒä¿¡æ¯
        self.training_info['high_level_loss'].append(coordinator_loss)
        self.training_info['low_level_loss'].append(discoverer_loss)
        self.training_info['discriminator_loss'].append(discriminator_loss)
        self.training_info['team_skill_entropy'].append(team_skill_entropy) # çœŸæ­£çš„å›¢é˜ŸæŠ€èƒ½ç†µ
        self.training_info['agent_skill_entropy'].append(agent_skill_entropy) # ä¸ªä½“æŠ€èƒ½ç†µï¼Œä¸å†æ˜¯å ä½ç¬¦
        self.training_info['action_entropy'].append(action_entropy)
        
        self.training_info['intrinsic_reward_low_level_average'].append(avg_intrinsic_reward)
        self.training_info['intrinsic_reward_env_component'].append(avg_env_comp)
        self.training_info['intrinsic_reward_team_disc_component'].append(avg_team_disc_comp)
        self.training_info['intrinsic_reward_ind_disc_component'].append(avg_ind_disc_comp)
        
        self.training_info['coordinator_state_value_mean'].append(mean_coord_state_val)
        self.training_info['coordinator_agent_value_mean'].append(mean_coord_agent_val)
        self.training_info['discoverer_value_mean'].append(avg_discoverer_val)

        # è®°å½•åˆ°TensorBoard
        # æŸå¤±å‡½æ•°è®°å½•
        self.writer.add_scalar('Losses/Coordinator/Total', coordinator_loss, self.global_step)
        self.writer.add_scalar('Losses/Discoverer/Total', discoverer_loss, self.global_step)
        self.writer.add_scalar('Losses/Discriminator/Total', discriminator_loss, self.global_step)
        
        # è¯¦ç»†æŸå¤±ç»„æˆ
        self.writer.add_scalar('Losses/Coordinator/Policy', coordinator_policy_loss, self.global_step)
        self.writer.add_scalar('Losses/Coordinator/Value', coordinator_value_loss, self.global_step)
        self.writer.add_scalar('Losses/Discoverer/Policy', discoverer_policy_loss, self.global_step)
        self.writer.add_scalar('Losses/Discoverer/Value', discoverer_value_loss, self.global_step)
        
        # ç†µè®°å½•
        # ç°åœ¨åˆ†åˆ«è®°å½•å›¢é˜Ÿå’Œä¸ªä½“æŠ€èƒ½ç†µï¼Œè€Œä¸æ˜¯å¹³å‡å€¼
        self.writer.add_scalar('Entropy/Coordinator/TeamSkill_Z', team_skill_entropy, self.global_step)
        self.writer.add_scalar('Entropy/Coordinator/AgentSkill_z_Average', agent_skill_entropy, self.global_step)
        self.writer.add_scalar('Entropy/Discoverer/Action', action_entropy, self.global_step)

        # å¥–åŠ±è®°å½•
        # æ–°å¢å¯¹é«˜å±‚å¥–åŠ±çš„è®°å½•ï¼ˆkæ­¥ç´¯ç§¯ç¯å¢ƒå¥–åŠ±å‡å€¼ï¼‰
        self.writer.add_scalar('Rewards/HighLevel/K_Step_Accumulated_Mean', mean_high_level_reward, self.global_step)
        
        # å†…åœ¨å¥–åŠ±è®°å½•
        self.writer.add_scalar('Rewards/Intrinsic/LowLevel_Average', avg_intrinsic_reward, self.global_step)
        self.writer.add_scalar('Rewards/Intrinsic/Components/Environmental_Portion_Average', avg_env_comp, self.global_step)
        self.writer.add_scalar('Rewards/Intrinsic/Components/TeamDiscriminator_Portion_Average', avg_team_disc_comp, self.global_step)
        self.writer.add_scalar('Rewards/Intrinsic/Components/IndividualDiscriminator_Portion_Average', avg_ind_disc_comp, self.global_step)

        # ä»·å€¼å‡½æ•°ä¼°è®¡è®°å½•
        self.writer.add_scalar('ValueEstimates/Coordinator/StateValue_Mean', mean_coord_state_val, self.global_step)
        self.writer.add_scalar('ValueEstimates/Coordinator/AgentValue_Average_Mean', mean_coord_agent_val, self.global_step)
        self.writer.add_scalar('ValueEstimates/Discoverer/Value_Mean', avg_discoverer_val, self.global_step)

        # æ·»åŠ ä¸€ä¸ªå›ºå®šçš„æµ‹è¯•å€¼ï¼Œç”¨äºè°ƒè¯•TensorBoardæ˜¾ç¤ºé—®é¢˜
        self.writer.add_scalar('Debug/test_value', 1.0, self.global_step)
        
        # æ¯æ¬¡æ›´æ–°åéƒ½åˆ·æ–°æ•°æ®åˆ°ç¡¬ç›˜ï¼Œç¡®ä¿TensorBoardèƒ½å°½å¿«çœ‹åˆ°
        self.writer.flush()
        
        # è¿”å›çš„å­—å…¸ä¹Ÿåº”åŒ…å«æ–°æŒ‡æ ‡ï¼Œæ–¹ä¾¿å¤–éƒ¨è°ƒç”¨è€…è·å–
        return {
            'discriminator_loss': discriminator_loss,
            'coordinator_loss': coordinator_loss,
            'coordinator_policy_loss': coordinator_policy_loss,
            'coordinator_value_loss': coordinator_value_loss,
            'discoverer_loss': discoverer_loss,
            'discoverer_policy_loss': discoverer_policy_loss,
            'discoverer_value_loss': discoverer_value_loss,
            'team_skill_entropy': team_skill_entropy, # å›¢é˜ŸæŠ€èƒ½ç†µ
            'agent_skill_entropy': agent_skill_entropy, # ä¸ªä½“æŠ€èƒ½ç†µ
            'action_entropy': action_entropy, # ä½å±‚åŠ¨ä½œç†µ
            'avg_intrinsic_reward': avg_intrinsic_reward,
            'avg_env_comp': avg_env_comp,
            'avg_team_disc_comp': avg_team_disc_comp,
            'avg_ind_disc_comp': avg_ind_disc_comp,
            'mean_coord_state_val': mean_coord_state_val,
            'mean_coord_agent_val': mean_coord_agent_val,
            'avg_discoverer_val': avg_discoverer_val,
            'mean_high_level_reward': mean_high_level_reward # é«˜å±‚å¥–åŠ±å‡å€¼
        }
    
    def save_model(self, path):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'skill_coordinator': self.skill_coordinator.state_dict(),
            'skill_discoverer': self.skill_discoverer.state_dict(),
            'team_discriminator': self.team_discriminator.state_dict(),
            'individual_discriminator': self.individual_discriminator.state_dict(),
            'config': self.config
        }, path)
        main_logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ° {path}")
    
    def log_skill_distribution(self, team_skill, agent_skills, episode=None):
        """è®°å½•æŠ€èƒ½åˆ†é…åˆ†å¸ƒåˆ°TensorBoard
        
        å‚æ•°:
            team_skill: å›¢é˜ŸæŠ€èƒ½ç´¢å¼•
            agent_skills: ä¸ªä½“æŠ€èƒ½ç´¢å¼•åˆ—è¡¨
            episode: å¦‚æœæä¾›ï¼Œå°†ä½œä¸ºxè½´è®°å½•ç‚¹ï¼›å¦åˆ™ä½¿ç”¨global_step
        """
        if not hasattr(self, 'writer'):
            return
            
        step = episode if episode is not None else self.global_step
        
        # è®°å½•å½“å‰å›¢é˜ŸæŠ€èƒ½ (ç¬æ—¶)
        self.writer.add_scalar('Skills/Current/TeamSkill', team_skill, step)
        
        # è®°å½•å½“å‰ä¸ªä½“æŠ€èƒ½åˆ†å¸ƒ (ç¬æ—¶)
        for i, skill_val in enumerate(agent_skills): # Renamed skill to skill_val to avoid conflict
            self.writer.add_scalar(f'Skills/Current/Agent{i}_Skill', skill_val, step)
        
        # è®¡ç®—å¹¶è®°å½•å½“å‰ä¸ªä½“æŠ€èƒ½çš„å¤šæ ·æ€§ (ç¬æ—¶)
        if len(agent_skills) > 0:
            current_skill_counts = {}
            for skill_val in agent_skills:
                current_skill_counts[skill_val] = current_skill_counts.get(skill_val, 0) + 1
            
            n_agents_current = len(agent_skills)
            current_skill_entropy = 0
            for count in current_skill_counts.values():
                p = count / n_agents_current
                if p > 0: # Avoid log(0)
                    current_skill_entropy -= p * np.log(p)
            self.writer.add_scalar('Skills/Current/Diversity', current_skill_entropy, step)

        # è®°å½•æ•´ä¸ªepisodeçš„æŠ€èƒ½ä½¿ç”¨è®¡æ•°
        if episode is not None: #åªåœ¨æä¾›äº†episodeï¼ˆé€šå¸¸åœ¨episodeç»“æŸæ—¶ï¼‰æ‰è®°å½•å’Œé‡ç½®è®¡æ•°
            for skill_id, count_val in self.episode_team_skill_counts.items():
                self.writer.add_scalar(f'Skills/EpisodeCounts/TeamSkill_{skill_id}', count_val, episode)
            
            for i, agent_counts in enumerate(self.episode_agent_skill_counts):
                for skill_id, count_val in agent_counts.items():
                    self.writer.add_scalar(f'Skills/EpisodeCounts/Agent{i}_Skill_{skill_id}', count_val, episode)
            
            # é‡ç½®è®¡æ•°å™¨ä¸ºä¸‹ä¸€ä¸ªepisodeåšå‡†å¤‡
            self.episode_team_skill_counts = {}
            # æ ¹æ®å½“å‰æ™ºèƒ½ä½“æ•°é‡ï¼ˆå¦‚æœæœ‰ï¼‰æˆ–é…ç½®é‡æ–°åˆå§‹åŒ–ï¼Œä»¥é˜²æ™ºèƒ½ä½“æ•°é‡å˜åŒ–
            num_current_agents = len(agent_skills) if agent_skills is not None and len(agent_skills) > 0 else self.config.n_agents
            self.episode_agent_skill_counts = [{} for _ in range(num_current_agents)]
            # é™çº§ä¸ºDEBUGæ—¥å¿—ï¼Œé¿å…é¢‘ç¹è¾“å‡ºåˆ°æ§åˆ¶å°
            main_logger.debug(f"Episode {episode} skill counts logged and reset.")

    def load_model(self, path):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path, map_location=self.device)
        self.skill_coordinator.load_state_dict(checkpoint['skill_coordinator'])
        self.skill_discoverer.load_state_dict(checkpoint['skill_discoverer'])
        self.team_discriminator.load_state_dict(checkpoint['team_discriminator'])
        self.individual_discriminator.load_state_dict(checkpoint['individual_discriminator'])
        main_logger.info(f"æ¨¡å‹å·²ä» {path} åŠ è½½")

#!/usr/bin/env python3
"""
æµ‹è¯•å¤šç¯å¢ƒå¹¶è¡Œè®­ç»ƒæ”¯æŒçš„å®ç°
éªŒè¯HMASDä»£ç†æ˜¯å¦æ­£ç¡®æ”¯æŒå¤šç¯å¢ƒå¹¶è¡Œè®­ç»ƒ
"""

import os
import sys
import time
import numpy as np
import torch
import unittest
from unittest.mock import Mock, patch
import tempfile
import shutil

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config_1 import Config
from hmasd.agent import HMASDAgent

class TestMultiEnvSupport(unittest.TestCase):
    """æµ‹è¯•å¤šç¯å¢ƒæ”¯æŒçš„å•å…ƒæµ‹è¯•ç±»"""
    
    def setUp(self):
        """æµ‹è¯•å‰çš„åˆå§‹åŒ–"""
        self.config = Config()
        # è®¾ç½®è¾ƒå°çš„å‚æ•°ä»¥ä¾¿å¿«é€Ÿæµ‹è¯•
        self.config.state_dim = 10
        self.config.obs_dim = 8
        self.config.n_agents = 3
        self.config.action_dim = 4
        self.config.k = 5  # æŠ€èƒ½å‘¨æœŸé•¿åº¦
        self.config.buffer_size = 100
        self.config.batch_size = 32
        self.config.high_level_batch_size = 16
        
        # åˆ›å»ºä¸´æ—¶æ—¥å¿—ç›®å½•
        self.temp_dir = tempfile.mkdtemp()
        
        # åˆ›å»ºä»£ç†
        self.agent = HMASDAgent(self.config, log_dir=self.temp_dir, device=torch.device('cpu'))
        
        # æµ‹è¯•ç¯å¢ƒæ•°é‡
        self.num_envs = 4
        
    def tearDown(self):
        """æµ‹è¯•åçš„æ¸…ç†"""
        # å…³é—­TensorBoard writer
        if hasattr(self.agent, 'writer'):
            self.agent.writer.close()
        
        # åˆ é™¤ä¸´æ—¶ç›®å½•
        shutil.rmtree(self.temp_dir, ignore_errors=True)
    
    def test_env_state_initialization(self):
        """æµ‹è¯•ç¯å¢ƒçŠ¶æ€çš„åˆå§‹åŒ–"""
        print("\n=== æµ‹è¯•ç¯å¢ƒçŠ¶æ€åˆå§‹åŒ– ===")
        
        # æ£€æŸ¥é¢„åˆå§‹åŒ–çš„ç¯å¢ƒçŠ¶æ€
        self.assertIsInstance(self.agent.env_team_skills, dict)
        self.assertIsInstance(self.agent.env_agent_skills, dict)
        self.assertIsInstance(self.agent.env_log_probs, dict)
        self.assertIsInstance(self.agent.env_hidden_states, dict)
        self.assertIsInstance(self.agent.env_reward_sums, dict)
        self.assertIsInstance(self.agent.env_timers, dict)
        
        # æ£€æŸ¥é¢„åˆå§‹åŒ–çš„32ä¸ªç¯å¢ƒ
        for env_id in range(32):
            self.assertIn(env_id, self.agent.env_reward_sums)
            self.assertIn(env_id, self.agent.env_timers)
            self.assertEqual(self.agent.env_reward_sums[env_id], 0.0)
            self.assertEqual(self.agent.env_timers[env_id], 0)
            self.assertIsNone(self.agent.env_team_skills[env_id])
            self.assertIsNone(self.agent.env_agent_skills[env_id])
        
        print("âœ“ ç¯å¢ƒçŠ¶æ€åˆå§‹åŒ–æ­£ç¡®")
    
    def test_multi_env_step(self):
        """æµ‹è¯•å¤šç¯å¢ƒçš„stepæ–¹æ³•"""
        print("\n=== æµ‹è¯•å¤šç¯å¢ƒstepæ–¹æ³• ===")
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        state = np.random.randn(self.config.state_dim)
        observations = np.random.randn(self.config.n_agents, self.config.obs_dim)
        
        env_skills = {}
        env_infos = {}
        
        # æµ‹è¯•å¤šä¸ªç¯å¢ƒçš„stepè°ƒç”¨
        for env_id in range(self.num_envs):
            actions, info = self.agent.step(state, observations, 0, deterministic=False, env_id=env_id)
            
            # éªŒè¯è¿”å›å€¼
            self.assertEqual(actions.shape, (self.config.n_agents, self.config.action_dim))
            self.assertIn('team_skill', info)
            self.assertIn('agent_skills', info)
            self.assertIn('env_id', info)
            self.assertEqual(info['env_id'], env_id)
            
            # å­˜å‚¨æŠ€èƒ½ä¿¡æ¯
            env_skills[env_id] = {
                'team_skill': info['team_skill'],
                'agent_skills': info['agent_skills']
            }
            env_infos[env_id] = info
            
            # éªŒè¯ç¯å¢ƒç‰¹å®šçŠ¶æ€å·²æ›´æ–°
            self.assertIsNotNone(self.agent.env_team_skills[env_id])
            self.assertIsNotNone(self.agent.env_agent_skills[env_id])
            self.assertIsNotNone(self.agent.env_log_probs[env_id])
        
        # éªŒè¯ä¸åŒç¯å¢ƒå¯èƒ½æœ‰ä¸åŒçš„æŠ€èƒ½ï¼ˆå…è®¸ç›¸åŒï¼Œä½†ç»“æ„åº”è¯¥æ­£ç¡®ï¼‰
        for env_id in range(self.num_envs):
            team_skill = env_skills[env_id]['team_skill']
            agent_skills = env_skills[env_id]['agent_skills']
            
            self.assertTrue(0 <= team_skill < self.config.n_Z)
            self.assertEqual(len(agent_skills), self.config.n_agents)
            for agent_skill in agent_skills:
                self.assertTrue(0 <= agent_skill < self.config.n_z)
        
        print(f"âœ“ å¤šç¯å¢ƒstepæ–¹æ³•æ­£ç¡®ï¼Œæµ‹è¯•äº†{self.num_envs}ä¸ªç¯å¢ƒ")
        print(f"  ç¯å¢ƒæŠ€èƒ½åˆ†é…: {[(env_id, skills['team_skill']) for env_id, skills in env_skills.items()]}")
    
    def test_skill_timer_management(self):
        """æµ‹è¯•æŠ€èƒ½è®¡æ—¶å™¨ç®¡ç†"""
        print("\n=== æµ‹è¯•æŠ€èƒ½è®¡æ—¶å™¨ç®¡ç† ===")
        
        state = np.random.randn(self.config.state_dim)
        observations = np.random.randn(self.config.n_agents, self.config.obs_dim)
        
        env_id = 0
        
        # æ¨¡æ‹Ÿkæ­¥çš„æŠ€èƒ½ä½¿ç”¨
        for step in range(self.config.k + 2):  # å¤šæ‰§è¡Œ2æ­¥ä»¥æµ‹è¯•é‡ç½®
            actions, info = self.agent.step(state, observations, step, deterministic=False, env_id=env_id)
            
            expected_skill_changed = (step % self.config.k == 0)
            actual_skill_changed = info['skill_changed']
            
            if step == 0:
                # ç¬¬ä¸€æ­¥åº”è¯¥åˆ†é…æŠ€èƒ½
                self.assertTrue(actual_skill_changed, f"æ­¥éª¤{step}: åº”è¯¥åˆ†é…åˆå§‹æŠ€èƒ½")
            else:
                # æ£€æŸ¥æŠ€èƒ½å˜åŒ–æ˜¯å¦ç¬¦åˆé¢„æœŸ
                self.assertEqual(expected_skill_changed, actual_skill_changed, 
                               f"æ­¥éª¤{step}: æŠ€èƒ½å˜åŒ–é¢„æœŸ={expected_skill_changed}, å®é™…={actual_skill_changed}")
            
            # éªŒè¯è®¡æ—¶å™¨å€¼
            expected_timer = 0 if expected_skill_changed else (step % self.config.k)
            actual_timer = info['skill_timer']
            
            print(f"  æ­¥éª¤{step}: æŠ€èƒ½å˜åŒ–={actual_skill_changed}, è®¡æ—¶å™¨={actual_timer}, é¢„æœŸè®¡æ—¶å™¨={expected_timer}")
        
        print("âœ“ æŠ€èƒ½è®¡æ—¶å™¨ç®¡ç†æ­£ç¡®")
    
    def test_experience_storage(self):
        """æµ‹è¯•ç»éªŒå­˜å‚¨"""
        print("\n=== æµ‹è¯•ç»éªŒå­˜å‚¨ ===")
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        state = np.random.randn(self.config.state_dim)
        next_state = np.random.randn(self.config.state_dim)
        observations = np.random.randn(self.config.n_agents, self.config.obs_dim)
        next_observations = np.random.randn(self.config.n_agents, self.config.obs_dim)
        actions = np.random.randn(self.config.n_agents, self.config.action_dim)
        rewards = 1.0
        dones = False
        
        initial_low_level_size = len(self.agent.low_level_buffer)
        initial_high_level_samples = self.agent.high_level_samples_total
        
        # ä¸ºä¸åŒç¯å¢ƒå­˜å‚¨ç»éªŒ
        for env_id in range(self.num_envs):
            # å…ˆæ‰§è¡Œstepè·å–æŠ€èƒ½ä¿¡æ¯
            _, info = self.agent.step(state, observations, 0, deterministic=False, env_id=env_id)
            
            # å­˜å‚¨è½¬æ¢
            self.agent.store_transition(
                state, next_state, observations, next_observations,
                actions, rewards, dones,
                info['team_skill'], info['agent_skills'], info['action_logprobs'],
                log_probs=info['log_probs'], skill_timer_for_env=0, env_id=env_id
            )
        
        # éªŒè¯ä½å±‚ç»éªŒå¢åŠ 
        expected_low_level_increase = self.num_envs * self.config.n_agents
        actual_low_level_increase = len(self.agent.low_level_buffer) - initial_low_level_size
        self.assertEqual(actual_low_level_increase, expected_low_level_increase,
                        f"ä½å±‚ç»éªŒåº”è¯¥å¢åŠ {expected_low_level_increase}ä¸ªï¼Œå®é™…å¢åŠ {actual_low_level_increase}ä¸ª")
        
        print(f"âœ“ ç»éªŒå­˜å‚¨æ­£ç¡®ï¼Œä½å±‚ç»éªŒå¢åŠ {actual_low_level_increase}ä¸ª")
        
        # æµ‹è¯•é«˜å±‚ç»éªŒæ”¶é›†ï¼ˆåœ¨æŠ€èƒ½å‘¨æœŸç»“æŸæ—¶ï¼‰
        print("\n--- æµ‹è¯•é«˜å±‚ç»éªŒæ”¶é›† ---")
        
        for env_id in range(self.num_envs):
            # æ¨¡æ‹ŸæŠ€èƒ½å‘¨æœŸç»“æŸ
            _, info = self.agent.step(state, observations, self.config.k-1, deterministic=False, env_id=env_id)
            
            self.agent.store_transition(
                state, next_state, observations, next_observations,
                actions, rewards, dones,
                info['team_skill'], info['agent_skills'], info['action_logprobs'],
                log_probs=info['log_probs'], skill_timer_for_env=self.config.k-1, env_id=env_id
            )
        
        # éªŒè¯é«˜å±‚ç»éªŒå¢åŠ 
        final_high_level_samples = self.agent.high_level_samples_total
        high_level_increase = final_high_level_samples - initial_high_level_samples
        self.assertGreater(high_level_increase, 0, "åº”è¯¥æœ‰é«˜å±‚ç»éªŒè¢«æ”¶é›†")
        
        print(f"âœ“ é«˜å±‚ç»éªŒæ”¶é›†æ­£ç¡®ï¼Œæ”¶é›†åˆ°{high_level_increase}ä¸ªé«˜å±‚æ ·æœ¬")
    
    def test_env_isolation(self):
        """æµ‹è¯•ç¯å¢ƒéš”ç¦»æ€§"""
        print("\n=== æµ‹è¯•ç¯å¢ƒéš”ç¦»æ€§ ===")
        
        state = np.random.randn(self.config.state_dim)
        observations = np.random.randn(self.config.n_agents, self.config.obs_dim)
        
        # ä¸ºä¸åŒç¯å¢ƒæ‰§è¡Œä¸åŒæ•°é‡çš„æ­¥éª¤
        env_steps = {0: 0, 1: 3, 2: 7, 3: 2}
        
        for env_id, steps in env_steps.items():
            for step in range(steps + 1):
                self.agent.step(state, observations, step, deterministic=False, env_id=env_id)
        
        # éªŒè¯ç¯å¢ƒçŠ¶æ€ç‹¬ç«‹æ€§
        for env_id, expected_steps in env_steps.items():
            # æ£€æŸ¥è®¡æ—¶å™¨çŠ¶æ€
            expected_timer = expected_steps % self.config.k
            # æ³¨æ„ï¼šç”±äºstepæ–¹æ³•å†…éƒ¨çš„é€»è¾‘ï¼Œtimerå¯èƒ½ä¼šåœ¨è¾¾åˆ°k-1åé‡ç½®
            if expected_steps > 0 and expected_steps % self.config.k == 0:
                expected_timer = 0
                
            print(f"  ç¯å¢ƒ{env_id}: æ‰§è¡Œäº†{expected_steps}æ­¥, å½“å‰è®¡æ—¶å™¨çŠ¶æ€={self.agent.env_timers[env_id]}")
            
            # éªŒè¯ç¯å¢ƒæœ‰è‡ªå·±çš„æŠ€èƒ½çŠ¶æ€
            self.assertIsNotNone(self.agent.env_team_skills[env_id])
            self.assertIsNotNone(self.agent.env_agent_skills[env_id])
        
        print("âœ“ ç¯å¢ƒéš”ç¦»æ€§æ­£ç¡®")
    
    def test_backward_compatibility(self):
        """æµ‹è¯•å‘åå…¼å®¹æ€§"""
        print("\n=== æµ‹è¯•å‘åå…¼å®¹æ€§ ===")
        
        state = np.random.randn(self.config.state_dim)
        observations = np.random.randn(self.config.n_agents, self.config.obs_dim)
        
        # æµ‹è¯•ä¸ä¼ é€’env_idå‚æ•°ï¼ˆåº”è¯¥é»˜è®¤ä¸º0ï¼‰
        actions1, info1 = self.agent.step(state, observations, 0, deterministic=False)
        actions2, info2 = self.agent.step(state, observations, 0, deterministic=False, env_id=0)
        
        # éªŒè¯ç»“æœç»“æ„ç›¸åŒ
        self.assertEqual(actions1.shape, actions2.shape)
        self.assertIn('team_skill', info1)
        self.assertIn('agent_skills', info1)
        self.assertIn('team_skill', info2)
        self.assertIn('agent_skills', info2)
        
        # éªŒè¯å…¨å±€çŠ¶æ€ä¸ç¯å¢ƒ0çŠ¶æ€åŒæ­¥
        self.assertEqual(self.agent.current_team_skill, self.agent.env_team_skills[0])
        np.testing.assert_array_equal(self.agent.current_agent_skills, self.agent.env_agent_skills[0])
        
        print("âœ“ å‘åå…¼å®¹æ€§æ­£ç¡®")
    
    def test_action_selection_consistency(self):
        """æµ‹è¯•åŠ¨ä½œé€‰æ‹©çš„ä¸€è‡´æ€§"""
        print("\n=== æµ‹è¯•åŠ¨ä½œé€‰æ‹©ä¸€è‡´æ€§ ===")
        
        state = np.random.randn(self.config.state_dim)
        observations = np.random.randn(self.config.n_agents, self.config.obs_dim)
        
        # è®¾ç½®ç›¸åŒçš„éšæœºç§å­
        torch.manual_seed(42)
        np.random.seed(42)
        
        # ä¸ºå¤šä¸ªç¯å¢ƒè·å–åŠ¨ä½œ
        actions_dict = {}
        for env_id in range(3):
            # é‡ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡ç°æ€§æµ‹è¯•
            if env_id == 0:
                torch.manual_seed(42)
                np.random.seed(42)
            
            actions, info = self.agent.step(state, observations, 0, deterministic=True, env_id=env_id)
            actions_dict[env_id] = actions
            
            # éªŒè¯åŠ¨ä½œå½¢çŠ¶
            self.assertEqual(actions.shape, (self.config.n_agents, self.config.action_dim))
            
            # éªŒè¯ç¡®å®šæ€§æ¨¡å¼ä¸‹çš„ä¸€è‡´æ€§ï¼ˆç›¸åŒç¯å¢ƒIDåº”è¯¥äº§ç”Ÿç›¸åŒç»“æœï¼‰
            if env_id > 0:
                actions2, _ = self.agent.step(state, observations, 1, deterministic=True, env_id=env_id)
                # æ³¨æ„ï¼šç”±äºæŠ€èƒ½å¯èƒ½ä¸åŒï¼ŒåŠ¨ä½œå¯èƒ½ä¸åŒï¼Œä½†åº”è¯¥æ˜¯æœ‰æ•ˆçš„
                self.assertEqual(actions2.shape, (self.config.n_agents, self.config.action_dim))
        
        print("âœ“ åŠ¨ä½œé€‰æ‹©ä¸€è‡´æ€§æ­£ç¡®")
    
    def run_comprehensive_test(self):
        """è¿è¡Œç»¼åˆæµ‹è¯•"""
        print("\n" + "="*50)
        print("å¼€å§‹HMASDå¤šç¯å¢ƒæ”¯æŒç»¼åˆæµ‹è¯•")
        print("="*50)
        
        try:
            self.test_env_state_initialization()
            self.test_multi_env_step()
            self.test_skill_timer_management()
            self.test_experience_storage()
            self.test_env_isolation()
            self.test_backward_compatibility()
            self.test_action_selection_consistency()
            
            print("\n" + "="*50)
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼HMASDå¤šç¯å¢ƒæ”¯æŒå®ç°æ­£ç¡®ï¼")
            print("="*50)
            return True
            
        except Exception as e:
            print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("HMASDå¤šç¯å¢ƒå¹¶è¡Œè®­ç»ƒæ”¯æŒéªŒè¯æµ‹è¯•")
    print("æµ‹è¯•ç›®æ ‡ï¼šéªŒè¯å¤šç¯å¢ƒå¹¶è¡Œè®­ç»ƒçš„æ­£ç¡®å®ç°")
    
    # åˆ›å»ºæµ‹è¯•å®ä¾‹
    test_instance = TestMultiEnvSupport()
    test_instance.setUp()
    
    try:
        # è¿è¡Œç»¼åˆæµ‹è¯•
        success = test_instance.run_comprehensive_test()
        
        if success:
            print("\nâœ… æµ‹è¯•ç»“è®ºï¼šå¤šç¯å¢ƒå¹¶è¡Œè®­ç»ƒæ”¯æŒå®ç°æ­£ç¡®ï¼Œå¯ä»¥å®‰å…¨ä½¿ç”¨ï¼")
            
            # è¾“å‡ºä¸€äº›å®ç”¨ä¿¡æ¯
            print("\nğŸ“‹ ä½¿ç”¨æŒ‡å—ï¼š")
            print("1. åœ¨agent.step()è°ƒç”¨æ—¶ä¼ é€’env_idå‚æ•°ï¼š")
            print("   actions, info = agent.step(state, obs, step, env_id=env_id)")
            print("2. åœ¨agent.store_transition()è°ƒç”¨æ—¶ä¼ é€’env_idå‚æ•°ï¼š")
            print("   agent.store_transition(..., env_id=env_id)")
            print("3. æ¯ä¸ªç¯å¢ƒç»´æŠ¤ç‹¬ç«‹çš„æŠ€èƒ½çŠ¶æ€å’Œè®¡æ—¶å™¨")
            print("4. ç¯å¢ƒ0çš„çŠ¶æ€ä¼šåŒæ­¥åˆ°å…¨å±€çŠ¶æ€ä»¥ä¿æŒå…¼å®¹æ€§")
            
            return 0
        else:
            print("\nâŒ æµ‹è¯•ç»“è®ºï¼šå‘ç°é—®é¢˜ï¼Œéœ€è¦ä¿®å¤åå†ä½¿ç”¨ï¼")
            return 1
            
    finally:
        test_instance.tearDown()

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
